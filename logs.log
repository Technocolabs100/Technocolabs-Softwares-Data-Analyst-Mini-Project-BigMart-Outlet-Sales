2023-03-20 14:51:57,838:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-20 14:51:57,838:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-20 14:51:57,838:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-20 14:51:57,838:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-20 14:52:02,361:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-20 14:52:03,355:INFO:PyCaret RegressionExperiment
2023-03-20 14:52:03,355:INFO:Logging name: reg-default-name
2023-03-20 14:52:03,355:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-20 14:52:03,355:INFO:version 3.0.0
2023-03-20 14:52:03,355:INFO:Initializing setup()
2023-03-20 14:52:03,356:INFO:self.USI: 7ad2
2023-03-20 14:52:03,356:INFO:self._variable_keys: {'X', 'log_plots_param', 'n_jobs_param', 'y_test', 'X_train', 'seed', 'pipeline', 'memory', 'target_param', 'logging_param', 'X_test', 'exp_id', 'fold_shuffle_param', '_available_plots', 'fold_groups_param', 'transform_target_param', 'y_train', 'html_param', 'gpu_n_jobs_param', 'exp_name_log', 'idx', 'data', 'fold_generator', 'gpu_param', 'USI', 'y', '_ml_usecase'}
2023-03-20 14:52:03,356:INFO:Checking environment
2023-03-20 14:52:03,356:INFO:python_version: 3.9.12
2023-03-20 14:52:03,356:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-20 14:52:03,356:INFO:machine: AMD64
2023-03-20 14:52:03,356:INFO:platform: Windows-10-10.0.19045-SP0
2023-03-20 14:52:03,356:INFO:Memory: svmem(total=8496553984, available=1411051520, percent=83.4, used=7085502464, free=1411051520)
2023-03-20 14:52:03,356:INFO:Physical Core: 4
2023-03-20 14:52:03,356:INFO:Logical Core: 8
2023-03-20 14:52:03,356:INFO:Checking libraries
2023-03-20 14:52:03,356:INFO:System:
2023-03-20 14:52:03,356:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-20 14:52:03,356:INFO:executable: C:\Users\CHUKWUKA\anaconda3\python.exe
2023-03-20 14:52:03,356:INFO:   machine: Windows-10-10.0.19045-SP0
2023-03-20 14:52:03,356:INFO:PyCaret required dependencies:
2023-03-20 14:52:03,357:INFO:                 pip: 22.3.1
2023-03-20 14:52:03,357:INFO:          setuptools: 65.5.0
2023-03-20 14:52:03,357:INFO:             pycaret: 3.0.0
2023-03-20 14:52:03,357:INFO:             IPython: 8.7.0
2023-03-20 14:52:03,357:INFO:          ipywidgets: 7.6.5
2023-03-20 14:52:03,357:INFO:                tqdm: 4.64.1
2023-03-20 14:52:03,357:INFO:               numpy: 1.21.5
2023-03-20 14:52:03,357:INFO:              pandas: 1.4.4
2023-03-20 14:52:03,357:INFO:              jinja2: 2.11.3
2023-03-20 14:52:03,357:INFO:               scipy: 1.9.3
2023-03-20 14:52:03,357:INFO:              joblib: 1.2.0
2023-03-20 14:52:03,357:INFO:             sklearn: 1.0.2
2023-03-20 14:52:03,357:INFO:                pyod: 1.0.9
2023-03-20 14:52:03,357:INFO:            imblearn: 0.10.1
2023-03-20 14:52:03,357:INFO:   category_encoders: 2.6.0
2023-03-20 14:52:03,357:INFO:            lightgbm: 3.3.5
2023-03-20 14:52:03,357:INFO:               numba: 0.56.4
2023-03-20 14:52:03,357:INFO:            requests: 2.27.1
2023-03-20 14:52:03,357:INFO:          matplotlib: 3.6.2
2023-03-20 14:52:03,357:INFO:          scikitplot: 0.3.7
2023-03-20 14:52:03,357:INFO:         yellowbrick: 1.5
2023-03-20 14:52:03,358:INFO:              plotly: 5.9.0
2023-03-20 14:52:03,358:INFO:             kaleido: 0.2.1
2023-03-20 14:52:03,358:INFO:         statsmodels: 0.13.2
2023-03-20 14:52:03,358:INFO:              sktime: 0.16.1
2023-03-20 14:52:03,358:INFO:               tbats: 1.1.2
2023-03-20 14:52:03,358:INFO:            pmdarima: 2.0.3
2023-03-20 14:52:03,358:INFO:              psutil: 5.9.0
2023-03-20 14:52:03,358:INFO:PyCaret optional dependencies:
2023-03-20 14:52:03,372:INFO:                shap: Not installed
2023-03-20 14:52:03,372:INFO:           interpret: Not installed
2023-03-20 14:52:03,372:INFO:                umap: Not installed
2023-03-20 14:52:03,372:INFO:    pandas_profiling: Not installed
2023-03-20 14:52:03,372:INFO:  explainerdashboard: Not installed
2023-03-20 14:52:03,372:INFO:             autoviz: Not installed
2023-03-20 14:52:03,372:INFO:           fairlearn: Not installed
2023-03-20 14:52:03,372:INFO:             xgboost: 1.7.1
2023-03-20 14:52:03,372:INFO:            catboost: 1.0.6
2023-03-20 14:52:03,372:INFO:              kmodes: Not installed
2023-03-20 14:52:03,372:INFO:             mlxtend: Not installed
2023-03-20 14:52:03,373:INFO:       statsforecast: Not installed
2023-03-20 14:52:03,373:INFO:        tune_sklearn: Not installed
2023-03-20 14:52:03,373:INFO:                 ray: Not installed
2023-03-20 14:52:03,373:INFO:            hyperopt: Not installed
2023-03-20 14:52:03,373:INFO:              optuna: Not installed
2023-03-20 14:52:03,373:INFO:               skopt: Not installed
2023-03-20 14:52:03,373:INFO:              mlflow: Not installed
2023-03-20 14:52:03,373:INFO:              gradio: Not installed
2023-03-20 14:52:03,373:INFO:             fastapi: Not installed
2023-03-20 14:52:03,373:INFO:             uvicorn: Not installed
2023-03-20 14:52:03,373:INFO:              m2cgen: Not installed
2023-03-20 14:52:03,373:INFO:           evidently: Not installed
2023-03-20 14:52:03,373:INFO:               fugue: Not installed
2023-03-20 14:52:03,373:INFO:           streamlit: Not installed
2023-03-20 14:52:03,373:INFO:             prophet: Not installed
2023-03-20 14:52:03,373:INFO:None
2023-03-20 14:52:03,373:INFO:Set up data.
2023-03-20 14:52:03,546:INFO:Set up train/test split.
2023-03-20 14:52:03,660:INFO:Set up index.
2023-03-20 14:52:03,662:INFO:Set up folding strategy.
2023-03-20 14:52:03,741:INFO:Assigning column types.
2023-03-20 14:52:03,751:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-20 14:52:03,751:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-20 14:52:03,798:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-20 14:52:03,826:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-20 14:52:03,895:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-20 14:52:03,942:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-20 14:52:03,943:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-20 14:52:08,756:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-20 14:52:59,709:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-20 14:52:59,723:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-20 14:52:59,728:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-20 14:52:59,791:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-20 14:52:59,848:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-20 14:52:59,849:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-20 14:52:59,852:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-20 14:52:59,853:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-20 14:52:59,858:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-20 14:52:59,863:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-20 14:52:59,925:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-20 14:52:59,974:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-20 14:52:59,974:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-20 14:52:59,977:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-20 14:52:59,983:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-20 14:52:59,988:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,058:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,119:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,121:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-20 14:53:00,124:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-20 14:53:00,124:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-20 14:53:00,135:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,206:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,261:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,261:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-20 14:53:00,264:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-20 14:53:00,275:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,338:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,386:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,387:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-20 14:53:00,390:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-20 14:53:00,391:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-20 14:53:00,466:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,514:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,514:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-20 14:53:00,518:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-20 14:53:00,606:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,654:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,654:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-20 14:53:00,657:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-20 14:53:00,658:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-20 14:53:00,731:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,790:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-20 14:53:00,793:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-20 14:53:00,871:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-20 14:53:00,921:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-20 14:53:00,924:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-20 14:53:00,924:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-20 14:53:01,050:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-20 14:53:01,053:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-20 14:53:01,176:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-20 14:53:01,179:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-20 14:53:01,194:INFO:Preparing preprocessing pipeline...
2023-03-20 14:53:01,194:INFO:Set up simple imputation.
2023-03-20 14:53:01,208:INFO:Set up encoding of ordinal features.
2023-03-20 14:53:01,224:INFO:Set up encoding of categorical features.
2023-03-20 14:53:01,225:INFO:Set up column name cleaning.
2023-03-20 14:53:01,878:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:53:01,886:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:53:01,918:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:53:01,945:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:53:02,137:INFO:Finished creating preprocessing pipeline.
2023-03-20 14:53:02,460:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\CHUKWUKA\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=[], transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['onehotencoder__Item_Fat_Content_Low '
                                             'Fat',
                                             'onehotencoder__Item_Fat_Content_Regular',
                                             'onehotencoder__Outlet_Size_High',
                                             'onehotenco...
                                             'remainder__Item_Visibility',
                                             'remainder__Item_MRP',
                                             'remainder__Item_Code'],
                                    transformer=LeaveOneOutEncoder(cols=['remainder__Item_Identifier',
                                                                         'remainder__Item_Weight',
                                                                         'remainder__Item_Visibility',
                                                                         'remainder__Item_MRP',
                                                                         'remainder__Item_Code'],
                                                                   handle_missing='return_nan',
                                                                   random_state=666))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-03-20 14:53:02,461:INFO:Creating final display dataframe.
2023-03-20 14:53:03,754:INFO:Setup _display_container:                     Description             Value
0                    Session id               666
1                        Target        Item_Sales
2                   Target type        Regression
3           Original data shape        (5966, 21)
4        Transformed data shape        (5966, 53)
5   Transformed train set shape        (4176, 53)
6    Transformed test set shape        (1790, 53)
7              Ordinal features                12
8          Categorical features                20
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              7ad2
2023-03-20 14:53:03,926:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-20 14:53:03,929:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-20 14:53:04,051:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-20 14:53:04,054:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-20 14:53:04,055:INFO:setup() successfully completed in 60.73s...............
2023-03-20 14:54:59,890:INFO:Initializing compare_models()
2023-03-20 14:54:59,891:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, include=None, fold=None, round=4, cross_validation=True, sort=MSE, n_select=1, budget_time=5, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MSE', 'n_select': 1, 'budget_time': 5, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-20 14:54:59,891:INFO:Checking exceptions
2023-03-20 14:54:59,895:INFO:Preparing display monitor
2023-03-20 14:55:00,032:INFO:Time budget is 5 minutes
2023-03-20 14:55:00,032:INFO:Initializing Linear Regression
2023-03-20 14:55:00,032:INFO:Total runtime is 0.0 minutes
2023-03-20 14:55:00,036:INFO:SubProcess create_model() called ==================================
2023-03-20 14:55:00,037:INFO:Initializing create_model()
2023-03-20 14:55:00,037:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:55:00,037:INFO:Checking exceptions
2023-03-20 14:55:00,037:INFO:Importing libraries
2023-03-20 14:55:00,037:INFO:Copying training dataset
2023-03-20 14:55:00,045:INFO:Defining folds
2023-03-20 14:55:00,046:INFO:Declaring metric variables
2023-03-20 14:55:00,050:INFO:Importing untrained model
2023-03-20 14:55:00,054:INFO:Linear Regression Imported successfully
2023-03-20 14:55:00,061:INFO:Starting cross validation
2023-03-20 14:55:00,072:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:55:20,039:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-20 14:55:20,074:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-20 14:55:20,080:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-20 14:55:20,132:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-20 14:55:20,183:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-20 14:55:20,309:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-03-20 14:55:20,666:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-20 14:55:20,940:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:20,944:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:20,956:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:20,957:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,008:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,013:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,013:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,022:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,022:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,035:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,035:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,047:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,058:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,058:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,071:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,078:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,084:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,088:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,089:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,103:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,106:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,123:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,134:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,146:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,150:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,156:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,163:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,164:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,194:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,210:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,218:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:21,269:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:25,181:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:25,185:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:25,187:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:25,195:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:25,201:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:25,214:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:25,214:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:25,231:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:25,766:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:25,773:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:25,790:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:25,797:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:25,800:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:25,823:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:25,826:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:25,851:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:26,747:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:26,772:INFO:Calculating mean and std
2023-03-20 14:55:26,903:INFO:Creating metrics dataframe
2023-03-20 14:55:26,926:INFO:Uploading results into container
2023-03-20 14:55:26,927:INFO:Uploading model into container now
2023-03-20 14:55:26,927:INFO:_master_model_container: 1
2023-03-20 14:55:26,927:INFO:_display_container: 2
2023-03-20 14:55:26,928:INFO:LinearRegression(n_jobs=-1)
2023-03-20 14:55:26,928:INFO:create_model() successfully completed......................................
2023-03-20 14:55:27,464:INFO:SubProcess create_model() end ==================================
2023-03-20 14:55:27,464:INFO:Creating metrics dataframe
2023-03-20 14:55:27,474:INFO:Initializing Lasso Regression
2023-03-20 14:55:27,474:INFO:Total runtime is 0.45736111799875895 minutes
2023-03-20 14:55:27,478:INFO:SubProcess create_model() called ==================================
2023-03-20 14:55:27,479:INFO:Initializing create_model()
2023-03-20 14:55:27,479:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:55:27,479:INFO:Checking exceptions
2023-03-20 14:55:27,479:INFO:Importing libraries
2023-03-20 14:55:27,479:INFO:Copying training dataset
2023-03-20 14:55:27,489:INFO:Defining folds
2023-03-20 14:55:27,490:INFO:Declaring metric variables
2023-03-20 14:55:27,494:INFO:Importing untrained model
2023-03-20 14:55:27,499:INFO:Lasso Regression Imported successfully
2023-03-20 14:55:27,509:INFO:Starting cross validation
2023-03-20 14:55:27,512:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:55:29,691:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:29,865:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:29,872:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:29,878:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:29,998:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:30,183:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:30,192:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:30,195:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:30,659:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:30,668:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:30,712:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:30,742:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:55:33,143:INFO:Calculating mean and std
2023-03-20 14:55:33,172:INFO:Creating metrics dataframe
2023-03-20 14:55:33,200:INFO:Uploading results into container
2023-03-20 14:55:33,201:INFO:Uploading model into container now
2023-03-20 14:55:33,202:INFO:_master_model_container: 2
2023-03-20 14:55:33,202:INFO:_display_container: 2
2023-03-20 14:55:33,202:INFO:Lasso(random_state=666)
2023-03-20 14:55:33,202:INFO:create_model() successfully completed......................................
2023-03-20 14:55:33,681:INFO:SubProcess create_model() end ==================================
2023-03-20 14:55:33,681:INFO:Creating metrics dataframe
2023-03-20 14:55:33,691:INFO:Initializing Ridge Regression
2023-03-20 14:55:33,691:INFO:Total runtime is 0.5609832525253295 minutes
2023-03-20 14:55:33,694:INFO:SubProcess create_model() called ==================================
2023-03-20 14:55:33,695:INFO:Initializing create_model()
2023-03-20 14:55:33,695:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:55:33,695:INFO:Checking exceptions
2023-03-20 14:55:33,695:INFO:Importing libraries
2023-03-20 14:55:33,695:INFO:Copying training dataset
2023-03-20 14:55:33,702:INFO:Defining folds
2023-03-20 14:55:33,702:INFO:Declaring metric variables
2023-03-20 14:55:33,706:INFO:Importing untrained model
2023-03-20 14:55:33,710:INFO:Ridge Regression Imported successfully
2023-03-20 14:55:33,717:INFO:Starting cross validation
2023-03-20 14:55:33,720:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:55:35,571:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:35,572:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:35,572:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:35,572:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:36,126:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:36,224:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:36,904:INFO:Calculating mean and std
2023-03-20 14:55:36,906:INFO:Creating metrics dataframe
2023-03-20 14:55:36,955:INFO:Uploading results into container
2023-03-20 14:55:36,956:INFO:Uploading model into container now
2023-03-20 14:55:36,956:INFO:_master_model_container: 3
2023-03-20 14:55:36,957:INFO:_display_container: 2
2023-03-20 14:55:36,957:INFO:Ridge(random_state=666)
2023-03-20 14:55:36,957:INFO:create_model() successfully completed......................................
2023-03-20 14:55:37,494:INFO:SubProcess create_model() end ==================================
2023-03-20 14:55:37,494:INFO:Creating metrics dataframe
2023-03-20 14:55:37,505:INFO:Initializing Elastic Net
2023-03-20 14:55:37,505:INFO:Total runtime is 0.6245478908220926 minutes
2023-03-20 14:55:37,509:INFO:SubProcess create_model() called ==================================
2023-03-20 14:55:37,509:INFO:Initializing create_model()
2023-03-20 14:55:37,509:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:55:37,510:INFO:Checking exceptions
2023-03-20 14:55:37,510:INFO:Importing libraries
2023-03-20 14:55:37,510:INFO:Copying training dataset
2023-03-20 14:55:37,517:INFO:Defining folds
2023-03-20 14:55:37,517:INFO:Declaring metric variables
2023-03-20 14:55:37,521:INFO:Importing untrained model
2023-03-20 14:55:37,525:INFO:Elastic Net Imported successfully
2023-03-20 14:55:37,532:INFO:Starting cross validation
2023-03-20 14:55:37,536:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:55:39,597:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:39,648:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:39,654:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:39,678:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:40,122:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:40,132:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:40,263:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:40,346:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:41,113:INFO:Calculating mean and std
2023-03-20 14:55:41,115:INFO:Creating metrics dataframe
2023-03-20 14:55:41,166:INFO:Uploading results into container
2023-03-20 14:55:41,166:INFO:Uploading model into container now
2023-03-20 14:55:41,167:INFO:_master_model_container: 4
2023-03-20 14:55:41,167:INFO:_display_container: 2
2023-03-20 14:55:41,167:INFO:ElasticNet(random_state=666)
2023-03-20 14:55:41,168:INFO:create_model() successfully completed......................................
2023-03-20 14:55:41,662:INFO:SubProcess create_model() end ==================================
2023-03-20 14:55:41,662:INFO:Creating metrics dataframe
2023-03-20 14:55:41,673:INFO:Initializing Least Angle Regression
2023-03-20 14:55:41,674:INFO:Total runtime is 0.6940322319666544 minutes
2023-03-20 14:55:41,677:INFO:SubProcess create_model() called ==================================
2023-03-20 14:55:41,677:INFO:Initializing create_model()
2023-03-20 14:55:41,677:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:55:41,678:INFO:Checking exceptions
2023-03-20 14:55:41,678:INFO:Importing libraries
2023-03-20 14:55:41,678:INFO:Copying training dataset
2023-03-20 14:55:41,685:INFO:Defining folds
2023-03-20 14:55:41,685:INFO:Declaring metric variables
2023-03-20 14:55:41,690:INFO:Importing untrained model
2023-03-20 14:55:41,694:INFO:Least Angle Regression Imported successfully
2023-03-20 14:55:41,702:INFO:Starting cross validation
2023-03-20 14:55:41,705:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:55:42,384:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:42,389:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:42,391:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:42,403:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.397e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,404:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.540e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,406:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.280e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,406:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.280e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,406:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.711e-01, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,407:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,407:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,407:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.670e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,407:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,408:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,408:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,408:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,408:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,409:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,409:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.470e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,409:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.140e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,409:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,409:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.140e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,410:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,410:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.997e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,410:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.140e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,410:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.997e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,410:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.976e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,411:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.631e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,411:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.457e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,411:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.829e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,411:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.631e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,411:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.297e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,412:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.553e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,412:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.504e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,412:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.332e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,412:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.244e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,413:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.130e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,413:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.122e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,413:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.656e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,413:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.867e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,414:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.867e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,414:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.867e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,414:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.892e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,414:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.194e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,415:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.867e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,415:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.194e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,415:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.842e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,415:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.715e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,415:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.050e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,415:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.820e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,415:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.130e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,415:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,415:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.197e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,415:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.565e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,416:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,416:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,416:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.809e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,416:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.524e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,416:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,417:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.907e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,417:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,417:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.524e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,417:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.576e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,417:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.915e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,418:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.220e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,418:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.784e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,419:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.220e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,419:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.328e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,419:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.458e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,419:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.171e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,419:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.403e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,419:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.181e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,419:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.709e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,420:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.072e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,420:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.069e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,420:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.709e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,420:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.767e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,420:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.611e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,420:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=8.932e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,420:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.646e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,420:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.123e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,421:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.084e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,421:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.076e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,421:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.847e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,421:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.909e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,422:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.280e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,422:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.621e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,422:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.110e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,422:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.533e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,422:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.581e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,422:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.122e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,423:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.529e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,423:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,423:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.528e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,423:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,423:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.523e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,423:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,424:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.524e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,424:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.368e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,424:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,424:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.521e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,424:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.508e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,424:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.015e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,424:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,425:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.471e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,425:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.651e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,425:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.346e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,425:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.573e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,425:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.317e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,425:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.290e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,426:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.206e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,426:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.597e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,426:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.908e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,426:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.054e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,426:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.778e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,426:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.908e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,426:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.539e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,426:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.778e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,426:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.061e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,426:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.031e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,426:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.774e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,427:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.990e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,427:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.704e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,427:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.626e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,427:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.582e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,427:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.592e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,427:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.421e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,428:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.476e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,428:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.151e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,428:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.467e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,428:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.480e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,428:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.310e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,428:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.183e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,429:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.980e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,429:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.880e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,429:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.726e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,429:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.770e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,430:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.525e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,430:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.303e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,430:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.280e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,430:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.784e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,431:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.875e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,431:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.651e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,431:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.089e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,431:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.147e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,432:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,434:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,434:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,434:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,435:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,435:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,436:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.865e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,446:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:42,457:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,457:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,457:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,458:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.299e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,458:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.334e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,460:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.599e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,460:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.599e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,461:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.940e-01, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,462:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.761e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,462:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.761e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,462:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.761e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,463:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.363e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,463:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.352e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,463:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.905e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,464:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.905e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,464:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.905e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,464:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.905e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,465:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.372e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,466:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.338e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,467:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.026e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,468:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.655e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,468:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.655e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,469:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.544e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,469:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.388e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,469:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.299e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,470:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.276e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,470:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.164e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,471:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.052e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,471:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.041e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,471:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.954e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,471:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.611e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,472:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.285e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,472:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.285e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,472:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.285e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,473:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.285e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,473:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.778e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,473:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.049e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,473:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.382e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,474:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.400e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,474:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.369e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,474:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.968e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,474:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.039e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,475:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=3.529e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,475:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.759e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,476:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.031e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,476:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.013e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,476:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.969e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,477:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.588e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,477:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.403e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,478:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.132e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,479:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.053e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,480:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,480:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,481:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,481:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,481:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,481:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=6.365e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,482:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.616e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,482:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.585e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,482:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.309e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,482:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.654e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,483:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.820e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,483:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.321e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,483:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.167e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,483:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.157e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,484:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.565e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,484:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.523e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,484:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.093e-06, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,536:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:42,550:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.759e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,551:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,551:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,551:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,552:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.647e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.093e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,552:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.034e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,553:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.602e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,554:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.258e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,554:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.258e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,556:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.729e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,556:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.729e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,556:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.729e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,557:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.429e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,558:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.084e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,558:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.949e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,559:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.539e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,560:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.471e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,560:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.467e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,561:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.975e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,561:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.822e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,561:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.730e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,562:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.575e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,563:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.328e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,563:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.274e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,563:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.022e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,564:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=8.154e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,564:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.678e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,564:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,564:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.187e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,564:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.186e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,565:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.106e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,565:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.280e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,565:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.218e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,565:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.157e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,566:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.980e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,567:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.926e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,567:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.273e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,568:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.771e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,568:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.527e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,568:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.661e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,569:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.598e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,569:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.356e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,569:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.819e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,569:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.900e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,570:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.776e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,570:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.769e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,571:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.762e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,571:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.745e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,571:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.718e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,571:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.705e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,572:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.511e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,572:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.273e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,572:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.925e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,573:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.484e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,574:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.400e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,574:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.399e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,574:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.238e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,574:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.185e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,575:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.051e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,575:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=6.808e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,575:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=5.671e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,576:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.634e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,576:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.072e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,576:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.031e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,576:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.042e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,577:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.877e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,577:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.859e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,577:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.857e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,578:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.851e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,578:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.839e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,578:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.831e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,579:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.267e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,629:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:42,642:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.420e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,643:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,643:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,644:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,644:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,645:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.020e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,646:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.954e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,648:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.440e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,648:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.440e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,650:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.108e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,650:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.107e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,650:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.048e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,651:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.754e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,651:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.735e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,651:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.636e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,652:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.629e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,657:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.909e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,657:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.825e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,658:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.738e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,658:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.602e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,659:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.263e+00, with an active set of 39 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,660:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.218e+00, with an active set of 40 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,660:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.066e+00, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,660:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=9.946e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,660:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.257e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,661:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.061e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,661:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=5.825e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,661:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=5.450e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,662:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.406e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,662:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.503e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,662:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=2.916e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,663:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.802e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,663:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.511e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,664:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.213e-01, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,664:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=9.021e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,664:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.087e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,664:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.087e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,665:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=5.864e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,665:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.022e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,665:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.264e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,665:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.372e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,717:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:42,726:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.088e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,727:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.088e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,728:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.258e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,729:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.258e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,730:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.258e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,731:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.526e-01, with an active set of 14 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,732:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.886e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,733:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.036e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,734:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.036e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,734:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.752e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,734:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.752e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,735:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.386e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,736:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.081e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,736:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.081e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,737:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.938e-01, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,737:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.938e-01, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,738:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.875e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,738:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.788e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,738:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.780e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,738:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.444e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,739:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.345e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,739:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.253e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,740:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.017e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,740:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=9.442e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,740:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.124e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,741:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.124e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,741:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.124e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,741:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=7.706e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,742:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.758e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,742:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.732e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,743:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.356e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,743:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.336e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,743:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.988e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,743:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.988e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,744:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.467e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,744:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.718e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,745:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,745:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,746:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,746:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,746:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,747:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.546e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,747:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.487e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,747:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.411e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,747:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.353e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,748:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=7.983e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,748:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.096e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,748:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.096e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,749:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.096e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,750:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.780e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,751:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.944e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,752:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.745e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,753:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.307e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,753:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,753:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,754:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,754:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,755:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,755:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,755:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.118e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,756:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.473e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,756:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.015e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,756:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.755e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,756:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.925e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,757:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.825e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,757:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.308e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,757:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.004e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,758:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.098e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,759:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.917e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,759:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.892e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,759:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.875e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,759:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.843e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,760:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.834e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,760:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.830e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,760:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.829e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,761:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.827e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,761:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.819e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,761:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.818e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,762:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.778e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,762:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.043e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,762:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.091e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,762:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.832e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,763:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.894e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,763:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.630e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,763:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.417e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,764:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.161e-06, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,777:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:42,793:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,794:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,794:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,795:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.711e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,796:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.137e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,797:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.927e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,797:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.927e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,797:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.925e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,798:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.138e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,799:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.138e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,799:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.138e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,800:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.872e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,801:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.274e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,801:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.274e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,802:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.876e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,802:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.374e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,803:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.298e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,803:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.216e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,803:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.116e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,804:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.013e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,804:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.955e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,805:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.457e-01, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,806:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.098e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,807:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.098e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,807:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.046e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,807:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.007e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,808:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.748e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,808:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.470e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,808:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.882e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,809:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.409e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,809:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.217e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,809:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.661e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,811:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.238e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,811:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.238e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,812:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.221e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,813:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.953e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,814:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.565e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,814:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.563e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,814:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.414e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,815:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.119e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,816:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.677e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,816:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.087e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,816:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.829e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,817:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.826e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,817:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.809e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,818:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.659e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,818:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.474e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,818:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.615e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,818:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.042e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,819:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.869e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,819:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.418e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,819:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.366e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,820:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.361e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,820:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.373e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,820:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.085e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,820:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.928e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,821:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.615e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,821:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.387e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:42,821:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.460e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:43,468:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:43,522:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:43,545:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:43,631:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:43,665:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:43,803:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:43,817:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:43,822:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:44,155:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:44,157:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:44,160:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.459e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,161:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,162:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,162:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,162:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,162:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,162:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,163:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.692e-01, with an active set of 22 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,164:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,164:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,164:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,164:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,164:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,164:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,164:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,164:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,165:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,165:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.023e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,165:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,165:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,165:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.215e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,165:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.215e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,166:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.184e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,166:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.389e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,166:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.184e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,166:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.184e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,166:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.358e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,166:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.096e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,166:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.811e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,166:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.551e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,166:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.362e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,167:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.776e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,167:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.291e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,167:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.308e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,167:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,167:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,167:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,167:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.258e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,167:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,168:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.258e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,168:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,168:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.258e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,168:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,168:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.258e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,168:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.436e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,168:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.226e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,168:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.809e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,168:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.114e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,168:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.259e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,169:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.162e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,169:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.426e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,169:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.126e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,169:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.037e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.814e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,169:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=8.163e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,169:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.539e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,169:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.786e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,169:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.396e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,169:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.786e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,169:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.849e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,169:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.786e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,169:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.625e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,170:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.615e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,170:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=5.427e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,170:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.609e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,170:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.917e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,170:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.592e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,170:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.834e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,170:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.576e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,170:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.785e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,170:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.040e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,170:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.603e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,170:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,170:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.419e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,171:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,171:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.015e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,171:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,171:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.971e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,171:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.146e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,171:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.780e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,171:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,171:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.011e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,171:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,172:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.362e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,172:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.308e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,172:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.572e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,172:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.160e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,172:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.768e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,172:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.630e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,173:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.535e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,173:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.534e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,173:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.534e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,173:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.524e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,173:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.503e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,173:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.502e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,174:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.475e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,174:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.332e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,174:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.937e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,174:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.835e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,174:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.435e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,174:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.769e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,175:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.156e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,175:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.739e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,175:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.234e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:44,761:INFO:Calculating mean and std
2023-03-20 14:55:44,764:INFO:Creating metrics dataframe
2023-03-20 14:55:44,849:INFO:Uploading results into container
2023-03-20 14:55:44,850:INFO:Uploading model into container now
2023-03-20 14:55:44,850:INFO:_master_model_container: 5
2023-03-20 14:55:44,850:INFO:_display_container: 2
2023-03-20 14:55:44,916:INFO:Lars(random_state=666)
2023-03-20 14:55:44,916:INFO:create_model() successfully completed......................................
2023-03-20 14:55:45,411:INFO:SubProcess create_model() end ==================================
2023-03-20 14:55:45,412:INFO:Creating metrics dataframe
2023-03-20 14:55:45,423:INFO:Initializing Lasso Least Angle Regression
2023-03-20 14:55:45,423:INFO:Total runtime is 0.756516687075297 minutes
2023-03-20 14:55:45,427:INFO:SubProcess create_model() called ==================================
2023-03-20 14:55:45,427:INFO:Initializing create_model()
2023-03-20 14:55:45,427:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:55:45,427:INFO:Checking exceptions
2023-03-20 14:55:45,428:INFO:Importing libraries
2023-03-20 14:55:45,428:INFO:Copying training dataset
2023-03-20 14:55:45,435:INFO:Defining folds
2023-03-20 14:55:45,435:INFO:Declaring metric variables
2023-03-20 14:55:45,439:INFO:Importing untrained model
2023-03-20 14:55:45,444:INFO:Lasso Least Angle Regression Imported successfully
2023-03-20 14:55:45,453:INFO:Starting cross validation
2023-03-20 14:55:45,456:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:55:46,135:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:55:46,154:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:55:46,187:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:55:46,205:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:55:46,207:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:55:46,223:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,223:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,223:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,237:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:55:46,244:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.420e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,245:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,246:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,246:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,246:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,246:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.020e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,268:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:55:46,277:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,277:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,278:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,315:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:55:46,323:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.759e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,324:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,325:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:46,325:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:55:47,192:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:47,277:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:47,280:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:47,288:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:47,293:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:47,433:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:47,461:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:47,874:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:55:47,886:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:55:48,466:INFO:Calculating mean and std
2023-03-20 14:55:48,467:INFO:Creating metrics dataframe
2023-03-20 14:55:48,525:INFO:Uploading results into container
2023-03-20 14:55:48,525:INFO:Uploading model into container now
2023-03-20 14:55:48,526:INFO:_master_model_container: 6
2023-03-20 14:55:48,526:INFO:_display_container: 2
2023-03-20 14:55:48,527:INFO:LassoLars(random_state=666)
2023-03-20 14:55:48,527:INFO:create_model() successfully completed......................................
2023-03-20 14:55:49,005:INFO:SubProcess create_model() end ==================================
2023-03-20 14:55:49,111:INFO:Creating metrics dataframe
2023-03-20 14:55:49,123:INFO:Initializing Orthogonal Matching Pursuit
2023-03-20 14:55:49,123:INFO:Total runtime is 0.8181883970896402 minutes
2023-03-20 14:55:49,127:INFO:SubProcess create_model() called ==================================
2023-03-20 14:55:49,128:INFO:Initializing create_model()
2023-03-20 14:55:49,128:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:55:49,128:INFO:Checking exceptions
2023-03-20 14:55:49,128:INFO:Importing libraries
2023-03-20 14:55:49,128:INFO:Copying training dataset
2023-03-20 14:55:49,136:INFO:Defining folds
2023-03-20 14:55:49,136:INFO:Declaring metric variables
2023-03-20 14:55:49,140:INFO:Importing untrained model
2023-03-20 14:55:49,145:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-20 14:55:49,153:INFO:Starting cross validation
2023-03-20 14:55:49,157:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:55:49,825:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:49,865:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:49,867:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:49,919:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:49,949:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:49,977:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:50,005:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:50,053:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:50,951:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:51,001:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:51,178:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:51,178:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:51,178:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:51,178:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:51,178:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:51,184:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:51,636:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:51,720:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:55:52,301:INFO:Calculating mean and std
2023-03-20 14:55:52,303:INFO:Creating metrics dataframe
2023-03-20 14:55:52,384:INFO:Uploading results into container
2023-03-20 14:55:52,384:INFO:Uploading model into container now
2023-03-20 14:55:52,385:INFO:_master_model_container: 7
2023-03-20 14:55:52,385:INFO:_display_container: 2
2023-03-20 14:55:52,385:INFO:OrthogonalMatchingPursuit()
2023-03-20 14:55:52,385:INFO:create_model() successfully completed......................................
2023-03-20 14:55:52,922:INFO:SubProcess create_model() end ==================================
2023-03-20 14:55:52,923:INFO:Creating metrics dataframe
2023-03-20 14:55:52,940:INFO:Initializing Bayesian Ridge
2023-03-20 14:55:52,940:INFO:Total runtime is 0.8817989110946655 minutes
2023-03-20 14:55:52,945:INFO:SubProcess create_model() called ==================================
2023-03-20 14:55:52,945:INFO:Initializing create_model()
2023-03-20 14:55:52,945:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:55:52,946:INFO:Checking exceptions
2023-03-20 14:55:52,947:INFO:Importing libraries
2023-03-20 14:55:52,947:INFO:Copying training dataset
2023-03-20 14:55:52,959:INFO:Defining folds
2023-03-20 14:55:52,959:INFO:Declaring metric variables
2023-03-20 14:55:52,967:INFO:Importing untrained model
2023-03-20 14:55:52,971:INFO:Bayesian Ridge Imported successfully
2023-03-20 14:55:52,979:INFO:Starting cross validation
2023-03-20 14:55:52,982:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:55:54,967:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.88s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:54,997:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:55,002:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:55,135:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:55,166:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:55,167:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:55,424:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:55,526:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:56,591:INFO:Calculating mean and std
2023-03-20 14:55:56,592:INFO:Creating metrics dataframe
2023-03-20 14:55:56,664:INFO:Uploading results into container
2023-03-20 14:55:56,664:INFO:Uploading model into container now
2023-03-20 14:55:56,665:INFO:_master_model_container: 8
2023-03-20 14:55:56,665:INFO:_display_container: 2
2023-03-20 14:55:56,665:INFO:BayesianRidge()
2023-03-20 14:55:56,665:INFO:create_model() successfully completed......................................
2023-03-20 14:55:57,227:INFO:SubProcess create_model() end ==================================
2023-03-20 14:55:57,227:INFO:Creating metrics dataframe
2023-03-20 14:55:57,240:INFO:Initializing Passive Aggressive Regressor
2023-03-20 14:55:57,240:INFO:Total runtime is 0.9534762183825174 minutes
2023-03-20 14:55:57,245:INFO:SubProcess create_model() called ==================================
2023-03-20 14:55:57,246:INFO:Initializing create_model()
2023-03-20 14:55:57,246:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:55:57,246:INFO:Checking exceptions
2023-03-20 14:55:57,246:INFO:Importing libraries
2023-03-20 14:55:57,246:INFO:Copying training dataset
2023-03-20 14:55:57,253:INFO:Defining folds
2023-03-20 14:55:57,253:INFO:Declaring metric variables
2023-03-20 14:55:57,257:INFO:Importing untrained model
2023-03-20 14:55:57,263:INFO:Passive Aggressive Regressor Imported successfully
2023-03-20 14:55:57,270:INFO:Starting cross validation
2023-03-20 14:55:57,273:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:55:59,602:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.14s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:59,616:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:59,617:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:59,617:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:59,839:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.17s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:59,888:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:55:59,897:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.22s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:00,443:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:01,382:INFO:Calculating mean and std
2023-03-20 14:56:01,384:INFO:Creating metrics dataframe
2023-03-20 14:56:01,460:INFO:Uploading results into container
2023-03-20 14:56:01,460:INFO:Uploading model into container now
2023-03-20 14:56:01,461:INFO:_master_model_container: 9
2023-03-20 14:56:01,461:INFO:_display_container: 2
2023-03-20 14:56:01,461:INFO:PassiveAggressiveRegressor(random_state=666)
2023-03-20 14:56:01,462:INFO:create_model() successfully completed......................................
2023-03-20 14:56:01,976:INFO:SubProcess create_model() end ==================================
2023-03-20 14:56:01,976:INFO:Creating metrics dataframe
2023-03-20 14:56:01,990:INFO:Initializing Huber Regressor
2023-03-20 14:56:01,990:INFO:Total runtime is 1.032634417215983 minutes
2023-03-20 14:56:01,995:INFO:SubProcess create_model() called ==================================
2023-03-20 14:56:01,995:INFO:Initializing create_model()
2023-03-20 14:56:01,996:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:56:01,996:INFO:Checking exceptions
2023-03-20 14:56:01,996:INFO:Importing libraries
2023-03-20 14:56:01,996:INFO:Copying training dataset
2023-03-20 14:56:02,006:INFO:Defining folds
2023-03-20 14:56:02,006:INFO:Declaring metric variables
2023-03-20 14:56:02,010:INFO:Importing untrained model
2023-03-20 14:56:02,015:INFO:Huber Regressor Imported successfully
2023-03-20 14:56:02,022:INFO:Starting cross validation
2023-03-20 14:56:02,026:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:56:03,452:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-20 14:56:03,606:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-20 14:56:03,657:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-20 14:56:03,671:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-20 14:56:03,706:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-20 14:56:03,708:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-20 14:56:03,797:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-20 14:56:03,829:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-20 14:56:05,127:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:05,142:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:05,167:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:05,298:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.99s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:05,354:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:05,378:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:05,466:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:06,266:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-20 14:56:06,282:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-20 14:56:07,034:INFO:Calculating mean and std
2023-03-20 14:56:07,035:INFO:Creating metrics dataframe
2023-03-20 14:56:07,122:INFO:Uploading results into container
2023-03-20 14:56:07,123:INFO:Uploading model into container now
2023-03-20 14:56:07,123:INFO:_master_model_container: 10
2023-03-20 14:56:07,123:INFO:_display_container: 2
2023-03-20 14:56:07,123:INFO:HuberRegressor()
2023-03-20 14:56:07,123:INFO:create_model() successfully completed......................................
2023-03-20 14:56:07,636:INFO:SubProcess create_model() end ==================================
2023-03-20 14:56:07,636:INFO:Creating metrics dataframe
2023-03-20 14:56:07,649:INFO:Initializing K Neighbors Regressor
2023-03-20 14:56:07,649:INFO:Total runtime is 1.1269569635391234 minutes
2023-03-20 14:56:07,653:INFO:SubProcess create_model() called ==================================
2023-03-20 14:56:07,653:INFO:Initializing create_model()
2023-03-20 14:56:07,653:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:56:07,653:INFO:Checking exceptions
2023-03-20 14:56:07,653:INFO:Importing libraries
2023-03-20 14:56:07,653:INFO:Copying training dataset
2023-03-20 14:56:07,661:INFO:Defining folds
2023-03-20 14:56:07,661:INFO:Declaring metric variables
2023-03-20 14:56:07,666:INFO:Importing untrained model
2023-03-20 14:56:07,670:INFO:K Neighbors Regressor Imported successfully
2023-03-20 14:56:07,677:INFO:Starting cross validation
2023-03-20 14:56:07,681:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:56:08,688:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-20 14:56:09,826:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:09,853:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:09,927:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:09,950:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.14s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:10,029:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.17s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:10,145:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.23s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:10,204:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.20s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:10,644:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:11,645:INFO:Calculating mean and std
2023-03-20 14:56:11,646:INFO:Creating metrics dataframe
2023-03-20 14:56:11,736:INFO:Uploading results into container
2023-03-20 14:56:11,737:INFO:Uploading model into container now
2023-03-20 14:56:11,737:INFO:_master_model_container: 11
2023-03-20 14:56:11,737:INFO:_display_container: 2
2023-03-20 14:56:11,738:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-20 14:56:11,738:INFO:create_model() successfully completed......................................
2023-03-20 14:56:12,228:INFO:SubProcess create_model() end ==================================
2023-03-20 14:56:12,229:INFO:Creating metrics dataframe
2023-03-20 14:56:12,561:INFO:Initializing Decision Tree Regressor
2023-03-20 14:56:12,612:INFO:Total runtime is 1.2096669634183248 minutes
2023-03-20 14:56:12,616:INFO:SubProcess create_model() called ==================================
2023-03-20 14:56:12,616:INFO:Initializing create_model()
2023-03-20 14:56:12,616:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:56:12,616:INFO:Checking exceptions
2023-03-20 14:56:12,617:INFO:Importing libraries
2023-03-20 14:56:12,617:INFO:Copying training dataset
2023-03-20 14:56:12,624:INFO:Defining folds
2023-03-20 14:56:12,624:INFO:Declaring metric variables
2023-03-20 14:56:12,628:INFO:Importing untrained model
2023-03-20 14:56:12,632:INFO:Decision Tree Regressor Imported successfully
2023-03-20 14:56:12,640:INFO:Starting cross validation
2023-03-20 14:56:12,643:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:56:14,821:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:14,916:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.14s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:15,103:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.24s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:15,147:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.09s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:15,293:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:15,620:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:15,664:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:15,718:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.17s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:16,723:INFO:Calculating mean and std
2023-03-20 14:56:16,725:INFO:Creating metrics dataframe
2023-03-20 14:56:16,829:INFO:Uploading results into container
2023-03-20 14:56:16,831:INFO:Uploading model into container now
2023-03-20 14:56:16,831:INFO:_master_model_container: 12
2023-03-20 14:56:16,831:INFO:_display_container: 2
2023-03-20 14:56:16,832:INFO:DecisionTreeRegressor(random_state=666)
2023-03-20 14:56:16,832:INFO:create_model() successfully completed......................................
2023-03-20 14:56:17,349:INFO:SubProcess create_model() end ==================================
2023-03-20 14:56:17,349:INFO:Creating metrics dataframe
2023-03-20 14:56:17,363:INFO:Initializing Random Forest Regressor
2023-03-20 14:56:17,363:INFO:Total runtime is 1.2888520956039429 minutes
2023-03-20 14:56:17,367:INFO:SubProcess create_model() called ==================================
2023-03-20 14:56:17,367:INFO:Initializing create_model()
2023-03-20 14:56:17,367:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:56:17,368:INFO:Checking exceptions
2023-03-20 14:56:17,368:INFO:Importing libraries
2023-03-20 14:56:17,368:INFO:Copying training dataset
2023-03-20 14:56:17,375:INFO:Defining folds
2023-03-20 14:56:17,376:INFO:Declaring metric variables
2023-03-20 14:56:17,380:INFO:Importing untrained model
2023-03-20 14:56:17,385:INFO:Random Forest Regressor Imported successfully
2023-03-20 14:56:17,393:INFO:Starting cross validation
2023-03-20 14:56:17,396:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:56:24,265:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-20 14:56:26,313:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:26,397:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:26,697:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:26,697:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.25s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:26,703:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.35s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:26,789:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:27,099:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:30,296:INFO:Calculating mean and std
2023-03-20 14:56:30,299:INFO:Creating metrics dataframe
2023-03-20 14:56:30,522:INFO:Uploading results into container
2023-03-20 14:56:30,524:INFO:Uploading model into container now
2023-03-20 14:56:30,524:INFO:_master_model_container: 13
2023-03-20 14:56:30,525:INFO:_display_container: 2
2023-03-20 14:56:30,526:INFO:RandomForestRegressor(n_jobs=-1, random_state=666)
2023-03-20 14:56:30,526:INFO:create_model() successfully completed......................................
2023-03-20 14:56:31,198:INFO:SubProcess create_model() end ==================================
2023-03-20 14:56:31,198:INFO:Creating metrics dataframe
2023-03-20 14:56:31,214:INFO:Initializing Extra Trees Regressor
2023-03-20 14:56:31,214:INFO:Total runtime is 1.5197047114372253 minutes
2023-03-20 14:56:31,219:INFO:SubProcess create_model() called ==================================
2023-03-20 14:56:31,220:INFO:Initializing create_model()
2023-03-20 14:56:31,220:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:56:31,220:INFO:Checking exceptions
2023-03-20 14:56:31,220:INFO:Importing libraries
2023-03-20 14:56:31,220:INFO:Copying training dataset
2023-03-20 14:56:31,231:INFO:Defining folds
2023-03-20 14:56:31,232:INFO:Declaring metric variables
2023-03-20 14:56:31,238:INFO:Importing untrained model
2023-03-20 14:56:31,243:INFO:Extra Trees Regressor Imported successfully
2023-03-20 14:56:31,255:INFO:Starting cross validation
2023-03-20 14:56:31,259:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:56:32,343:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-20 14:56:32,487:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-20 14:56:32,521:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-20 14:56:33,636:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-20 14:56:35,883:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-20 14:56:37,610:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.30s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-20 14:56:37,646:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.30s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-20 14:56:37,879:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-20 14:56:38,188:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.26s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-20 14:56:39,971:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:40,223:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.23s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:40,348:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.23s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:40,408:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.33s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:40,841:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.39s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:41,032:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.30s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:41,133:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.37s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:41,190:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.42s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:41,898:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-03-20 14:56:44,538:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:44,834:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:44,865:INFO:Calculating mean and std
2023-03-20 14:56:44,867:INFO:Creating metrics dataframe
2023-03-20 14:56:45,015:INFO:Uploading results into container
2023-03-20 14:56:45,016:INFO:Uploading model into container now
2023-03-20 14:56:45,017:INFO:_master_model_container: 14
2023-03-20 14:56:45,017:INFO:_display_container: 2
2023-03-20 14:56:45,017:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=666)
2023-03-20 14:56:45,017:INFO:create_model() successfully completed......................................
2023-03-20 14:56:45,594:INFO:SubProcess create_model() end ==================================
2023-03-20 14:56:45,594:INFO:Creating metrics dataframe
2023-03-20 14:56:45,611:INFO:Initializing AdaBoost Regressor
2023-03-20 14:56:45,664:INFO:Total runtime is 1.7605297485987346 minutes
2023-03-20 14:56:45,669:INFO:SubProcess create_model() called ==================================
2023-03-20 14:56:45,669:INFO:Initializing create_model()
2023-03-20 14:56:45,669:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:56:45,669:INFO:Checking exceptions
2023-03-20 14:56:45,670:INFO:Importing libraries
2023-03-20 14:56:45,670:INFO:Copying training dataset
2023-03-20 14:56:45,677:INFO:Defining folds
2023-03-20 14:56:45,678:INFO:Declaring metric variables
2023-03-20 14:56:45,683:INFO:Importing untrained model
2023-03-20 14:56:45,690:INFO:AdaBoost Regressor Imported successfully
2023-03-20 14:56:45,702:INFO:Starting cross validation
2023-03-20 14:56:45,706:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:56:48,608:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:48,799:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:48,805:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.22s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:48,808:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.14s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:48,918:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.20s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:48,925:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.20s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:49,133:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.18s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:49,661:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:51,132:INFO:Calculating mean and std
2023-03-20 14:56:51,134:INFO:Creating metrics dataframe
2023-03-20 14:56:51,324:INFO:Uploading results into container
2023-03-20 14:56:51,324:INFO:Uploading model into container now
2023-03-20 14:56:51,325:INFO:_master_model_container: 15
2023-03-20 14:56:51,325:INFO:_display_container: 2
2023-03-20 14:56:51,325:INFO:AdaBoostRegressor(random_state=666)
2023-03-20 14:56:51,325:INFO:create_model() successfully completed......................................
2023-03-20 14:56:51,875:INFO:SubProcess create_model() end ==================================
2023-03-20 14:56:51,875:INFO:Creating metrics dataframe
2023-03-20 14:56:51,889:INFO:Initializing Gradient Boosting Regressor
2023-03-20 14:56:51,890:INFO:Total runtime is 1.8643009702364604 minutes
2023-03-20 14:56:51,895:INFO:SubProcess create_model() called ==================================
2023-03-20 14:56:51,896:INFO:Initializing create_model()
2023-03-20 14:56:51,896:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:56:51,896:INFO:Checking exceptions
2023-03-20 14:56:51,896:INFO:Importing libraries
2023-03-20 14:56:51,896:INFO:Copying training dataset
2023-03-20 14:56:51,904:INFO:Defining folds
2023-03-20 14:56:51,904:INFO:Declaring metric variables
2023-03-20 14:56:51,908:INFO:Importing untrained model
2023-03-20 14:56:51,954:INFO:Gradient Boosting Regressor Imported successfully
2023-03-20 14:56:51,961:INFO:Starting cross validation
2023-03-20 14:56:51,964:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:56:56,375:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:56,465:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:56,477:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.21s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:56,515:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:56,559:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.20s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:56,634:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.28s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:56,682:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.27s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:56,745:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.22s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:56:59,054:INFO:Calculating mean and std
2023-03-20 14:56:59,056:INFO:Creating metrics dataframe
2023-03-20 14:56:59,265:INFO:Uploading results into container
2023-03-20 14:56:59,266:INFO:Uploading model into container now
2023-03-20 14:56:59,266:INFO:_master_model_container: 16
2023-03-20 14:56:59,266:INFO:_display_container: 2
2023-03-20 14:56:59,267:INFO:GradientBoostingRegressor(random_state=666)
2023-03-20 14:56:59,267:INFO:create_model() successfully completed......................................
2023-03-20 14:56:59,769:INFO:SubProcess create_model() end ==================================
2023-03-20 14:56:59,769:INFO:Creating metrics dataframe
2023-03-20 14:56:59,786:INFO:Initializing Extreme Gradient Boosting
2023-03-20 14:56:59,786:INFO:Total runtime is 1.9959089716275533 minutes
2023-03-20 14:56:59,791:INFO:SubProcess create_model() called ==================================
2023-03-20 14:56:59,792:INFO:Initializing create_model()
2023-03-20 14:56:59,792:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:56:59,793:INFO:Checking exceptions
2023-03-20 14:56:59,793:INFO:Importing libraries
2023-03-20 14:56:59,793:INFO:Copying training dataset
2023-03-20 14:56:59,801:INFO:Defining folds
2023-03-20 14:56:59,801:INFO:Declaring metric variables
2023-03-20 14:56:59,806:INFO:Importing untrained model
2023-03-20 14:56:59,812:INFO:Extreme Gradient Boosting Imported successfully
2023-03-20 14:56:59,820:INFO:Starting cross validation
2023-03-20 14:56:59,823:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:57:05,903:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-03-20 14:57:06,484:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.15s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:06,512:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:06,545:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:06,567:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:06,576:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.14s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:06,603:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.23s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:07,050:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.14s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:07,235:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:09,591:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:09,646:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:09,744:INFO:Calculating mean and std
2023-03-20 14:57:09,745:INFO:Creating metrics dataframe
2023-03-20 14:57:09,935:INFO:Uploading results into container
2023-03-20 14:57:09,936:INFO:Uploading model into container now
2023-03-20 14:57:09,937:INFO:_master_model_container: 17
2023-03-20 14:57:09,937:INFO:_display_container: 2
2023-03-20 14:57:09,938:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=666, ...)
2023-03-20 14:57:09,938:INFO:create_model() successfully completed......................................
2023-03-20 14:57:10,503:INFO:SubProcess create_model() end ==================================
2023-03-20 14:57:10,504:INFO:Creating metrics dataframe
2023-03-20 14:57:10,602:INFO:Initializing Light Gradient Boosting Machine
2023-03-20 14:57:10,603:INFO:Total runtime is 2.1761809905370075 minutes
2023-03-20 14:57:10,607:INFO:SubProcess create_model() called ==================================
2023-03-20 14:57:10,608:INFO:Initializing create_model()
2023-03-20 14:57:10,608:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:57:10,608:INFO:Checking exceptions
2023-03-20 14:57:10,608:INFO:Importing libraries
2023-03-20 14:57:10,608:INFO:Copying training dataset
2023-03-20 14:57:10,616:INFO:Defining folds
2023-03-20 14:57:10,616:INFO:Declaring metric variables
2023-03-20 14:57:10,620:INFO:Importing untrained model
2023-03-20 14:57:10,625:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-20 14:57:10,634:INFO:Starting cross validation
2023-03-20 14:57:10,712:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:57:22,324:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:22,327:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:22,391:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:22,392:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:22,408:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:22,421:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:22,434:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:22,522:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:24,206:INFO:Calculating mean and std
2023-03-20 14:57:24,208:INFO:Creating metrics dataframe
2023-03-20 14:57:24,402:INFO:Uploading results into container
2023-03-20 14:57:24,402:INFO:Uploading model into container now
2023-03-20 14:57:24,403:INFO:_master_model_container: 18
2023-03-20 14:57:24,403:INFO:_display_container: 2
2023-03-20 14:57:24,403:INFO:LGBMRegressor(random_state=666)
2023-03-20 14:57:24,403:INFO:create_model() successfully completed......................................
2023-03-20 14:57:24,915:INFO:SubProcess create_model() end ==================================
2023-03-20 14:57:24,916:INFO:Creating metrics dataframe
2023-03-20 14:57:24,935:INFO:Initializing CatBoost Regressor
2023-03-20 14:57:24,935:INFO:Total runtime is 2.4150516033172607 minutes
2023-03-20 14:57:24,939:INFO:SubProcess create_model() called ==================================
2023-03-20 14:57:24,940:INFO:Initializing create_model()
2023-03-20 14:57:24,940:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:57:24,940:INFO:Checking exceptions
2023-03-20 14:57:24,940:INFO:Importing libraries
2023-03-20 14:57:24,940:INFO:Copying training dataset
2023-03-20 14:57:24,948:INFO:Defining folds
2023-03-20 14:57:24,948:INFO:Declaring metric variables
2023-03-20 14:57:24,953:INFO:Importing untrained model
2023-03-20 14:57:24,997:INFO:CatBoost Regressor Imported successfully
2023-03-20 14:57:25,008:INFO:Starting cross validation
2023-03-20 14:57:25,011:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:57:43,383:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:43,450:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:44,036:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:44,205:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.25s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:44,245:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.25s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:44,269:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.28s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:44,379:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.20s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:46,667:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:50,384:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:50,473:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:50,536:INFO:Calculating mean and std
2023-03-20 14:57:50,538:INFO:Creating metrics dataframe
2023-03-20 14:57:50,744:INFO:Uploading results into container
2023-03-20 14:57:50,745:INFO:Uploading model into container now
2023-03-20 14:57:50,746:INFO:_master_model_container: 19
2023-03-20 14:57:50,746:INFO:_display_container: 2
2023-03-20 14:57:50,746:INFO:<catboost.core.CatBoostRegressor object at 0x000002617923DC40>
2023-03-20 14:57:50,746:INFO:create_model() successfully completed......................................
2023-03-20 14:57:51,282:INFO:SubProcess create_model() end ==================================
2023-03-20 14:57:51,282:INFO:Creating metrics dataframe
2023-03-20 14:57:51,297:INFO:Initializing Dummy Regressor
2023-03-20 14:57:51,298:INFO:Total runtime is 2.8544359326362607 minutes
2023-03-20 14:57:51,302:INFO:SubProcess create_model() called ==================================
2023-03-20 14:57:51,302:INFO:Initializing create_model()
2023-03-20 14:57:51,303:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026179A31040>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:57:51,303:INFO:Checking exceptions
2023-03-20 14:57:51,303:INFO:Importing libraries
2023-03-20 14:57:51,303:INFO:Copying training dataset
2023-03-20 14:57:51,311:INFO:Defining folds
2023-03-20 14:57:51,311:INFO:Declaring metric variables
2023-03-20 14:57:51,315:INFO:Importing untrained model
2023-03-20 14:57:51,319:INFO:Dummy Regressor Imported successfully
2023-03-20 14:57:51,330:INFO:Starting cross validation
2023-03-20 14:57:51,335:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:57:53,727:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:53,875:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:53,953:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:53,961:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:53,964:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:54,020:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.13s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:54,064:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.14s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:54,199:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.23s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:55,444:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:57:55,597:INFO:Calculating mean and std
2023-03-20 14:57:55,599:INFO:Creating metrics dataframe
2023-03-20 14:57:55,803:INFO:Uploading results into container
2023-03-20 14:57:55,804:INFO:Uploading model into container now
2023-03-20 14:57:55,804:INFO:_master_model_container: 20
2023-03-20 14:57:55,804:INFO:_display_container: 2
2023-03-20 14:57:55,804:INFO:DummyRegressor()
2023-03-20 14:57:55,805:INFO:create_model() successfully completed......................................
2023-03-20 14:57:56,306:INFO:SubProcess create_model() end ==================================
2023-03-20 14:57:56,306:INFO:Creating metrics dataframe
2023-03-20 14:57:56,440:INFO:Initializing create_model()
2023-03-20 14:57:56,441:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=GradientBoostingRegressor(random_state=666), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:57:56,441:INFO:Checking exceptions
2023-03-20 14:57:57,021:INFO:Importing libraries
2023-03-20 14:57:57,022:INFO:Copying training dataset
2023-03-20 14:57:57,030:INFO:Defining folds
2023-03-20 14:57:57,030:INFO:Declaring metric variables
2023-03-20 14:57:57,030:INFO:Importing untrained model
2023-03-20 14:57:57,030:INFO:Declaring custom model
2023-03-20 14:57:57,031:INFO:Gradient Boosting Regressor Imported successfully
2023-03-20 14:57:57,033:INFO:Cross validation set to False
2023-03-20 14:57:57,033:INFO:Fitting Model
2023-03-20 14:58:00,578:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:58:00,595:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:58:00,629:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:58:00,657:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pandas\core\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  uniques = Index(uniques)

2023-03-20 14:58:01,904:INFO:GradientBoostingRegressor(random_state=666)
2023-03-20 14:58:01,904:INFO:create_model() successfully completed......................................
2023-03-20 14:58:02,463:INFO:_master_model_container: 20
2023-03-20 14:58:02,463:INFO:_display_container: 2
2023-03-20 14:58:02,463:INFO:GradientBoostingRegressor(random_state=666)
2023-03-20 14:58:02,463:INFO:compare_models() successfully completed......................................
2023-03-20 14:59:22,871:INFO:Initializing compare_models()
2023-03-20 14:59:22,871:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, include=None, fold=None, round=4, cross_validation=True, sort=MSE, n_select=1, budget_time=10, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MSE', 'n_select': 1, 'budget_time': 10, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-20 14:59:22,871:INFO:Checking exceptions
2023-03-20 14:59:22,875:INFO:Preparing display monitor
2023-03-20 14:59:22,904:INFO:Time budget is 10 minutes
2023-03-20 14:59:22,904:INFO:Initializing Linear Regression
2023-03-20 14:59:22,904:INFO:Total runtime is 0.0 minutes
2023-03-20 14:59:22,908:INFO:SubProcess create_model() called ==================================
2023-03-20 14:59:22,908:INFO:Initializing create_model()
2023-03-20 14:59:22,909:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:59:22,909:INFO:Checking exceptions
2023-03-20 14:59:22,909:INFO:Importing libraries
2023-03-20 14:59:22,909:INFO:Copying training dataset
2023-03-20 14:59:22,916:INFO:Defining folds
2023-03-20 14:59:22,917:INFO:Declaring metric variables
2023-03-20 14:59:22,920:INFO:Importing untrained model
2023-03-20 14:59:22,924:INFO:Linear Regression Imported successfully
2023-03-20 14:59:22,932:INFO:Starting cross validation
2023-03-20 14:59:22,936:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:59:24,419:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:24,450:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:24,455:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:24,557:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:24,567:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:24,576:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:24,619:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:24,644:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:25,794:INFO:Calculating mean and std
2023-03-20 14:59:25,795:INFO:Creating metrics dataframe
2023-03-20 14:59:25,980:INFO:Uploading results into container
2023-03-20 14:59:25,981:INFO:Uploading model into container now
2023-03-20 14:59:25,981:INFO:_master_model_container: 21
2023-03-20 14:59:25,981:INFO:_display_container: 3
2023-03-20 14:59:25,982:INFO:LinearRegression(n_jobs=-1)
2023-03-20 14:59:25,982:INFO:create_model() successfully completed......................................
2023-03-20 14:59:26,493:INFO:SubProcess create_model() end ==================================
2023-03-20 14:59:26,494:INFO:Creating metrics dataframe
2023-03-20 14:59:26,503:INFO:Initializing Lasso Regression
2023-03-20 14:59:26,504:INFO:Total runtime is 0.05999913215637207 minutes
2023-03-20 14:59:26,508:INFO:SubProcess create_model() called ==================================
2023-03-20 14:59:26,508:INFO:Initializing create_model()
2023-03-20 14:59:26,508:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:59:26,508:INFO:Checking exceptions
2023-03-20 14:59:26,508:INFO:Importing libraries
2023-03-20 14:59:26,509:INFO:Copying training dataset
2023-03-20 14:59:26,516:INFO:Defining folds
2023-03-20 14:59:26,516:INFO:Declaring metric variables
2023-03-20 14:59:26,520:INFO:Importing untrained model
2023-03-20 14:59:26,525:INFO:Lasso Regression Imported successfully
2023-03-20 14:59:26,533:INFO:Starting cross validation
2023-03-20 14:59:26,536:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:59:28,061:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:28,074:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:28,150:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:28,194:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:28,209:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:28,240:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:28,357:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.87s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:28,390:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:29,451:INFO:Calculating mean and std
2023-03-20 14:59:29,452:INFO:Creating metrics dataframe
2023-03-20 14:59:29,632:INFO:Uploading results into container
2023-03-20 14:59:29,633:INFO:Uploading model into container now
2023-03-20 14:59:29,633:INFO:_master_model_container: 22
2023-03-20 14:59:29,633:INFO:_display_container: 3
2023-03-20 14:59:29,633:INFO:Lasso(random_state=666)
2023-03-20 14:59:29,634:INFO:create_model() successfully completed......................................
2023-03-20 14:59:30,110:INFO:SubProcess create_model() end ==================================
2023-03-20 14:59:30,110:INFO:Creating metrics dataframe
2023-03-20 14:59:30,121:INFO:Initializing Ridge Regression
2023-03-20 14:59:30,121:INFO:Total runtime is 0.12028402884801229 minutes
2023-03-20 14:59:30,125:INFO:SubProcess create_model() called ==================================
2023-03-20 14:59:30,125:INFO:Initializing create_model()
2023-03-20 14:59:30,125:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:59:30,126:INFO:Checking exceptions
2023-03-20 14:59:30,126:INFO:Importing libraries
2023-03-20 14:59:30,126:INFO:Copying training dataset
2023-03-20 14:59:30,136:INFO:Defining folds
2023-03-20 14:59:30,136:INFO:Declaring metric variables
2023-03-20 14:59:30,140:INFO:Importing untrained model
2023-03-20 14:59:30,144:INFO:Ridge Regression Imported successfully
2023-03-20 14:59:30,153:INFO:Starting cross validation
2023-03-20 14:59:30,155:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:59:31,623:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:31,649:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:31,686:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:31,703:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:31,766:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:31,783:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:31,857:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:31,930:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:33,097:INFO:Calculating mean and std
2023-03-20 14:59:33,100:INFO:Creating metrics dataframe
2023-03-20 14:59:33,378:INFO:Uploading results into container
2023-03-20 14:59:33,379:INFO:Uploading model into container now
2023-03-20 14:59:33,379:INFO:_master_model_container: 23
2023-03-20 14:59:33,379:INFO:_display_container: 3
2023-03-20 14:59:33,380:INFO:Ridge(random_state=666)
2023-03-20 14:59:33,380:INFO:create_model() successfully completed......................................
2023-03-20 14:59:33,904:INFO:SubProcess create_model() end ==================================
2023-03-20 14:59:33,905:INFO:Creating metrics dataframe
2023-03-20 14:59:33,917:INFO:Initializing Elastic Net
2023-03-20 14:59:33,918:INFO:Total runtime is 0.18357063531875611 minutes
2023-03-20 14:59:33,922:INFO:SubProcess create_model() called ==================================
2023-03-20 14:59:33,922:INFO:Initializing create_model()
2023-03-20 14:59:33,922:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:59:33,922:INFO:Checking exceptions
2023-03-20 14:59:33,922:INFO:Importing libraries
2023-03-20 14:59:33,922:INFO:Copying training dataset
2023-03-20 14:59:33,933:INFO:Defining folds
2023-03-20 14:59:33,933:INFO:Declaring metric variables
2023-03-20 14:59:33,937:INFO:Importing untrained model
2023-03-20 14:59:33,942:INFO:Elastic Net Imported successfully
2023-03-20 14:59:33,951:INFO:Starting cross validation
2023-03-20 14:59:33,956:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:59:35,475:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:35,566:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:35,587:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:35,650:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.83s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:35,688:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:36,438:INFO:Calculating mean and std
2023-03-20 14:59:36,439:INFO:Creating metrics dataframe
2023-03-20 14:59:36,631:INFO:Uploading results into container
2023-03-20 14:59:36,631:INFO:Uploading model into container now
2023-03-20 14:59:36,632:INFO:_master_model_container: 24
2023-03-20 14:59:36,632:INFO:_display_container: 3
2023-03-20 14:59:36,633:INFO:ElasticNet(random_state=666)
2023-03-20 14:59:36,633:INFO:create_model() successfully completed......................................
2023-03-20 14:59:37,116:INFO:SubProcess create_model() end ==================================
2023-03-20 14:59:37,117:INFO:Creating metrics dataframe
2023-03-20 14:59:37,130:INFO:Initializing Least Angle Regression
2023-03-20 14:59:37,130:INFO:Total runtime is 0.23710215886433922 minutes
2023-03-20 14:59:37,135:INFO:SubProcess create_model() called ==================================
2023-03-20 14:59:37,135:INFO:Initializing create_model()
2023-03-20 14:59:37,135:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:59:37,135:INFO:Checking exceptions
2023-03-20 14:59:37,136:INFO:Importing libraries
2023-03-20 14:59:37,136:INFO:Copying training dataset
2023-03-20 14:59:37,147:INFO:Defining folds
2023-03-20 14:59:37,147:INFO:Declaring metric variables
2023-03-20 14:59:37,152:INFO:Importing untrained model
2023-03-20 14:59:37,157:INFO:Least Angle Regression Imported successfully
2023-03-20 14:59:37,167:INFO:Starting cross validation
2023-03-20 14:59:37,170:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:59:37,698:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:37,702:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:37,709:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.397e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,709:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.540e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,710:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.711e-01, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,711:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.541e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,712:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,712:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,712:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,713:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,713:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,714:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.976e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,714:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,714:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.457e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,714:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,714:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.297e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,714:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,715:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,715:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,716:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.470e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,716:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.656e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,716:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.997e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,717:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.997e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,717:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.194e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,717:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.194e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,717:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.829e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,718:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.050e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,718:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,718:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.553e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,718:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,718:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.504e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,718:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,719:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.332e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,719:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,719:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.244e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,719:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,719:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.122e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,719:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.915e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,720:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.784e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,720:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.867e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,720:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.458e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,720:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.867e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,720:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.403e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,720:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.867e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,720:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.181e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,721:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.069e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,721:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.867e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,721:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.767e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,721:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.715e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,721:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.130e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,721:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.646e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,721:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.197e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,722:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.084e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,722:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.809e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,722:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.076e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,722:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.058e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,722:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.847e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,722:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.907e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,722:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.576e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,723:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.533e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,723:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.122e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,723:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.328e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,723:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,723:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.083e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,724:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,724:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,724:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.072e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,724:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.611e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,724:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,724:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.123e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,725:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,725:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.651e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,726:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.317e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,726:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.621e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,726:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.206e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,726:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.054e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,726:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.581e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,726:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.539e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,726:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.529e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,727:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.031e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,727:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.151e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,727:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.528e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,727:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.480e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,727:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.524e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,727:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.726e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,727:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.524e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,728:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.525e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,728:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.522e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,728:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.147e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,728:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.521e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,728:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,728:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.508e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,728:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,728:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.471e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,729:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,729:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.346e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,729:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,729:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.573e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,729:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,729:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.290e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,729:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.908e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,730:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,730:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.908e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,730:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.865e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,730:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.061e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,730:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.990e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,730:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.626e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,731:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.421e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,745:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:37,755:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:37,763:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.759e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,764:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,764:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,765:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.280e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,765:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,765:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.647e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.093e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,765:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.034e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,766:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.670e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,766:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.602e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,767:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.140e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,768:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.258e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,768:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.258e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,770:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.729e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,771:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.729e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,771:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.729e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,772:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.429e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,767:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.140e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,773:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.140e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,773:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.084e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,774:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.949e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,774:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.631e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,774:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.631e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,774:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.539e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,775:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.471e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,775:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.130e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,775:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.467e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,776:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.975e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,777:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.822e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,777:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.892e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,777:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.730e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,777:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.575e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,777:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:37,778:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.328e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,777:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.842e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,778:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.274e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,778:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.820e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,778:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.022e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,778:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.565e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,778:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=8.154e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,779:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.678e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,779:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,779:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.524e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,779:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.187e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,780:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.524e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,780:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.186e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,780:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.106e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,780:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.220e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,780:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.280e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,781:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.218e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,781:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.157e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,781:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.171e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,781:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.709e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,782:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:37,782:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.709e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,782:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.980e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,782:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.569e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,782:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.926e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,783:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=8.932e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,783:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.273e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,783:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.771e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,783:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.177e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,784:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.661e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,784:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.909e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,785:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.598e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,785:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.280e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,785:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.356e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,785:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.110e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,785:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.819e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,785:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=5.728e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,785:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.900e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,786:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.523e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,786:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.523e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,786:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.523e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,786:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.661e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,787:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.368e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,787:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.762e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,787:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.420e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,787:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.745e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,787:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.718e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,787:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.015e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,788:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.705e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,788:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.511e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,788:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,788:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.273e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,789:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,789:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,789:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.925e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,789:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,789:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.020e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,790:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.954e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,790:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.484e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,790:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.400e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,790:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.597e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,790:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.399e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,791:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.778e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,791:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.238e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,791:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.778e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,791:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.185e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,791:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.774e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,791:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,791:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.051e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,791:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.704e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,791:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,791:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=6.808e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,792:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.582e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,792:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.440e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,792:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.634e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,792:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.592e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,792:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.299e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,792:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.440e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,792:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.072e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,792:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.334e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,793:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.467e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,793:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.310e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,793:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.031e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,793:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.183e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,794:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.980e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,794:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.880e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,794:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.108e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,794:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.599e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,794:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.770e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,794:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.042e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,794:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.599e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,794:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.107e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,795:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.280e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,795:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.877e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,795:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.048e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,795:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.784e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,795:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.859e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,795:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.754e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,795:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.875e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,795:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.940e-01, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,795:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.857e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,796:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.651e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,796:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.851e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,796:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.636e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,796:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.761e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,796:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.839e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,796:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.761e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,797:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.831e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,797:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.629e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,797:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.761e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,797:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.267e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,797:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.363e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,797:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.352e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,798:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.905e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,798:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.905e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,798:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:37,799:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.905e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,799:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.905e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,800:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.372e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,800:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.338e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,801:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.026e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,802:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.655e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,803:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.655e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,803:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.825e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,803:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.738e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,804:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.544e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,804:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.388e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,804:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.602e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,804:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.299e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,805:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.276e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,805:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.263e+00, with an active set of 39 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,805:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.164e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,806:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,806:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.052e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,806:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,806:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.041e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,806:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.954e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,806:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.218e+00, with an active set of 40 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,807:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.611e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,807:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.066e+00, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,807:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.285e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,807:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=9.946e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,807:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.285e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,807:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.257e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,808:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.285e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,808:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.061e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,808:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.285e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,808:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=5.825e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,808:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.778e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,808:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=5.450e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,808:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.137e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,809:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.049e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,809:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.927e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,810:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.503e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,809:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.382e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,810:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.927e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,810:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.400e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,810:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.925e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,811:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.369e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,811:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.968e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,811:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.039e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,811:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.138e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,812:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=2.916e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,812:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=3.529e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,812:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.138e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,812:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.802e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,812:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.138e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,812:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.511e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,813:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.031e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,813:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.872e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,813:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.013e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,813:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.213e-01, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,813:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.969e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,813:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=9.021e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,813:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.274e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,813:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.087e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,814:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.274e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,814:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.087e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,814:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.588e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,814:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.403e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,814:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.876e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,814:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.022e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,815:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.132e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,815:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.374e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,815:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.372e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,815:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.298e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,815:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.053e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,815:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.216e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,816:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,816:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.116e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,816:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,816:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.013e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,816:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,816:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.955e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,817:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,817:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,817:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=6.365e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,817:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.457e-01, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,817:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.616e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,818:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.585e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,818:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.309e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,818:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.654e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,818:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.098e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,818:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.820e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,819:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.098e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,819:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.321e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,819:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.167e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,819:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.007e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,820:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.748e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,820:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.470e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,820:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.157e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,820:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.882e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,820:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.565e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,821:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.217e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,821:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.523e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,821:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.661e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,821:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.093e-06, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,822:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.238e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,823:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.238e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,823:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.221e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,823:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.953e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,824:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.565e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,824:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.563e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,825:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.414e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,826:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.119e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,826:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.677e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,827:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.087e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,827:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.829e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,827:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.826e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,828:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.809e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,828:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.659e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,828:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.474e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,828:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.615e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,829:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.042e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,829:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.869e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,829:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.418e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,829:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.366e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,830:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.361e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,830:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.373e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,830:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.085e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,830:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.928e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,831:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.615e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,831:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.387e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,831:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.460e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,862:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:37,870:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.088e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,870:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.088e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,871:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.258e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,872:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.258e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,872:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.258e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,873:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.526e-01, with an active set of 14 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,874:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.886e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,875:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.036e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,876:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.036e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,876:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.752e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,877:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.752e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,877:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.386e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,878:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.081e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,878:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.081e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,879:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.938e-01, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,879:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.938e-01, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,880:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.875e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,880:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.788e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,880:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.780e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,880:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.444e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,881:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.345e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,881:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.253e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,882:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.017e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,882:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=9.442e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,882:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.124e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,882:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.124e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,883:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.124e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,883:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=7.706e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,884:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.758e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,884:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.732e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,884:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.356e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,885:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.336e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,885:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.988e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,885:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.988e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,885:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.467e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,886:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.718e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,886:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,887:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,887:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,887:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,888:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,888:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.546e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,888:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.487e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,888:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.411e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,889:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.353e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,889:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=7.983e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,889:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.096e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,890:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.096e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,890:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.096e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,891:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.780e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,891:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.944e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,892:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.745e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,893:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.307e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,893:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,894:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,894:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,894:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,895:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,895:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,895:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.118e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,896:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.473e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,896:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.015e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,896:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.755e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,897:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.925e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,897:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.825e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,897:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.308e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,897:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.004e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,898:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.098e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,899:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.917e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,899:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.892e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,899:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.875e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,899:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.843e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,900:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.834e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,900:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.830e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,900:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.829e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,901:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.827e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,901:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.819e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,901:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.818e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,901:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.778e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,902:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.043e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,902:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.091e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,902:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.832e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,902:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.894e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,903:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.630e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,903:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.417e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:37,903:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.161e-06, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,586:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:38,596:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.459e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,600:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,601:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,601:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,602:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,602:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,603:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,604:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.692e-01, with an active set of 22 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,605:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,606:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,607:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,607:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,608:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,608:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.023e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,608:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.007e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,609:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.215e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

odel = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:38,610:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.215e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,610:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.389e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,611:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.358e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,611:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.811e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,611:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.362e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,611:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.291e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,611:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,612:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,612:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,612:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,612:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,612:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,613:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.436e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,613:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.809e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,613:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.426e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,613:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.037e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.814e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,614:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.539e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,614:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.396e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,614:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.849e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,614:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.625e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,614:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.615e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,614:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.609e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,614:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.592e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,615:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.576e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,615:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.040e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,615:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.603e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,615:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.419e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,615:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.015e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,616:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.971e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,616:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.146e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,616:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.780e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,616:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.011e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,616:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.362e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,621:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,622:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,622:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,622:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,623:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,623:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,624:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.184e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,625:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.184e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,625:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.184e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,625:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.096e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,626:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.551e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,626:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.776e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,626:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.308e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,627:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.258e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,627:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.258e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,627:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.258e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,628:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.258e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,628:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.258e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,628:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.226e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,629:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.114e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,629:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.259e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,629:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.162e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,630:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.126e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,630:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=8.163e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,630:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.786e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,630:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.786e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,631:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.786e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,631:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=5.427e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,632:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.917e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,632:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.834e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,632:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.785e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,633:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,633:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,633:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,634:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,634:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,635:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,636:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.308e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,636:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.572e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,636:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.160e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,636:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.768e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,637:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.630e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,637:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.535e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,637:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.534e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,638:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.534e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,638:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.524e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,638:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.503e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,638:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.502e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,639:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.475e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,639:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.332e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,639:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.937e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,639:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.835e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,640:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.435e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,640:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.769e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,640:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.156e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,641:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.739e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,641:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.234e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:38,764:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:38,792:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:38,821:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:38,904:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:39,539:INFO:Calculating mean and std
2023-03-20 14:59:39,541:INFO:Creating metrics dataframe
2023-03-20 14:59:39,738:INFO:Uploading results into container
2023-03-20 14:59:39,739:INFO:Uploading model into container now
2023-03-20 14:59:39,740:INFO:_master_model_container: 25
2023-03-20 14:59:39,740:INFO:_display_container: 3
2023-03-20 14:59:39,740:INFO:Lars(random_state=666)
2023-03-20 14:59:39,740:INFO:create_model() successfully completed......................................
2023-03-20 14:59:40,265:INFO:SubProcess create_model() end ==================================
2023-03-20 14:59:40,265:INFO:Creating metrics dataframe
2023-03-20 14:59:40,277:INFO:Initializing Lasso Least Angle Regression
2023-03-20 14:59:40,277:INFO:Total runtime is 0.28954603274663293 minutes
2023-03-20 14:59:40,282:INFO:SubProcess create_model() called ==================================
2023-03-20 14:59:40,283:INFO:Initializing create_model()
2023-03-20 14:59:40,283:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:59:40,283:INFO:Checking exceptions
2023-03-20 14:59:40,283:INFO:Importing libraries
2023-03-20 14:59:40,283:INFO:Copying training dataset
2023-03-20 14:59:40,292:INFO:Defining folds
2023-03-20 14:59:40,293:INFO:Declaring metric variables
2023-03-20 14:59:40,298:INFO:Importing untrained model
2023-03-20 14:59:40,303:INFO:Lasso Least Angle Regression Imported successfully
2023-03-20 14:59:40,315:INFO:Starting cross validation
2023-03-20 14:59:40,318:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:59:40,909:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:59:40,925:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:59:40,959:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:59:40,967:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.420e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:40,968:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:40,969:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:40,969:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:40,969:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:40,970:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.020e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:40,981:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:59:41,018:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:59:41,026:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:41,026:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:41,026:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:41,051:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:59:41,065:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.759e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:41,066:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:41,067:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:41,067:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:41,079:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:59:41,163:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:59:41,171:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:41,171:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:41,171:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-20 14:59:41,719:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:59:41,789:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-20 14:59:41,925:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:41,998:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:42,620:INFO:Calculating mean and std
2023-03-20 14:59:42,621:INFO:Creating metrics dataframe
2023-03-20 14:59:42,813:INFO:Uploading results into container
2023-03-20 14:59:42,813:INFO:Uploading model into container now
2023-03-20 14:59:42,814:INFO:_master_model_container: 26
2023-03-20 14:59:42,814:INFO:_display_container: 3
2023-03-20 14:59:42,814:INFO:LassoLars(random_state=666)
2023-03-20 14:59:42,814:INFO:create_model() successfully completed......................................
2023-03-20 14:59:43,299:INFO:SubProcess create_model() end ==================================
2023-03-20 14:59:43,299:INFO:Creating metrics dataframe
2023-03-20 14:59:43,311:INFO:Initializing Orthogonal Matching Pursuit
2023-03-20 14:59:43,312:INFO:Total runtime is 0.34013777971267706 minutes
2023-03-20 14:59:43,316:INFO:SubProcess create_model() called ==================================
2023-03-20 14:59:43,316:INFO:Initializing create_model()
2023-03-20 14:59:43,316:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:59:43,316:INFO:Checking exceptions
2023-03-20 14:59:43,317:INFO:Importing libraries
2023-03-20 14:59:43,317:INFO:Copying training dataset
2023-03-20 14:59:43,325:INFO:Defining folds
2023-03-20 14:59:43,325:INFO:Declaring metric variables
2023-03-20 14:59:43,330:INFO:Importing untrained model
2023-03-20 14:59:43,334:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-20 14:59:43,343:INFO:Starting cross validation
2023-03-20 14:59:43,347:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:59:43,860:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:43,889:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:43,900:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:43,909:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:43,935:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:43,973:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:43,985:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:44,059:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:44,657:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:44,700:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-20 14:59:44,883:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:44,915:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:45,445:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:45,964:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.06s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:45,969:INFO:Calculating mean and std
2023-03-20 14:59:45,970:INFO:Creating metrics dataframe
2023-03-20 14:59:46,184:INFO:Uploading results into container
2023-03-20 14:59:46,185:INFO:Uploading model into container now
2023-03-20 14:59:46,186:INFO:_master_model_container: 27
2023-03-20 14:59:46,186:INFO:_display_container: 3
2023-03-20 14:59:46,186:INFO:OrthogonalMatchingPursuit()
2023-03-20 14:59:46,187:INFO:create_model() successfully completed......................................
2023-03-20 14:59:46,654:INFO:SubProcess create_model() end ==================================
2023-03-20 14:59:46,654:INFO:Creating metrics dataframe
2023-03-20 14:59:46,665:INFO:Initializing Bayesian Ridge
2023-03-20 14:59:46,665:INFO:Total runtime is 0.39600933790206916 minutes
2023-03-20 14:59:46,669:INFO:SubProcess create_model() called ==================================
2023-03-20 14:59:46,669:INFO:Initializing create_model()
2023-03-20 14:59:46,669:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:59:46,669:INFO:Checking exceptions
2023-03-20 14:59:46,670:INFO:Importing libraries
2023-03-20 14:59:46,670:INFO:Copying training dataset
2023-03-20 14:59:46,677:INFO:Defining folds
2023-03-20 14:59:46,677:INFO:Declaring metric variables
2023-03-20 14:59:46,682:INFO:Importing untrained model
2023-03-20 14:59:46,686:INFO:Bayesian Ridge Imported successfully
2023-03-20 14:59:46,694:INFO:Starting cross validation
2023-03-20 14:59:46,697:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:59:48,181:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:48,214:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 14:59:48,848:INFO:Calculating mean and std
2023-03-20 14:59:48,850:INFO:Creating metrics dataframe
2023-03-20 14:59:49,042:INFO:Uploading results into container
2023-03-20 14:59:49,042:INFO:Uploading model into container now
2023-03-20 14:59:49,043:INFO:_master_model_container: 28
2023-03-20 14:59:49,043:INFO:_display_container: 3
2023-03-20 14:59:49,043:INFO:BayesianRidge()
2023-03-20 14:59:49,043:INFO:create_model() successfully completed......................................
2023-03-20 14:59:49,521:INFO:SubProcess create_model() end ==================================
2023-03-20 14:59:49,521:INFO:Creating metrics dataframe
2023-03-20 14:59:49,551:INFO:Initializing Passive Aggressive Regressor
2023-03-20 14:59:49,552:INFO:Total runtime is 0.444124988714854 minutes
2023-03-20 14:59:49,556:INFO:SubProcess create_model() called ==================================
2023-03-20 14:59:49,557:INFO:Initializing create_model()
2023-03-20 14:59:49,557:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:59:49,557:INFO:Checking exceptions
2023-03-20 14:59:49,557:INFO:Importing libraries
2023-03-20 14:59:49,557:INFO:Copying training dataset
2023-03-20 14:59:49,566:INFO:Defining folds
2023-03-20 14:59:49,566:INFO:Declaring metric variables
2023-03-20 14:59:49,569:INFO:Importing untrained model
2023-03-20 14:59:49,573:INFO:Passive Aggressive Regressor Imported successfully
2023-03-20 14:59:49,581:INFO:Starting cross validation
2023-03-20 14:59:49,584:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:59:51,596:INFO:Calculating mean and std
2023-03-20 14:59:51,597:INFO:Creating metrics dataframe
2023-03-20 14:59:51,797:INFO:Uploading results into container
2023-03-20 14:59:51,797:INFO:Uploading model into container now
2023-03-20 14:59:51,798:INFO:_master_model_container: 29
2023-03-20 14:59:51,798:INFO:_display_container: 3
2023-03-20 14:59:51,798:INFO:PassiveAggressiveRegressor(random_state=666)
2023-03-20 14:59:51,798:INFO:create_model() successfully completed......................................
2023-03-20 14:59:52,275:INFO:SubProcess create_model() end ==================================
2023-03-20 14:59:52,275:INFO:Creating metrics dataframe
2023-03-20 14:59:52,288:INFO:Initializing Huber Regressor
2023-03-20 14:59:52,288:INFO:Total runtime is 0.4897296388943991 minutes
2023-03-20 14:59:52,293:INFO:SubProcess create_model() called ==================================
2023-03-20 14:59:52,293:INFO:Initializing create_model()
2023-03-20 14:59:52,293:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:59:52,293:INFO:Checking exceptions
2023-03-20 14:59:52,293:INFO:Importing libraries
2023-03-20 14:59:52,293:INFO:Copying training dataset
2023-03-20 14:59:52,301:INFO:Defining folds
2023-03-20 14:59:52,301:INFO:Declaring metric variables
2023-03-20 14:59:52,305:INFO:Importing untrained model
2023-03-20 14:59:52,309:INFO:Huber Regressor Imported successfully
2023-03-20 14:59:52,316:INFO:Starting cross validation
2023-03-20 14:59:52,319:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:59:54,527:INFO:Calculating mean and std
2023-03-20 14:59:54,528:INFO:Creating metrics dataframe
2023-03-20 14:59:54,732:INFO:Uploading results into container
2023-03-20 14:59:54,733:INFO:Uploading model into container now
2023-03-20 14:59:54,734:INFO:_master_model_container: 30
2023-03-20 14:59:54,734:INFO:_display_container: 3
2023-03-20 14:59:54,734:INFO:HuberRegressor()
2023-03-20 14:59:54,735:INFO:create_model() successfully completed......................................
2023-03-20 14:59:55,212:INFO:SubProcess create_model() end ==================================
2023-03-20 14:59:55,212:INFO:Creating metrics dataframe
2023-03-20 14:59:55,224:INFO:Initializing K Neighbors Regressor
2023-03-20 14:59:55,224:INFO:Total runtime is 0.538669526576996 minutes
2023-03-20 14:59:55,228:INFO:SubProcess create_model() called ==================================
2023-03-20 14:59:55,229:INFO:Initializing create_model()
2023-03-20 14:59:55,229:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:59:55,229:INFO:Checking exceptions
2023-03-20 14:59:55,229:INFO:Importing libraries
2023-03-20 14:59:55,229:INFO:Copying training dataset
2023-03-20 14:59:55,238:INFO:Defining folds
2023-03-20 14:59:55,238:INFO:Declaring metric variables
2023-03-20 14:59:55,242:INFO:Importing untrained model
2023-03-20 14:59:55,246:INFO:K Neighbors Regressor Imported successfully
2023-03-20 14:59:55,256:INFO:Starting cross validation
2023-03-20 14:59:55,260:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 14:59:57,383:INFO:Calculating mean and std
2023-03-20 14:59:57,662:INFO:Creating metrics dataframe
2023-03-20 14:59:57,802:INFO:Uploading results into container
2023-03-20 14:59:57,803:INFO:Uploading model into container now
2023-03-20 14:59:57,804:INFO:_master_model_container: 31
2023-03-20 14:59:57,804:INFO:_display_container: 3
2023-03-20 14:59:57,804:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-20 14:59:57,804:INFO:create_model() successfully completed......................................
2023-03-20 14:59:58,309:INFO:SubProcess create_model() end ==================================
2023-03-20 14:59:58,309:INFO:Creating metrics dataframe
2023-03-20 14:59:58,322:INFO:Initializing Decision Tree Regressor
2023-03-20 14:59:58,322:INFO:Total runtime is 0.5902950962384543 minutes
2023-03-20 14:59:58,328:INFO:SubProcess create_model() called ==================================
2023-03-20 14:59:58,328:INFO:Initializing create_model()
2023-03-20 14:59:58,329:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 14:59:58,329:INFO:Checking exceptions
2023-03-20 14:59:58,329:INFO:Importing libraries
2023-03-20 14:59:58,329:INFO:Copying training dataset
2023-03-20 14:59:58,336:INFO:Defining folds
2023-03-20 14:59:58,336:INFO:Declaring metric variables
2023-03-20 14:59:58,340:INFO:Importing untrained model
2023-03-20 14:59:58,344:INFO:Decision Tree Regressor Imported successfully
2023-03-20 14:59:58,352:INFO:Starting cross validation
2023-03-20 14:59:58,355:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 15:00:00,479:INFO:Calculating mean and std
2023-03-20 15:00:00,481:INFO:Creating metrics dataframe
2023-03-20 15:00:00,686:INFO:Uploading results into container
2023-03-20 15:00:00,687:INFO:Uploading model into container now
2023-03-20 15:00:00,687:INFO:_master_model_container: 32
2023-03-20 15:00:00,687:INFO:_display_container: 3
2023-03-20 15:00:00,687:INFO:DecisionTreeRegressor(random_state=666)
2023-03-20 15:00:00,687:INFO:create_model() successfully completed......................................
2023-03-20 15:00:01,171:INFO:SubProcess create_model() end ==================================
2023-03-20 15:00:01,171:INFO:Creating metrics dataframe
2023-03-20 15:00:01,184:INFO:Initializing Random Forest Regressor
2023-03-20 15:00:01,184:INFO:Total runtime is 0.6379927515983583 minutes
2023-03-20 15:00:01,189:INFO:SubProcess create_model() called ==================================
2023-03-20 15:00:01,190:INFO:Initializing create_model()
2023-03-20 15:00:01,190:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 15:00:01,190:INFO:Checking exceptions
2023-03-20 15:00:01,190:INFO:Importing libraries
2023-03-20 15:00:01,190:INFO:Copying training dataset
2023-03-20 15:00:01,197:INFO:Defining folds
2023-03-20 15:00:01,197:INFO:Declaring metric variables
2023-03-20 15:00:01,201:INFO:Importing untrained model
2023-03-20 15:00:01,205:INFO:Random Forest Regressor Imported successfully
2023-03-20 15:00:01,213:INFO:Starting cross validation
2023-03-20 15:00:01,216:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 15:00:04,063:INFO:Calculating mean and std
2023-03-20 15:00:04,065:INFO:Creating metrics dataframe
2023-03-20 15:00:04,268:INFO:Uploading results into container
2023-03-20 15:00:04,268:INFO:Uploading model into container now
2023-03-20 15:00:04,269:INFO:_master_model_container: 33
2023-03-20 15:00:04,269:INFO:_display_container: 3
2023-03-20 15:00:04,269:INFO:RandomForestRegressor(n_jobs=-1, random_state=666)
2023-03-20 15:00:04,270:INFO:create_model() successfully completed......................................
2023-03-20 15:00:04,750:INFO:SubProcess create_model() end ==================================
2023-03-20 15:00:04,750:INFO:Creating metrics dataframe
2023-03-20 15:00:04,763:INFO:Initializing Extra Trees Regressor
2023-03-20 15:00:04,763:INFO:Total runtime is 0.6976509332656862 minutes
2023-03-20 15:00:04,767:INFO:SubProcess create_model() called ==================================
2023-03-20 15:00:04,768:INFO:Initializing create_model()
2023-03-20 15:00:04,768:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 15:00:04,768:INFO:Checking exceptions
2023-03-20 15:00:04,768:INFO:Importing libraries
2023-03-20 15:00:04,768:INFO:Copying training dataset
2023-03-20 15:00:04,775:INFO:Defining folds
2023-03-20 15:00:04,775:INFO:Declaring metric variables
2023-03-20 15:00:04,779:INFO:Importing untrained model
2023-03-20 15:00:04,784:INFO:Extra Trees Regressor Imported successfully
2023-03-20 15:00:04,791:INFO:Starting cross validation
2023-03-20 15:00:04,794:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 15:00:08,057:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 15:00:08,521:INFO:Calculating mean and std
2023-03-20 15:00:08,523:INFO:Creating metrics dataframe
2023-03-20 15:00:08,662:INFO:Uploading results into container
2023-03-20 15:00:08,662:INFO:Uploading model into container now
2023-03-20 15:00:08,663:INFO:_master_model_container: 34
2023-03-20 15:00:08,663:INFO:_display_container: 3
2023-03-20 15:00:08,663:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=666)
2023-03-20 15:00:08,663:INFO:create_model() successfully completed......................................
2023-03-20 15:00:09,189:INFO:SubProcess create_model() end ==================================
2023-03-20 15:00:09,190:INFO:Creating metrics dataframe
2023-03-20 15:00:09,216:INFO:Initializing AdaBoost Regressor
2023-03-20 15:00:09,217:INFO:Total runtime is 0.7718795736630759 minutes
2023-03-20 15:00:09,221:INFO:SubProcess create_model() called ==================================
2023-03-20 15:00:09,221:INFO:Initializing create_model()
2023-03-20 15:00:09,221:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 15:00:09,221:INFO:Checking exceptions
2023-03-20 15:00:09,221:INFO:Importing libraries
2023-03-20 15:00:09,221:INFO:Copying training dataset
2023-03-20 15:00:09,229:INFO:Defining folds
2023-03-20 15:00:09,229:INFO:Declaring metric variables
2023-03-20 15:00:09,233:INFO:Importing untrained model
2023-03-20 15:00:09,237:INFO:AdaBoost Regressor Imported successfully
2023-03-20 15:00:09,244:INFO:Starting cross validation
2023-03-20 15:00:09,247:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 15:00:11,718:INFO:Calculating mean and std
2023-03-20 15:00:11,719:INFO:Creating metrics dataframe
2023-03-20 15:00:11,930:INFO:Uploading results into container
2023-03-20 15:00:11,930:INFO:Uploading model into container now
2023-03-20 15:00:11,931:INFO:_master_model_container: 35
2023-03-20 15:00:11,931:INFO:_display_container: 3
2023-03-20 15:00:11,931:INFO:AdaBoostRegressor(random_state=666)
2023-03-20 15:00:11,931:INFO:create_model() successfully completed......................................
2023-03-20 15:00:12,429:INFO:SubProcess create_model() end ==================================
2023-03-20 15:00:12,429:INFO:Creating metrics dataframe
2023-03-20 15:00:12,443:INFO:Initializing Gradient Boosting Regressor
2023-03-20 15:00:12,443:INFO:Total runtime is 0.8256460666656495 minutes
2023-03-20 15:00:12,534:INFO:SubProcess create_model() called ==================================
2023-03-20 15:00:12,534:INFO:Initializing create_model()
2023-03-20 15:00:12,534:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 15:00:12,535:INFO:Checking exceptions
2023-03-20 15:00:12,535:INFO:Importing libraries
2023-03-20 15:00:12,535:INFO:Copying training dataset
2023-03-20 15:00:12,542:INFO:Defining folds
2023-03-20 15:00:12,542:INFO:Declaring metric variables
2023-03-20 15:00:12,547:INFO:Importing untrained model
2023-03-20 15:00:12,555:INFO:Gradient Boosting Regressor Imported successfully
2023-03-20 15:00:12,564:INFO:Starting cross validation
2023-03-20 15:00:12,569:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 15:00:14,660:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 15:00:14,734:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 15:00:14,752:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 15:00:14,759:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 15:00:14,773:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 15:00:14,786:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.16s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 15:00:14,855:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.17s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 15:00:14,902:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 15:00:16,314:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 15:00:16,327:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-20 15:00:16,528:INFO:Calculating mean and std
2023-03-20 15:00:16,529:INFO:Creating metrics dataframe
2023-03-20 15:00:16,773:INFO:Uploading results into container
2023-03-20 15:00:16,774:INFO:Uploading model into container now
2023-03-20 15:00:16,775:INFO:_master_model_container: 36
2023-03-20 15:00:16,775:INFO:_display_container: 3
2023-03-20 15:00:16,776:INFO:GradientBoostingRegressor(random_state=666)
2023-03-20 15:00:16,776:INFO:create_model() successfully completed......................................
2023-03-20 15:00:17,282:INFO:SubProcess create_model() end ==================================
2023-03-20 15:00:17,282:INFO:Creating metrics dataframe
2023-03-20 15:00:17,296:INFO:Initializing Extreme Gradient Boosting
2023-03-20 15:00:17,296:INFO:Total runtime is 0.9065224409103395 minutes
2023-03-20 15:00:17,300:INFO:SubProcess create_model() called ==================================
2023-03-20 15:00:17,301:INFO:Initializing create_model()
2023-03-20 15:00:17,301:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 15:00:17,301:INFO:Checking exceptions
2023-03-20 15:00:17,301:INFO:Importing libraries
2023-03-20 15:00:17,301:INFO:Copying training dataset
2023-03-20 15:00:17,309:INFO:Defining folds
2023-03-20 15:00:17,309:INFO:Declaring metric variables
2023-03-20 15:00:17,313:INFO:Importing untrained model
2023-03-20 15:00:17,320:INFO:Extreme Gradient Boosting Imported successfully
2023-03-20 15:00:17,329:INFO:Starting cross validation
2023-03-20 15:00:17,333:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 15:00:19,927:INFO:Calculating mean and std
2023-03-20 15:00:19,929:INFO:Creating metrics dataframe
2023-03-20 15:00:20,140:INFO:Uploading results into container
2023-03-20 15:00:20,140:INFO:Uploading model into container now
2023-03-20 15:00:20,141:INFO:_master_model_container: 37
2023-03-20 15:00:20,141:INFO:_display_container: 3
2023-03-20 15:00:20,142:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=666, ...)
2023-03-20 15:00:20,142:INFO:create_model() successfully completed......................................
2023-03-20 15:00:20,636:INFO:SubProcess create_model() end ==================================
2023-03-20 15:00:20,636:INFO:Creating metrics dataframe
2023-03-20 15:00:20,651:INFO:Initializing Light Gradient Boosting Machine
2023-03-20 15:00:20,651:INFO:Total runtime is 0.9624429504076641 minutes
2023-03-20 15:00:20,655:INFO:SubProcess create_model() called ==================================
2023-03-20 15:00:20,656:INFO:Initializing create_model()
2023-03-20 15:00:20,656:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 15:00:20,656:INFO:Checking exceptions
2023-03-20 15:00:20,656:INFO:Importing libraries
2023-03-20 15:00:20,656:INFO:Copying training dataset
2023-03-20 15:00:20,663:INFO:Defining folds
2023-03-20 15:00:20,663:INFO:Declaring metric variables
2023-03-20 15:00:20,667:INFO:Importing untrained model
2023-03-20 15:00:20,670:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-20 15:00:20,677:INFO:Starting cross validation
2023-03-20 15:00:20,680:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 15:00:23,060:INFO:Calculating mean and std
2023-03-20 15:00:23,062:INFO:Creating metrics dataframe
2023-03-20 15:00:23,262:INFO:Uploading results into container
2023-03-20 15:00:23,263:INFO:Uploading model into container now
2023-03-20 15:00:23,263:INFO:_master_model_container: 38
2023-03-20 15:00:23,263:INFO:_display_container: 3
2023-03-20 15:00:23,264:INFO:LGBMRegressor(random_state=666)
2023-03-20 15:00:23,264:INFO:create_model() successfully completed......................................
2023-03-20 15:00:23,747:INFO:SubProcess create_model() end ==================================
2023-03-20 15:00:23,747:INFO:Creating metrics dataframe
2023-03-20 15:00:23,762:INFO:Initializing CatBoost Regressor
2023-03-20 15:00:23,762:INFO:Total runtime is 1.0142894943555198 minutes
2023-03-20 15:00:23,767:INFO:SubProcess create_model() called ==================================
2023-03-20 15:00:23,767:INFO:Initializing create_model()
2023-03-20 15:00:23,767:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 15:00:23,767:INFO:Checking exceptions
2023-03-20 15:00:23,767:INFO:Importing libraries
2023-03-20 15:00:23,768:INFO:Copying training dataset
2023-03-20 15:00:23,777:INFO:Defining folds
2023-03-20 15:00:23,777:INFO:Declaring metric variables
2023-03-20 15:00:23,781:INFO:Importing untrained model
2023-03-20 15:00:23,785:INFO:CatBoost Regressor Imported successfully
2023-03-20 15:00:23,794:INFO:Starting cross validation
2023-03-20 15:00:23,799:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 15:00:26,230:INFO:Calculating mean and std
2023-03-20 15:00:26,231:INFO:Creating metrics dataframe
2023-03-20 15:00:26,437:INFO:Uploading results into container
2023-03-20 15:00:26,438:INFO:Uploading model into container now
2023-03-20 15:00:26,438:INFO:_master_model_container: 39
2023-03-20 15:00:26,438:INFO:_display_container: 3
2023-03-20 15:00:26,439:INFO:<catboost.core.CatBoostRegressor object at 0x0000026179448160>
2023-03-20 15:00:26,439:INFO:create_model() successfully completed......................................
2023-03-20 15:00:26,904:INFO:SubProcess create_model() end ==================================
2023-03-20 15:00:26,904:INFO:Creating metrics dataframe
2023-03-20 15:00:26,919:INFO:Initializing Dummy Regressor
2023-03-20 15:00:26,919:INFO:Total runtime is 1.0669121583302819 minutes
2023-03-20 15:00:26,924:INFO:SubProcess create_model() called ==================================
2023-03-20 15:00:26,925:INFO:Initializing create_model()
2023-03-20 15:00:26,925:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002617A645610>, model_only=True, return_train_score=False, kwargs={})
2023-03-20 15:00:26,925:INFO:Checking exceptions
2023-03-20 15:00:26,925:INFO:Importing libraries
2023-03-20 15:00:26,925:INFO:Copying training dataset
2023-03-20 15:00:26,933:INFO:Defining folds
2023-03-20 15:00:26,933:INFO:Declaring metric variables
2023-03-20 15:00:26,938:INFO:Importing untrained model
2023-03-20 15:00:26,945:INFO:Dummy Regressor Imported successfully
2023-03-20 15:00:26,955:INFO:Starting cross validation
2023-03-20 15:00:26,958:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-20 15:00:29,521:INFO:Calculating mean and std
2023-03-20 15:00:29,522:INFO:Creating metrics dataframe
2023-03-20 15:00:29,734:INFO:Uploading results into container
2023-03-20 15:00:29,734:INFO:Uploading model into container now
2023-03-20 15:00:29,735:INFO:_master_model_container: 40
2023-03-20 15:00:29,735:INFO:_display_container: 3
2023-03-20 15:00:29,735:INFO:DummyRegressor()
2023-03-20 15:00:29,735:INFO:create_model() successfully completed......................................
2023-03-20 15:00:30,199:INFO:SubProcess create_model() end ==================================
2023-03-20 15:00:30,199:INFO:Creating metrics dataframe
2023-03-20 15:00:30,226:INFO:Initializing create_model()
2023-03-20 15:00:30,227:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=GradientBoostingRegressor(random_state=666), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-20 15:00:30,227:INFO:Checking exceptions
2023-03-20 15:00:30,229:INFO:Importing libraries
2023-03-20 15:00:30,230:INFO:Copying training dataset
2023-03-20 15:00:30,237:INFO:Defining folds
2023-03-20 15:00:30,237:INFO:Declaring metric variables
2023-03-20 15:00:30,237:INFO:Importing untrained model
2023-03-20 15:00:30,238:INFO:Declaring custom model
2023-03-20 15:00:30,238:INFO:Gradient Boosting Regressor Imported successfully
2023-03-20 15:00:30,241:INFO:Cross validation set to False
2023-03-20 15:00:30,241:INFO:Fitting Model
2023-03-20 15:00:32,224:INFO:GradientBoostingRegressor(random_state=666)
2023-03-20 15:00:32,224:INFO:create_model() successfully completed......................................
2023-03-20 15:00:32,725:INFO:_master_model_container: 40
2023-03-20 15:00:32,725:INFO:_display_container: 3
2023-03-20 15:00:32,726:INFO:GradientBoostingRegressor(random_state=666)
2023-03-20 15:00:32,726:INFO:compare_models() successfully completed......................................
2023-03-20 15:09:15,803:INFO:Initializing predict_model()
2023-03-20 15:09:15,804:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=GradientBoostingRegressor(random_state=666), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x000002617C03DAF0>)
2023-03-20 15:09:15,804:INFO:Checking exceptions
2023-03-20 15:09:15,804:INFO:Preloading libraries
2023-03-20 15:09:15,806:INFO:Set up data.
2023-03-20 15:09:15,952:INFO:Set up index.
2023-03-20 15:10:45,721:INFO:Initializing predict_model()
2023-03-20 15:10:45,721:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002617A63DA60>, estimator=GradientBoostingRegressor(random_state=666), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x000002617A4DBAF0>)
2023-03-20 15:10:45,721:INFO:Checking exceptions
2023-03-20 15:10:45,721:INFO:Preloading libraries
2023-03-20 15:10:45,739:INFO:Set up data.
2023-03-20 15:10:45,771:INFO:Set up index.
2023-03-22 12:39:47,444:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-22 12:39:47,472:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-22 12:39:47,472:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-22 12:39:47,472:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-22 12:39:48,909:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-22 12:39:49,691:INFO:PyCaret RegressionExperiment
2023-03-22 12:39:49,691:INFO:Logging name: reg-default-name
2023-03-22 12:39:49,691:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-22 12:39:49,691:INFO:version 3.0.0
2023-03-22 12:39:49,691:INFO:Initializing setup()
2023-03-22 12:39:49,691:INFO:self.USI: aa71
2023-03-22 12:39:49,691:INFO:self._variable_keys: {'gpu_param', '_available_plots', 'X', 'html_param', '_ml_usecase', 'target_param', 'gpu_n_jobs_param', 'USI', 'X_test', 'logging_param', 'idx', 'data', 'fold_groups_param', 'memory', 'pipeline', 'log_plots_param', 'n_jobs_param', 'y_test', 'X_train', 'fold_generator', 'exp_id', 'transform_target_param', 'exp_name_log', 'seed', 'y_train', 'fold_shuffle_param', 'y'}
2023-03-22 12:39:49,692:INFO:Checking environment
2023-03-22 12:39:49,692:INFO:python_version: 3.9.12
2023-03-22 12:39:49,692:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-22 12:39:49,692:INFO:machine: AMD64
2023-03-22 12:39:49,692:INFO:platform: Windows-10-10.0.19045-SP0
2023-03-22 12:39:49,692:INFO:Memory: svmem(total=8496553984, available=1031708672, percent=87.9, used=7464845312, free=1031708672)
2023-03-22 12:39:49,692:INFO:Physical Core: 4
2023-03-22 12:39:49,692:INFO:Logical Core: 8
2023-03-22 12:39:49,692:INFO:Checking libraries
2023-03-22 12:39:49,692:INFO:System:
2023-03-22 12:39:49,692:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-22 12:39:49,692:INFO:executable: C:\Users\CHUKWUKA\anaconda3\python.exe
2023-03-22 12:39:49,692:INFO:   machine: Windows-10-10.0.19045-SP0
2023-03-22 12:39:49,692:INFO:PyCaret required dependencies:
2023-03-22 12:39:49,693:INFO:                 pip: 22.3.1
2023-03-22 12:39:49,693:INFO:          setuptools: 65.5.0
2023-03-22 12:39:49,693:INFO:             pycaret: 3.0.0
2023-03-22 12:39:49,693:INFO:             IPython: 8.7.0
2023-03-22 12:39:49,693:INFO:          ipywidgets: 7.6.5
2023-03-22 12:39:49,693:INFO:                tqdm: 4.64.1
2023-03-22 12:39:49,693:INFO:               numpy: 1.21.5
2023-03-22 12:39:49,693:INFO:              pandas: 1.4.4
2023-03-22 12:39:49,693:INFO:              jinja2: 2.11.3
2023-03-22 12:39:49,693:INFO:               scipy: 1.9.3
2023-03-22 12:39:49,693:INFO:              joblib: 1.2.0
2023-03-22 12:39:49,693:INFO:             sklearn: 1.0.2
2023-03-22 12:39:49,693:INFO:                pyod: 1.0.9
2023-03-22 12:39:49,693:INFO:            imblearn: 0.10.1
2023-03-22 12:39:49,693:INFO:   category_encoders: 2.6.0
2023-03-22 12:39:49,693:INFO:            lightgbm: 3.3.5
2023-03-22 12:39:49,693:INFO:               numba: 0.56.4
2023-03-22 12:39:49,693:INFO:            requests: 2.27.1
2023-03-22 12:39:49,694:INFO:          matplotlib: 3.6.2
2023-03-22 12:39:49,694:INFO:          scikitplot: 0.3.7
2023-03-22 12:39:49,694:INFO:         yellowbrick: 1.5
2023-03-22 12:39:49,694:INFO:              plotly: 5.9.0
2023-03-22 12:39:49,694:INFO:             kaleido: 0.2.1
2023-03-22 12:39:49,694:INFO:         statsmodels: 0.13.2
2023-03-22 12:39:49,694:INFO:              sktime: 0.16.1
2023-03-22 12:39:49,694:INFO:               tbats: 1.1.2
2023-03-22 12:39:49,694:INFO:            pmdarima: 2.0.3
2023-03-22 12:39:49,694:INFO:              psutil: 5.9.0
2023-03-22 12:39:49,694:INFO:PyCaret optional dependencies:
2023-03-22 12:39:49,709:INFO:                shap: Not installed
2023-03-22 12:39:49,709:INFO:           interpret: Not installed
2023-03-22 12:39:49,709:INFO:                umap: Not installed
2023-03-22 12:39:49,709:INFO:    pandas_profiling: Not installed
2023-03-22 12:39:49,709:INFO:  explainerdashboard: Not installed
2023-03-22 12:39:49,710:INFO:             autoviz: Not installed
2023-03-22 12:39:49,710:INFO:           fairlearn: Not installed
2023-03-22 12:39:49,710:INFO:             xgboost: 1.7.1
2023-03-22 12:39:49,710:INFO:            catboost: 1.0.6
2023-03-22 12:39:49,710:INFO:              kmodes: Not installed
2023-03-22 12:39:49,710:INFO:             mlxtend: Not installed
2023-03-22 12:39:49,710:INFO:       statsforecast: Not installed
2023-03-22 12:39:49,710:INFO:        tune_sklearn: Not installed
2023-03-22 12:39:49,710:INFO:                 ray: Not installed
2023-03-22 12:39:49,710:INFO:            hyperopt: Not installed
2023-03-22 12:39:49,710:INFO:              optuna: Not installed
2023-03-22 12:39:49,710:INFO:               skopt: Not installed
2023-03-22 12:39:49,710:INFO:              mlflow: Not installed
2023-03-22 12:39:49,710:INFO:              gradio: Not installed
2023-03-22 12:39:49,710:INFO:             fastapi: Not installed
2023-03-22 12:39:49,710:INFO:             uvicorn: Not installed
2023-03-22 12:39:49,710:INFO:              m2cgen: Not installed
2023-03-22 12:39:49,710:INFO:           evidently: Not installed
2023-03-22 12:39:49,710:INFO:               fugue: Not installed
2023-03-22 12:39:49,710:INFO:           streamlit: Not installed
2023-03-22 12:39:49,711:INFO:             prophet: Not installed
2023-03-22 12:39:49,711:INFO:None
2023-03-22 12:39:49,711:INFO:Set up data.
2023-03-22 12:39:49,749:INFO:Set up train/test split.
2023-03-22 12:39:49,763:INFO:Set up index.
2023-03-22 12:39:49,763:INFO:Set up folding strategy.
2023-03-22 12:39:49,763:INFO:Assigning column types.
2023-03-22 12:39:49,767:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-22 12:39:49,767:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-22 12:39:49,772:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 12:39:49,777:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 12:39:49,845:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 12:39:49,895:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 12:39:49,896:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 12:39:50,321:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 12:39:50,451:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,457:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,461:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,525:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,573:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,575:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 12:39:50,577:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 12:39:50,578:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-22 12:39:50,583:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,588:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,654:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,701:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,702:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 12:39:50,705:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 12:39:50,711:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,717:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,782:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,834:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,836:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 12:39:50,840:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 12:39:50,841:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-22 12:39:50,851:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,916:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,964:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 12:39:50,965:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 12:39:50,968:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 12:39:50,978:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 12:39:51,053:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 12:39:51,101:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 12:39:51,102:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 12:39:51,105:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 12:39:51,105:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-22 12:39:51,179:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 12:39:51,227:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 12:39:51,228:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 12:39:51,230:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 12:39:51,304:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 12:39:51,351:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 12:39:51,352:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 12:39:51,354:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 12:39:51,355:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-22 12:39:51,430:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 12:39:51,481:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 12:39:51,483:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 12:39:51,560:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 12:39:51,608:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 12:39:51,611:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 12:39:51,611:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-22 12:39:51,759:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 12:39:51,761:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 12:39:51,931:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 12:39:51,935:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 12:39:51,939:INFO:Preparing preprocessing pipeline...
2023-03-22 12:39:51,940:INFO:Set up simple imputation.
2023-03-22 12:39:51,951:INFO:Set up encoding of ordinal features.
2023-03-22 12:39:51,967:INFO:Set up encoding of categorical features.
2023-03-22 12:39:51,968:INFO:Set up column name cleaning.
2023-03-22 12:39:52,516:INFO:Finished creating preprocessing pipeline.
2023-03-22 12:39:52,876:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\CHUKWUKA\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=[], transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['onehotencoder__Item_Fat_Content_Low '
                                             'Fat',
                                             'onehotencoder__Item_Fat_Content_Regular',
                                             'onehotencoder__Outlet_Size_High',
                                             'onehotenco...
                                             'remainder__Item_Visibility',
                                             'remainder__Item_MRP',
                                             'remainder__Item_Code'],
                                    transformer=LeaveOneOutEncoder(cols=['remainder__Item_Identifier',
                                                                         'remainder__Item_Weight',
                                                                         'remainder__Item_Visibility',
                                                                         'remainder__Item_MRP',
                                                                         'remainder__Item_Code'],
                                                                   handle_missing='return_nan',
                                                                   random_state=666))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-03-22 12:39:52,876:INFO:Creating final display dataframe.
2023-03-22 12:39:54,226:INFO:Setup _display_container:                     Description             Value
0                    Session id               666
1                        Target        Item_Sales
2                   Target type        Regression
3           Original data shape        (5966, 21)
4        Transformed data shape        (5966, 53)
5   Transformed train set shape        (4176, 53)
6    Transformed test set shape        (1790, 53)
7              Ordinal features                12
8          Categorical features                20
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator             KFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  reg-default-name
21                          USI              aa71
2023-03-22 12:39:54,361:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 12:39:54,364:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 12:39:54,504:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 12:39:54,507:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 12:39:54,508:INFO:setup() successfully completed in 5.33s...............
2023-03-22 12:39:54,528:INFO:Initializing compare_models()
2023-03-22 12:39:54,528:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, include=None, fold=None, round=4, cross_validation=True, sort=MSE, n_select=1, budget_time=10, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MSE', 'n_select': 1, 'budget_time': 10, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-22 12:39:54,528:INFO:Checking exceptions
2023-03-22 12:39:54,537:INFO:Preparing display monitor
2023-03-22 12:39:54,593:INFO:Time budget is 10 minutes
2023-03-22 12:39:54,593:INFO:Initializing Linear Regression
2023-03-22 12:39:54,593:INFO:Total runtime is 0.0 minutes
2023-03-22 12:39:54,597:INFO:SubProcess create_model() called ==================================
2023-03-22 12:39:54,598:INFO:Initializing create_model()
2023-03-22 12:39:54,598:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:39:54,598:INFO:Checking exceptions
2023-03-22 12:39:54,599:INFO:Importing libraries
2023-03-22 12:39:54,599:INFO:Copying training dataset
2023-03-22 12:39:54,613:INFO:Defining folds
2023-03-22 12:39:54,613:INFO:Declaring metric variables
2023-03-22 12:39:54,621:INFO:Importing untrained model
2023-03-22 12:39:54,629:INFO:Linear Regression Imported successfully
2023-03-22 12:39:54,641:INFO:Starting cross validation
2023-03-22 12:39:54,657:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:04,193:INFO:Calculating mean and std
2023-03-22 12:40:04,195:INFO:Creating metrics dataframe
2023-03-22 12:40:04,409:INFO:Uploading results into container
2023-03-22 12:40:04,410:INFO:Uploading model into container now
2023-03-22 12:40:04,411:INFO:_master_model_container: 1
2023-03-22 12:40:04,411:INFO:_display_container: 2
2023-03-22 12:40:04,411:INFO:LinearRegression(n_jobs=-1)
2023-03-22 12:40:04,411:INFO:create_model() successfully completed......................................
2023-03-22 12:40:04,540:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:04,540:INFO:Creating metrics dataframe
2023-03-22 12:40:04,550:INFO:Initializing Lasso Regression
2023-03-22 12:40:04,550:INFO:Total runtime is 0.16595751841862996 minutes
2023-03-22 12:40:04,554:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:04,554:INFO:Initializing create_model()
2023-03-22 12:40:04,555:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:04,555:INFO:Checking exceptions
2023-03-22 12:40:04,555:INFO:Importing libraries
2023-03-22 12:40:04,555:INFO:Copying training dataset
2023-03-22 12:40:04,562:INFO:Defining folds
2023-03-22 12:40:04,562:INFO:Declaring metric variables
2023-03-22 12:40:04,566:INFO:Importing untrained model
2023-03-22 12:40:04,571:INFO:Lasso Regression Imported successfully
2023-03-22 12:40:04,582:INFO:Starting cross validation
2023-03-22 12:40:04,584:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:06,833:INFO:Calculating mean and std
2023-03-22 12:40:06,835:INFO:Creating metrics dataframe
2023-03-22 12:40:07,040:INFO:Uploading results into container
2023-03-22 12:40:07,157:INFO:Uploading model into container now
2023-03-22 12:40:07,157:INFO:_master_model_container: 2
2023-03-22 12:40:07,157:INFO:_display_container: 2
2023-03-22 12:40:07,158:INFO:Lasso(random_state=666)
2023-03-22 12:40:07,158:INFO:create_model() successfully completed......................................
2023-03-22 12:40:07,280:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:07,281:INFO:Creating metrics dataframe
2023-03-22 12:40:07,290:INFO:Initializing Ridge Regression
2023-03-22 12:40:07,290:INFO:Total runtime is 0.2116166631380717 minutes
2023-03-22 12:40:07,294:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:07,295:INFO:Initializing create_model()
2023-03-22 12:40:07,295:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:07,295:INFO:Checking exceptions
2023-03-22 12:40:07,295:INFO:Importing libraries
2023-03-22 12:40:07,296:INFO:Copying training dataset
2023-03-22 12:40:07,305:INFO:Defining folds
2023-03-22 12:40:07,306:INFO:Declaring metric variables
2023-03-22 12:40:07,313:INFO:Importing untrained model
2023-03-22 12:40:07,318:INFO:Ridge Regression Imported successfully
2023-03-22 12:40:07,328:INFO:Starting cross validation
2023-03-22 12:40:07,331:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:10,096:INFO:Calculating mean and std
2023-03-22 12:40:10,097:INFO:Creating metrics dataframe
2023-03-22 12:40:10,305:INFO:Uploading results into container
2023-03-22 12:40:10,306:INFO:Uploading model into container now
2023-03-22 12:40:10,306:INFO:_master_model_container: 3
2023-03-22 12:40:10,306:INFO:_display_container: 2
2023-03-22 12:40:10,307:INFO:Ridge(random_state=666)
2023-03-22 12:40:10,307:INFO:create_model() successfully completed......................................
2023-03-22 12:40:10,417:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:10,417:INFO:Creating metrics dataframe
2023-03-22 12:40:10,428:INFO:Initializing Elastic Net
2023-03-22 12:40:10,428:INFO:Total runtime is 0.2639232397079468 minutes
2023-03-22 12:40:10,432:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:10,433:INFO:Initializing create_model()
2023-03-22 12:40:10,433:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:10,433:INFO:Checking exceptions
2023-03-22 12:40:10,433:INFO:Importing libraries
2023-03-22 12:40:10,433:INFO:Copying training dataset
2023-03-22 12:40:10,442:INFO:Defining folds
2023-03-22 12:40:10,442:INFO:Declaring metric variables
2023-03-22 12:40:10,447:INFO:Importing untrained model
2023-03-22 12:40:10,451:INFO:Elastic Net Imported successfully
2023-03-22 12:40:10,459:INFO:Starting cross validation
2023-03-22 12:40:10,463:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:13,096:INFO:Calculating mean and std
2023-03-22 12:40:13,097:INFO:Creating metrics dataframe
2023-03-22 12:40:13,323:INFO:Uploading results into container
2023-03-22 12:40:13,324:INFO:Uploading model into container now
2023-03-22 12:40:13,324:INFO:_master_model_container: 4
2023-03-22 12:40:13,324:INFO:_display_container: 2
2023-03-22 12:40:13,325:INFO:ElasticNet(random_state=666)
2023-03-22 12:40:13,325:INFO:create_model() successfully completed......................................
2023-03-22 12:40:13,473:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:13,473:INFO:Creating metrics dataframe
2023-03-22 12:40:13,493:INFO:Initializing Least Angle Regression
2023-03-22 12:40:13,493:INFO:Total runtime is 0.315003498395284 minutes
2023-03-22 12:40:13,502:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:13,503:INFO:Initializing create_model()
2023-03-22 12:40:13,504:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:13,504:INFO:Checking exceptions
2023-03-22 12:40:13,504:INFO:Importing libraries
2023-03-22 12:40:13,504:INFO:Copying training dataset
2023-03-22 12:40:13,520:INFO:Defining folds
2023-03-22 12:40:13,521:INFO:Declaring metric variables
2023-03-22 12:40:13,534:INFO:Importing untrained model
2023-03-22 12:40:13,542:INFO:Least Angle Regression Imported successfully
2023-03-22 12:40:13,556:INFO:Starting cross validation
2023-03-22 12:40:13,563:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:14,525:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:14,532:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:14,553:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.280e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,553:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.397e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,553:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.280e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,554:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.540e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,555:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:14,555:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.711e-01, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,559:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,564:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,566:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,567:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,567:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.392e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,571:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,571:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,572:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,572:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.976e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,572:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.299e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,573:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.457e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,573:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=7.334e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,574:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.297e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,576:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.599e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,576:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.656e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,576:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.599e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,576:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:14,578:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.940e-01, with an active set of 14 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,579:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.194e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,580:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.194e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,581:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.761e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,582:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.761e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,582:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.050e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,582:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.761e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,582:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,583:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.363e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,583:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,583:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.352e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,583:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,583:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,584:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.046e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,585:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.915e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,586:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.784e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,587:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.458e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,588:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.403e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,588:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.181e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,588:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.670e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,588:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.069e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,589:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.767e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,589:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.646e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,584:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.905e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,591:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.084e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,591:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.905e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,591:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.076e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,592:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.140e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,592:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.847e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,592:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.905e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,592:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.140e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,592:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.905e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,592:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.140e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,592:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.533e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,593:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.122e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,594:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,594:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,595:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,593:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.631e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,595:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,597:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.631e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,597:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,598:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.647e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.093e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,598:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.034e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,600:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.602e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-03-22 12:40:14,601:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.130e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,602:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.258e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,603:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.258e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,604:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.892e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,605:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.842e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,605:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.820e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,605:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.338e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,606:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.026e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,607:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.729e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,607:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.729e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,608:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.524e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,730:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.729e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,730:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.524e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,731:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.220e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,731:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.220e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,732:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.429e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,732:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.171e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,608:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,732:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.709e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,733:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,733:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.709e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,733:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.084e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,733:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.569e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,733:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,734:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=8.932e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,734:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.024e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,735:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.177e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,735:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.651e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,736:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.909e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,736:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.206e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,736:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.110e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,736:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.054e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,737:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.539e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,737:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=5.728e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,738:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.031e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,738:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.523e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,738:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.151e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,740:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.523e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,740:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.480e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,741:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.523e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,742:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.661e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,742:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.726e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,742:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.368e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,742:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.525e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,743:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.147e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,743:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.015e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,743:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,744:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,747:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.597e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,748:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.778e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,749:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.778e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,749:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,749:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.774e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,749:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,750:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.704e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,750:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,750:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.582e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,750:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.592e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,750:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.359e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,751:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.476e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,751:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.865e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,751:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.467e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,752:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.310e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,752:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.183e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,752:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.949e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,753:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.980e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,753:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.880e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,753:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.770e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,754:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.303e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,754:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.539e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,754:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.280e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,755:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=8.784e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,755:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.875e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,756:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.471e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,756:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.651e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,756:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.467e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,756:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.089e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,756:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.975e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,761:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.822e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,762:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.730e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,609:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.655e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,768:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.328e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,768:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.274e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,769:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.022e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,769:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=8.154e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,770:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.678e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,770:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,771:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.187e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,771:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.186e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,772:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.106e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,772:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.280e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,774:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.218e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,774:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.157e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,775:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.980e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,776:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.926e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,777:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.273e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,778:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=8.771e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,778:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=7.527e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,779:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.661e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,780:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.598e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,780:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=5.356e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,781:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=3.819e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,781:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.900e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,782:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.655e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,782:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.776e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,783:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.769e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,783:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.762e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,783:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.544e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,783:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.745e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,783:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.388e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,784:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.718e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,784:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.299e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,784:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.705e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,784:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.276e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,784:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.511e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,785:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.273e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,785:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.164e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,785:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.925e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,786:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.052e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,786:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.041e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,786:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.484e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,787:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.954e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,787:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.400e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,787:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.399e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,788:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.238e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,788:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.185e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,788:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.051e-03, with an active set of 32 regressors, and the smallest cholesky pivot element being 9.940e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,789:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=6.808e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,789:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=5.671e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,790:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.611e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,790:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.285e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,791:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.285e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,792:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.285e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,792:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.285e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,792:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.778e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,793:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.049e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,796:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.382e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,798:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.400e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,799:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.369e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,800:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.968e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,801:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.039e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,801:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=3.529e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,802:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.759e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,803:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.031e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,803:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.013e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,803:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.969e-03, with an active set of 27 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,804:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.588e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,804:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=1.403e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,805:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.132e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,806:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.053e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,807:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,807:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,808:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,808:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,809:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.618e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,809:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=6.365e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,809:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.616e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,810:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.585e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,811:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=4.309e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,811:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.654e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,812:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.820e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,813:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.321e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,813:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.167e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,813:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.157e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,814:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=9.565e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,814:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.523e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,814:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=8.093e-06, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,816:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.634e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,817:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.072e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,817:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=3.031e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,818:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=2.042e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,818:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.877e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,819:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.859e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,819:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.857e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,819:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.851e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,820:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.839e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,820:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.831e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,820:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.267e-04, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,866:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:14,887:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.541e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,892:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,893:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,893:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,895:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,899:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.819e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,900:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.470e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,901:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.997e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,902:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.997e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,903:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.829e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,904:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.553e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,904:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.504e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,905:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.332e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,905:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.244e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,906:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.122e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,906:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.867e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,907:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.867e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,908:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.867e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,909:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.867e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,910:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.715e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,911:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.130e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,911:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.197e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,912:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.809e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,915:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.058e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,915:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.907e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,916:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.576e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,918:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.328e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,919:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.083e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,920:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.072e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,920:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=9.611e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,921:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.123e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,923:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.621e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,923:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.581e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,924:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.529e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,924:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.528e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,924:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.524e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,925:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.524e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,925:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.522e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,926:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.521e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,926:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.508e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,926:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.471e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,927:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.346e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,928:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.573e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,928:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.290e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,930:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.908e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,931:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.908e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,932:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.061e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,933:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.990e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,933:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.626e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:14,934:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.421e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,079:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:15,096:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,096:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,097:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,098:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.711e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,100:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.137e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,101:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.927e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,102:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.927e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,102:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.925e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,104:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.138e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,104:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.138e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,105:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.138e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,106:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.872e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,107:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.274e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,108:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.274e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,108:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.876e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,109:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.374e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,110:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.298e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,116:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.216e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,118:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.116e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,119:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.013e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,120:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.955e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,121:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.457e-01, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,122:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.098e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,123:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.098e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,124:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.046e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,124:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=1.007e-01, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,124:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.748e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,125:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.470e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,125:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.882e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,125:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.409e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,126:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.217e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,126:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.661e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,132:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.238e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,132:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.238e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,133:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.221e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,133:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.953e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,134:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.565e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,135:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.563e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,136:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.414e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,137:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.119e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,160:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.677e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,160:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=7.087e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,161:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.829e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,161:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.826e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,161:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.809e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,162:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.659e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,162:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.474e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,163:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.615e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,163:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=4.042e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,163:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.869e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,164:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.418e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,164:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.366e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,164:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=3.361e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,165:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.373e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,165:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.085e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,166:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.928e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,166:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.615e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,166:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=6.387e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,167:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.460e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,190:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:15,205:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.088e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,206:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=9.088e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,209:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.258e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,209:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.258e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,211:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.258e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,215:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.526e-01, with an active set of 14 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,216:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=3.886e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,219:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.036e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,220:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.036e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,220:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.752e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,221:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.752e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,222:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.386e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,223:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.081e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,223:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.081e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,224:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.938e-01, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,224:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.938e-01, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,225:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.875e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,226:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.788e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,226:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.780e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,228:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.444e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,229:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.345e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,230:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.253e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,231:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.017e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,232:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=9.442e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,232:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.124e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,233:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.124e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,233:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.124e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,233:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=7.706e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,234:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.758e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,235:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.732e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,235:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.356e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,235:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=4.336e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,236:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.988e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,236:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.988e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,237:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.467e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,237:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.718e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,238:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,238:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,239:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,239:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,240:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=2.211e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,240:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.546e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,241:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.487e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,241:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.411e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,241:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=1.353e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,242:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=7.983e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,242:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.096e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,243:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.096e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,244:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=4.096e-03, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,247:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.780e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,248:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.944e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,248:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=2.745e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,249:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.307e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,250:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,250:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,251:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,251:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,252:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,252:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.563e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,253:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.118e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,253:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.473e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,254:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.015e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,254:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.755e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,255:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.925e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,255:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.825e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,255:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.308e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,256:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.004e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,257:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.098e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,258:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.917e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,259:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.892e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,259:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.875e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,260:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.843e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,261:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.834e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,262:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.830e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,263:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.829e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,263:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.827e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,264:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.819e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,264:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.818e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,265:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.778e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,265:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.043e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,266:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.091e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,267:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.832e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,267:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.894e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,268:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.630e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,268:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.417e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,269:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.161e-06, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,351:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:15,363:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.420e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,379:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,385:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,385:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,386:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,386:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.020e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,387:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.954e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,391:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.440e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,392:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.440e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,394:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.108e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,396:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.107e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,396:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.048e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,396:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.754e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,396:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.735e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,397:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.636e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,398:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.629e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,410:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.909e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,412:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.825e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,413:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.738e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,414:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.602e+00, with an active set of 37 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,415:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.263e+00, with an active set of 39 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,418:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.218e+00, with an active set of 40 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,419:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.066e+00, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,420:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=9.946e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,421:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.257e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,422:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=8.061e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,442:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=5.825e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,442:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=5.450e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,444:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.406e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,444:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.503e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,445:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=2.916e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,445:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.802e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,446:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.511e-01, with an active set of 42 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,448:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.213e-01, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,449:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=9.021e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,450:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.087e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,450:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=8.087e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,451:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=5.864e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,452:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.022e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,452:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.264e-02, with an active set of 43 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:15,453:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.372e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,532:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:16,544:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,545:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,545:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,545:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,545:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,546:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=1.318e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,546:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.184e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,547:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.184e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,547:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.184e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,547:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=8.096e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,547:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.551e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,548:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.776e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,548:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=5.308e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,548:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.258e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,549:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.258e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,549:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.258e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,549:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.258e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,549:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=4.258e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,550:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.226e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,550:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.114e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,550:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.259e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,550:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.162e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,551:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=1.126e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,551:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=8.163e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,552:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.786e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,552:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.786e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,553:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.786e-03, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,554:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=5.427e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,554:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.917e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,555:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.834e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,555:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=4.785e-03, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,556:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,556:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,556:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,556:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,557:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,557:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.836e-03, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,558:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.308e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,558:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.572e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,558:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:16,558:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=9.160e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,559:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.768e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,559:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.630e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,559:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.535e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,559:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.534e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,559:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.534e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,560:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.524e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,560:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.503e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,560:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.502e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,561:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.475e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,561:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.332e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,561:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.937e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,561:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.835e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,561:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.435e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,562:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.769e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,562:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.156e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,562:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.739e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,562:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.234e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,568:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=6.459e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,571:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,571:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,571:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,571:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,572:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,572:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.789e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,572:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.692e-01, with an active set of 22 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,573:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,574:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,574:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,574:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,575:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.138e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,575:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.023e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,575:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=1.007e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,576:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.215e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,576:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.215e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,576:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=6.389e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,577:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=5.358e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,579:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.811e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,581:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.362e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,581:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=4.291e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,581:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,581:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,582:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,582:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,582:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,582:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.964e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,583:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.436e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,583:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.809e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,584:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.426e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,584:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.037e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 7.814e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,584:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.539e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,584:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=7.396e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,585:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.849e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,585:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.625e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,585:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.615e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,585:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.609e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,585:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.592e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,586:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.576e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,586:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.040e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,586:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=4.603e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,586:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.419e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,586:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.015e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,587:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.971e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,587:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.146e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,587:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=1.780e-03, with an active set of 31 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,587:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=8.011e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:16,587:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=6.362e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:17,610:INFO:Calculating mean and std
2023-03-22 12:40:17,612:INFO:Creating metrics dataframe
2023-03-22 12:40:17,843:INFO:Uploading results into container
2023-03-22 12:40:17,844:INFO:Uploading model into container now
2023-03-22 12:40:17,845:INFO:_master_model_container: 5
2023-03-22 12:40:17,845:INFO:_display_container: 2
2023-03-22 12:40:17,846:INFO:Lars(random_state=666)
2023-03-22 12:40:17,846:INFO:create_model() successfully completed......................................
2023-03-22 12:40:17,975:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:17,975:INFO:Creating metrics dataframe
2023-03-22 12:40:17,987:INFO:Initializing Lasso Least Angle Regression
2023-03-22 12:40:17,987:INFO:Total runtime is 0.38990307251612344 minutes
2023-03-22 12:40:17,991:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:17,992:INFO:Initializing create_model()
2023-03-22 12:40:17,992:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:17,992:INFO:Checking exceptions
2023-03-22 12:40:17,992:INFO:Importing libraries
2023-03-22 12:40:17,992:INFO:Copying training dataset
2023-03-22 12:40:18,001:INFO:Defining folds
2023-03-22 12:40:18,001:INFO:Declaring metric variables
2023-03-22 12:40:18,007:INFO:Importing untrained model
2023-03-22 12:40:18,016:INFO:Lasso Least Angle Regression Imported successfully
2023-03-22 12:40:18,029:INFO:Starting cross validation
2023-03-22 12:40:18,033:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:18,832:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 12:40:18,847:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 12:40:18,862:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 12:40:18,872:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:18,873:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:18,873:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.211e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:18,906:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 12:40:18,953:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 12:40:18,974:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 12:40:18,979:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:18,980:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:18,980:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.030e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:18,986:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 12:40:19,001:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.759e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:19,003:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:19,003:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:19,004:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.283e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:19,031:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.420e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:19,033:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:19,033:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:19,034:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:19,034:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.482e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:19,035:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.020e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 12:40:19,202:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 12:40:20,115:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 12:40:20,150:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 12:40:21,058:INFO:Calculating mean and std
2023-03-22 12:40:21,059:INFO:Creating metrics dataframe
2023-03-22 12:40:21,284:INFO:Uploading results into container
2023-03-22 12:40:21,285:INFO:Uploading model into container now
2023-03-22 12:40:21,285:INFO:_master_model_container: 6
2023-03-22 12:40:21,285:INFO:_display_container: 2
2023-03-22 12:40:21,285:INFO:LassoLars(random_state=666)
2023-03-22 12:40:21,286:INFO:create_model() successfully completed......................................
2023-03-22 12:40:21,395:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:21,395:INFO:Creating metrics dataframe
2023-03-22 12:40:21,406:INFO:Initializing Orthogonal Matching Pursuit
2023-03-22 12:40:21,406:INFO:Total runtime is 0.44688626925150554 minutes
2023-03-22 12:40:21,411:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:21,412:INFO:Initializing create_model()
2023-03-22 12:40:21,412:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:21,412:INFO:Checking exceptions
2023-03-22 12:40:21,412:INFO:Importing libraries
2023-03-22 12:40:21,412:INFO:Copying training dataset
2023-03-22 12:40:21,419:INFO:Defining folds
2023-03-22 12:40:21,420:INFO:Declaring metric variables
2023-03-22 12:40:21,424:INFO:Importing untrained model
2023-03-22 12:40:21,429:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-22 12:40:21,438:INFO:Starting cross validation
2023-03-22 12:40:21,442:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:22,311:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:22,322:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:22,323:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:22,382:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:22,389:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:22,393:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:22,412:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:22,570:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:23,640:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:23,647:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 12:40:24,681:INFO:Calculating mean and std
2023-03-22 12:40:24,682:INFO:Creating metrics dataframe
2023-03-22 12:40:24,924:INFO:Uploading results into container
2023-03-22 12:40:24,925:INFO:Uploading model into container now
2023-03-22 12:40:24,927:INFO:_master_model_container: 7
2023-03-22 12:40:24,927:INFO:_display_container: 2
2023-03-22 12:40:24,928:INFO:OrthogonalMatchingPursuit()
2023-03-22 12:40:24,928:INFO:create_model() successfully completed......................................
2023-03-22 12:40:25,049:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:25,049:INFO:Creating metrics dataframe
2023-03-22 12:40:25,062:INFO:Initializing Bayesian Ridge
2023-03-22 12:40:25,063:INFO:Total runtime is 0.5078300436337789 minutes
2023-03-22 12:40:25,067:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:25,067:INFO:Initializing create_model()
2023-03-22 12:40:25,067:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:25,067:INFO:Checking exceptions
2023-03-22 12:40:25,067:INFO:Importing libraries
2023-03-22 12:40:25,067:INFO:Copying training dataset
2023-03-22 12:40:25,077:INFO:Defining folds
2023-03-22 12:40:25,077:INFO:Declaring metric variables
2023-03-22 12:40:25,081:INFO:Importing untrained model
2023-03-22 12:40:25,086:INFO:Bayesian Ridge Imported successfully
2023-03-22 12:40:25,099:INFO:Starting cross validation
2023-03-22 12:40:25,104:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:28,055:INFO:Calculating mean and std
2023-03-22 12:40:28,058:INFO:Creating metrics dataframe
2023-03-22 12:40:28,282:INFO:Uploading results into container
2023-03-22 12:40:28,283:INFO:Uploading model into container now
2023-03-22 12:40:28,284:INFO:_master_model_container: 8
2023-03-22 12:40:28,284:INFO:_display_container: 2
2023-03-22 12:40:28,284:INFO:BayesianRidge()
2023-03-22 12:40:28,284:INFO:create_model() successfully completed......................................
2023-03-22 12:40:28,400:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:28,400:INFO:Creating metrics dataframe
2023-03-22 12:40:28,412:INFO:Initializing Passive Aggressive Regressor
2023-03-22 12:40:28,413:INFO:Total runtime is 0.5636744658152263 minutes
2023-03-22 12:40:28,417:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:28,418:INFO:Initializing create_model()
2023-03-22 12:40:28,418:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:28,418:INFO:Checking exceptions
2023-03-22 12:40:28,418:INFO:Importing libraries
2023-03-22 12:40:28,419:INFO:Copying training dataset
2023-03-22 12:40:28,434:INFO:Defining folds
2023-03-22 12:40:28,435:INFO:Declaring metric variables
2023-03-22 12:40:28,442:INFO:Importing untrained model
2023-03-22 12:40:28,449:INFO:Passive Aggressive Regressor Imported successfully
2023-03-22 12:40:28,462:INFO:Starting cross validation
2023-03-22 12:40:28,465:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:31,325:INFO:Calculating mean and std
2023-03-22 12:40:31,326:INFO:Creating metrics dataframe
2023-03-22 12:40:31,595:INFO:Uploading results into container
2023-03-22 12:40:31,596:INFO:Uploading model into container now
2023-03-22 12:40:31,597:INFO:_master_model_container: 9
2023-03-22 12:40:31,597:INFO:_display_container: 2
2023-03-22 12:40:31,597:INFO:PassiveAggressiveRegressor(random_state=666)
2023-03-22 12:40:31,597:INFO:create_model() successfully completed......................................
2023-03-22 12:40:31,723:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:31,723:INFO:Creating metrics dataframe
2023-03-22 12:40:31,739:INFO:Initializing Huber Regressor
2023-03-22 12:40:31,739:INFO:Total runtime is 0.6191049774487813 minutes
2023-03-22 12:40:31,746:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:31,747:INFO:Initializing create_model()
2023-03-22 12:40:31,747:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:31,747:INFO:Checking exceptions
2023-03-22 12:40:31,747:INFO:Importing libraries
2023-03-22 12:40:31,747:INFO:Copying training dataset
2023-03-22 12:40:31,760:INFO:Defining folds
2023-03-22 12:40:31,760:INFO:Declaring metric variables
2023-03-22 12:40:31,765:INFO:Importing untrained model
2023-03-22 12:40:31,770:INFO:Huber Regressor Imported successfully
2023-03-22 12:40:31,783:INFO:Starting cross validation
2023-03-22 12:40:31,793:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:34,960:INFO:Calculating mean and std
2023-03-22 12:40:34,962:INFO:Creating metrics dataframe
2023-03-22 12:40:35,209:INFO:Uploading results into container
2023-03-22 12:40:35,210:INFO:Uploading model into container now
2023-03-22 12:40:35,210:INFO:_master_model_container: 10
2023-03-22 12:40:35,210:INFO:_display_container: 2
2023-03-22 12:40:35,211:INFO:HuberRegressor()
2023-03-22 12:40:35,211:INFO:create_model() successfully completed......................................
2023-03-22 12:40:35,329:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:35,330:INFO:Creating metrics dataframe
2023-03-22 12:40:35,344:INFO:Initializing K Neighbors Regressor
2023-03-22 12:40:35,345:INFO:Total runtime is 0.6792009353637696 minutes
2023-03-22 12:40:35,351:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:35,351:INFO:Initializing create_model()
2023-03-22 12:40:35,351:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:35,351:INFO:Checking exceptions
2023-03-22 12:40:35,352:INFO:Importing libraries
2023-03-22 12:40:35,352:INFO:Copying training dataset
2023-03-22 12:40:35,363:INFO:Defining folds
2023-03-22 12:40:35,363:INFO:Declaring metric variables
2023-03-22 12:40:35,369:INFO:Importing untrained model
2023-03-22 12:40:35,375:INFO:K Neighbors Regressor Imported successfully
2023-03-22 12:40:35,384:INFO:Starting cross validation
2023-03-22 12:40:35,390:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:38,625:INFO:Calculating mean and std
2023-03-22 12:40:38,626:INFO:Creating metrics dataframe
2023-03-22 12:40:38,859:INFO:Uploading results into container
2023-03-22 12:40:38,859:INFO:Uploading model into container now
2023-03-22 12:40:38,860:INFO:_master_model_container: 11
2023-03-22 12:40:38,860:INFO:_display_container: 2
2023-03-22 12:40:38,860:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-22 12:40:38,860:INFO:create_model() successfully completed......................................
2023-03-22 12:40:38,979:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:38,979:INFO:Creating metrics dataframe
2023-03-22 12:40:38,992:INFO:Initializing Decision Tree Regressor
2023-03-22 12:40:38,992:INFO:Total runtime is 0.7399869879086812 minutes
2023-03-22 12:40:38,996:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:38,997:INFO:Initializing create_model()
2023-03-22 12:40:38,997:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:38,997:INFO:Checking exceptions
2023-03-22 12:40:38,997:INFO:Importing libraries
2023-03-22 12:40:38,997:INFO:Copying training dataset
2023-03-22 12:40:39,007:INFO:Defining folds
2023-03-22 12:40:39,007:INFO:Declaring metric variables
2023-03-22 12:40:39,011:INFO:Importing untrained model
2023-03-22 12:40:39,015:INFO:Decision Tree Regressor Imported successfully
2023-03-22 12:40:39,025:INFO:Starting cross validation
2023-03-22 12:40:39,028:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:42,373:INFO:Calculating mean and std
2023-03-22 12:40:42,375:INFO:Creating metrics dataframe
2023-03-22 12:40:42,637:INFO:Uploading results into container
2023-03-22 12:40:42,639:INFO:Uploading model into container now
2023-03-22 12:40:42,640:INFO:_master_model_container: 12
2023-03-22 12:40:42,640:INFO:_display_container: 2
2023-03-22 12:40:42,641:INFO:DecisionTreeRegressor(random_state=666)
2023-03-22 12:40:42,641:INFO:create_model() successfully completed......................................
2023-03-22 12:40:42,792:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:42,792:INFO:Creating metrics dataframe
2023-03-22 12:40:42,811:INFO:Initializing Random Forest Regressor
2023-03-22 12:40:42,811:INFO:Total runtime is 0.8036301533381144 minutes
2023-03-22 12:40:42,817:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:42,817:INFO:Initializing create_model()
2023-03-22 12:40:42,817:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:42,817:INFO:Checking exceptions
2023-03-22 12:40:42,818:INFO:Importing libraries
2023-03-22 12:40:42,818:INFO:Copying training dataset
2023-03-22 12:40:42,829:INFO:Defining folds
2023-03-22 12:40:42,829:INFO:Declaring metric variables
2023-03-22 12:40:42,834:INFO:Importing untrained model
2023-03-22 12:40:42,844:INFO:Random Forest Regressor Imported successfully
2023-03-22 12:40:42,857:INFO:Starting cross validation
2023-03-22 12:40:42,861:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:46,796:INFO:Calculating mean and std
2023-03-22 12:40:46,797:INFO:Creating metrics dataframe
2023-03-22 12:40:47,032:INFO:Uploading results into container
2023-03-22 12:40:47,033:INFO:Uploading model into container now
2023-03-22 12:40:47,034:INFO:_master_model_container: 13
2023-03-22 12:40:47,034:INFO:_display_container: 2
2023-03-22 12:40:47,034:INFO:RandomForestRegressor(n_jobs=-1, random_state=666)
2023-03-22 12:40:47,035:INFO:create_model() successfully completed......................................
2023-03-22 12:40:47,151:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:47,151:INFO:Creating metrics dataframe
2023-03-22 12:40:47,165:INFO:Initializing Extra Trees Regressor
2023-03-22 12:40:47,165:INFO:Total runtime is 0.8761986732482909 minutes
2023-03-22 12:40:47,169:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:47,169:INFO:Initializing create_model()
2023-03-22 12:40:47,169:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:47,170:INFO:Checking exceptions
2023-03-22 12:40:47,170:INFO:Importing libraries
2023-03-22 12:40:47,170:INFO:Copying training dataset
2023-03-22 12:40:47,178:INFO:Defining folds
2023-03-22 12:40:47,179:INFO:Declaring metric variables
2023-03-22 12:40:47,184:INFO:Importing untrained model
2023-03-22 12:40:47,192:INFO:Extra Trees Regressor Imported successfully
2023-03-22 12:40:47,202:INFO:Starting cross validation
2023-03-22 12:40:47,205:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:50,803:INFO:Calculating mean and std
2023-03-22 12:40:50,805:INFO:Creating metrics dataframe
2023-03-22 12:40:51,041:INFO:Uploading results into container
2023-03-22 12:40:51,043:INFO:Uploading model into container now
2023-03-22 12:40:51,043:INFO:_master_model_container: 14
2023-03-22 12:40:51,043:INFO:_display_container: 2
2023-03-22 12:40:51,044:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=666)
2023-03-22 12:40:51,044:INFO:create_model() successfully completed......................................
2023-03-22 12:40:51,166:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:51,167:INFO:Creating metrics dataframe
2023-03-22 12:40:51,181:INFO:Initializing AdaBoost Regressor
2023-03-22 12:40:51,181:INFO:Total runtime is 0.9431357026100158 minutes
2023-03-22 12:40:51,186:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:51,186:INFO:Initializing create_model()
2023-03-22 12:40:51,187:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:51,187:INFO:Checking exceptions
2023-03-22 12:40:51,187:INFO:Importing libraries
2023-03-22 12:40:51,187:INFO:Copying training dataset
2023-03-22 12:40:51,194:INFO:Defining folds
2023-03-22 12:40:51,194:INFO:Declaring metric variables
2023-03-22 12:40:51,198:INFO:Importing untrained model
2023-03-22 12:40:51,205:INFO:AdaBoost Regressor Imported successfully
2023-03-22 12:40:51,218:INFO:Starting cross validation
2023-03-22 12:40:51,223:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:54,096:INFO:Calculating mean and std
2023-03-22 12:40:54,097:INFO:Creating metrics dataframe
2023-03-22 12:40:54,337:INFO:Uploading results into container
2023-03-22 12:40:54,338:INFO:Uploading model into container now
2023-03-22 12:40:54,339:INFO:_master_model_container: 15
2023-03-22 12:40:54,339:INFO:_display_container: 2
2023-03-22 12:40:54,339:INFO:AdaBoostRegressor(random_state=666)
2023-03-22 12:40:54,340:INFO:create_model() successfully completed......................................
2023-03-22 12:40:54,456:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:54,456:INFO:Creating metrics dataframe
2023-03-22 12:40:54,470:INFO:Initializing Gradient Boosting Regressor
2023-03-22 12:40:54,470:INFO:Total runtime is 0.9979582587877909 minutes
2023-03-22 12:40:54,474:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:54,474:INFO:Initializing create_model()
2023-03-22 12:40:54,475:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:54,475:INFO:Checking exceptions
2023-03-22 12:40:54,475:INFO:Importing libraries
2023-03-22 12:40:54,475:INFO:Copying training dataset
2023-03-22 12:40:54,484:INFO:Defining folds
2023-03-22 12:40:54,484:INFO:Declaring metric variables
2023-03-22 12:40:54,488:INFO:Importing untrained model
2023-03-22 12:40:54,494:INFO:Gradient Boosting Regressor Imported successfully
2023-03-22 12:40:54,507:INFO:Starting cross validation
2023-03-22 12:40:54,510:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:40:57,755:INFO:Calculating mean and std
2023-03-22 12:40:57,756:INFO:Creating metrics dataframe
2023-03-22 12:40:58,005:INFO:Uploading results into container
2023-03-22 12:40:58,006:INFO:Uploading model into container now
2023-03-22 12:40:58,006:INFO:_master_model_container: 16
2023-03-22 12:40:58,006:INFO:_display_container: 2
2023-03-22 12:40:58,007:INFO:GradientBoostingRegressor(random_state=666)
2023-03-22 12:40:58,007:INFO:create_model() successfully completed......................................
2023-03-22 12:40:58,129:INFO:SubProcess create_model() end ==================================
2023-03-22 12:40:58,129:INFO:Creating metrics dataframe
2023-03-22 12:40:58,144:INFO:Initializing Extreme Gradient Boosting
2023-03-22 12:40:58,144:INFO:Total runtime is 1.0591892043749491 minutes
2023-03-22 12:40:58,151:INFO:SubProcess create_model() called ==================================
2023-03-22 12:40:58,151:INFO:Initializing create_model()
2023-03-22 12:40:58,151:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:40:58,151:INFO:Checking exceptions
2023-03-22 12:40:58,151:INFO:Importing libraries
2023-03-22 12:40:58,151:INFO:Copying training dataset
2023-03-22 12:40:58,159:INFO:Defining folds
2023-03-22 12:40:58,159:INFO:Declaring metric variables
2023-03-22 12:40:58,164:INFO:Importing untrained model
2023-03-22 12:40:58,170:INFO:Extreme Gradient Boosting Imported successfully
2023-03-22 12:40:58,179:INFO:Starting cross validation
2023-03-22 12:40:58,186:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:41:05,354:INFO:Calculating mean and std
2023-03-22 12:41:05,355:INFO:Creating metrics dataframe
2023-03-22 12:41:05,653:INFO:Uploading results into container
2023-03-22 12:41:05,654:INFO:Uploading model into container now
2023-03-22 12:41:05,655:INFO:_master_model_container: 17
2023-03-22 12:41:05,655:INFO:_display_container: 2
2023-03-22 12:41:05,657:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=666, ...)
2023-03-22 12:41:05,657:INFO:create_model() successfully completed......................................
2023-03-22 12:41:05,786:INFO:SubProcess create_model() end ==================================
2023-03-22 12:41:05,786:INFO:Creating metrics dataframe
2023-03-22 12:41:05,804:INFO:Initializing Light Gradient Boosting Machine
2023-03-22 12:41:05,804:INFO:Total runtime is 1.186847158273061 minutes
2023-03-22 12:41:05,808:INFO:SubProcess create_model() called ==================================
2023-03-22 12:41:05,809:INFO:Initializing create_model()
2023-03-22 12:41:05,809:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:41:05,809:INFO:Checking exceptions
2023-03-22 12:41:05,809:INFO:Importing libraries
2023-03-22 12:41:05,809:INFO:Copying training dataset
2023-03-22 12:41:05,821:INFO:Defining folds
2023-03-22 12:41:05,821:INFO:Declaring metric variables
2023-03-22 12:41:05,825:INFO:Importing untrained model
2023-03-22 12:41:05,835:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-22 12:41:05,843:INFO:Starting cross validation
2023-03-22 12:41:05,848:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:41:11,813:INFO:Calculating mean and std
2023-03-22 12:41:11,814:INFO:Creating metrics dataframe
2023-03-22 12:41:12,053:INFO:Uploading results into container
2023-03-22 12:41:12,053:INFO:Uploading model into container now
2023-03-22 12:41:12,054:INFO:_master_model_container: 18
2023-03-22 12:41:12,054:INFO:_display_container: 2
2023-03-22 12:41:12,055:INFO:LGBMRegressor(random_state=666)
2023-03-22 12:41:12,055:INFO:create_model() successfully completed......................................
2023-03-22 12:41:12,184:INFO:SubProcess create_model() end ==================================
2023-03-22 12:41:12,184:INFO:Creating metrics dataframe
2023-03-22 12:41:12,203:INFO:Initializing CatBoost Regressor
2023-03-22 12:41:12,203:INFO:Total runtime is 1.2934991836547851 minutes
2023-03-22 12:41:12,207:INFO:SubProcess create_model() called ==================================
2023-03-22 12:41:12,208:INFO:Initializing create_model()
2023-03-22 12:41:12,208:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:41:12,208:INFO:Checking exceptions
2023-03-22 12:41:12,208:INFO:Importing libraries
2023-03-22 12:41:12,208:INFO:Copying training dataset
2023-03-22 12:41:12,222:INFO:Defining folds
2023-03-22 12:41:12,222:INFO:Declaring metric variables
2023-03-22 12:41:12,229:INFO:Importing untrained model
2023-03-22 12:41:12,239:INFO:CatBoost Regressor Imported successfully
2023-03-22 12:41:12,257:INFO:Starting cross validation
2023-03-22 12:41:12,262:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:41:17,566:INFO:Calculating mean and std
2023-03-22 12:41:17,568:INFO:Creating metrics dataframe
2023-03-22 12:41:17,823:INFO:Uploading results into container
2023-03-22 12:41:17,825:INFO:Uploading model into container now
2023-03-22 12:41:17,825:INFO:_master_model_container: 19
2023-03-22 12:41:17,825:INFO:_display_container: 2
2023-03-22 12:41:17,825:INFO:<catboost.core.CatBoostRegressor object at 0x00000164FD3CB640>
2023-03-22 12:41:17,825:INFO:create_model() successfully completed......................................
2023-03-22 12:41:17,943:INFO:SubProcess create_model() end ==================================
2023-03-22 12:41:17,943:INFO:Creating metrics dataframe
2023-03-22 12:41:17,960:INFO:Initializing Dummy Regressor
2023-03-22 12:41:17,961:INFO:Total runtime is 1.3894643704096477 minutes
2023-03-22 12:41:17,967:INFO:SubProcess create_model() called ==================================
2023-03-22 12:41:17,967:INFO:Initializing create_model()
2023-03-22 12:41:17,968:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164EC905F40>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:41:17,968:INFO:Checking exceptions
2023-03-22 12:41:17,969:INFO:Importing libraries
2023-03-22 12:41:17,969:INFO:Copying training dataset
2023-03-22 12:41:17,980:INFO:Defining folds
2023-03-22 12:41:17,980:INFO:Declaring metric variables
2023-03-22 12:41:17,984:INFO:Importing untrained model
2023-03-22 12:41:17,991:INFO:Dummy Regressor Imported successfully
2023-03-22 12:41:18,006:INFO:Starting cross validation
2023-03-22 12:41:18,012:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 12:41:21,062:INFO:Calculating mean and std
2023-03-22 12:41:21,064:INFO:Creating metrics dataframe
2023-03-22 12:41:21,302:INFO:Uploading results into container
2023-03-22 12:41:21,303:INFO:Uploading model into container now
2023-03-22 12:41:21,303:INFO:_master_model_container: 20
2023-03-22 12:41:21,303:INFO:_display_container: 2
2023-03-22 12:41:21,303:INFO:DummyRegressor()
2023-03-22 12:41:21,304:INFO:create_model() successfully completed......................................
2023-03-22 12:41:21,423:INFO:SubProcess create_model() end ==================================
2023-03-22 12:41:21,423:INFO:Creating metrics dataframe
2023-03-22 12:41:21,454:INFO:Initializing create_model()
2023-03-22 12:41:21,454:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164F7523220>, estimator=GradientBoostingRegressor(random_state=666), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-22 12:41:21,454:INFO:Checking exceptions
2023-03-22 12:41:21,456:INFO:Importing libraries
2023-03-22 12:41:21,456:INFO:Copying training dataset
2023-03-22 12:41:21,463:INFO:Defining folds
2023-03-22 12:41:21,463:INFO:Declaring metric variables
2023-03-22 12:41:21,464:INFO:Importing untrained model
2023-03-22 12:41:21,464:INFO:Declaring custom model
2023-03-22 12:41:21,464:INFO:Gradient Boosting Regressor Imported successfully
2023-03-22 12:41:21,467:INFO:Cross validation set to False
2023-03-22 12:41:21,467:INFO:Fitting Model
2023-03-22 12:41:22,121:INFO:GradientBoostingRegressor(random_state=666)
2023-03-22 12:41:22,121:INFO:create_model() successfully completed......................................
2023-03-22 12:41:22,288:INFO:_master_model_container: 20
2023-03-22 12:41:22,288:INFO:_display_container: 2
2023-03-22 12:41:22,289:INFO:GradientBoostingRegressor(random_state=666)
2023-03-22 12:41:22,289:INFO:compare_models() successfully completed......................................
2023-03-22 14:09:19,343:INFO:PyCaret RegressionExperiment
2023-03-22 14:09:19,343:INFO:Logging name: reg-default-name
2023-03-22 14:09:19,343:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-22 14:09:19,343:INFO:version 3.0.0
2023-03-22 14:09:19,343:INFO:Initializing setup()
2023-03-22 14:09:19,343:INFO:self.USI: 239c
2023-03-22 14:09:19,343:INFO:self._variable_keys: {'gpu_param', '_available_plots', 'X', 'html_param', '_ml_usecase', 'target_param', 'gpu_n_jobs_param', 'USI', 'X_test', 'logging_param', 'idx', 'data', 'fold_groups_param', 'memory', 'pipeline', 'log_plots_param', 'n_jobs_param', 'y_test', 'X_train', 'fold_generator', 'exp_id', 'transform_target_param', 'exp_name_log', 'seed', 'y_train', 'fold_shuffle_param', 'y'}
2023-03-22 14:09:19,344:INFO:Checking environment
2023-03-22 14:09:19,344:INFO:python_version: 3.9.12
2023-03-22 14:09:19,344:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-22 14:09:19,344:INFO:machine: AMD64
2023-03-22 14:09:19,344:INFO:platform: Windows-10-10.0.19045-SP0
2023-03-22 14:09:19,350:INFO:Memory: svmem(total=8496553984, available=1446203392, percent=83.0, used=7050350592, free=1446203392)
2023-03-22 14:09:19,350:INFO:Physical Core: 4
2023-03-22 14:09:19,350:INFO:Logical Core: 8
2023-03-22 14:09:19,350:INFO:Checking libraries
2023-03-22 14:09:19,350:INFO:System:
2023-03-22 14:09:19,350:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-22 14:09:19,350:INFO:executable: C:\Users\CHUKWUKA\anaconda3\python.exe
2023-03-22 14:09:19,351:INFO:   machine: Windows-10-10.0.19045-SP0
2023-03-22 14:09:19,351:INFO:PyCaret required dependencies:
2023-03-22 14:09:19,352:INFO:                 pip: 22.3.1
2023-03-22 14:09:19,352:INFO:          setuptools: 65.5.0
2023-03-22 14:09:19,353:INFO:             pycaret: 3.0.0
2023-03-22 14:09:19,353:INFO:             IPython: 8.7.0
2023-03-22 14:09:19,353:INFO:          ipywidgets: 7.6.5
2023-03-22 14:09:19,353:INFO:                tqdm: 4.64.1
2023-03-22 14:09:19,353:INFO:               numpy: 1.21.5
2023-03-22 14:09:19,353:INFO:              pandas: 1.4.4
2023-03-22 14:09:19,353:INFO:              jinja2: 2.11.3
2023-03-22 14:09:19,353:INFO:               scipy: 1.9.3
2023-03-22 14:09:19,353:INFO:              joblib: 1.2.0
2023-03-22 14:09:19,353:INFO:             sklearn: 1.0.2
2023-03-22 14:09:19,353:INFO:                pyod: 1.0.9
2023-03-22 14:09:19,353:INFO:            imblearn: 0.10.1
2023-03-22 14:09:19,353:INFO:   category_encoders: 2.6.0
2023-03-22 14:09:19,353:INFO:            lightgbm: 3.3.5
2023-03-22 14:09:19,353:INFO:               numba: 0.56.4
2023-03-22 14:09:19,353:INFO:            requests: 2.27.1
2023-03-22 14:09:19,353:INFO:          matplotlib: 3.6.2
2023-03-22 14:09:19,353:INFO:          scikitplot: 0.3.7
2023-03-22 14:09:19,353:INFO:         yellowbrick: 1.5
2023-03-22 14:09:19,353:INFO:              plotly: 5.9.0
2023-03-22 14:09:19,353:INFO:             kaleido: 0.2.1
2023-03-22 14:09:19,353:INFO:         statsmodels: 0.13.2
2023-03-22 14:09:19,354:INFO:              sktime: 0.16.1
2023-03-22 14:09:19,354:INFO:               tbats: 1.1.2
2023-03-22 14:09:19,354:INFO:            pmdarima: 2.0.3
2023-03-22 14:09:19,354:INFO:              psutil: 5.9.0
2023-03-22 14:09:19,354:INFO:PyCaret optional dependencies:
2023-03-22 14:09:19,354:INFO:                shap: Not installed
2023-03-22 14:09:19,354:INFO:           interpret: Not installed
2023-03-22 14:09:19,354:INFO:                umap: Not installed
2023-03-22 14:09:19,354:INFO:    pandas_profiling: Not installed
2023-03-22 14:09:19,354:INFO:  explainerdashboard: Not installed
2023-03-22 14:09:19,354:INFO:             autoviz: Not installed
2023-03-22 14:09:19,354:INFO:           fairlearn: Not installed
2023-03-22 14:09:19,354:INFO:             xgboost: 1.7.1
2023-03-22 14:09:19,354:INFO:            catboost: 1.0.6
2023-03-22 14:09:19,354:INFO:              kmodes: Not installed
2023-03-22 14:09:19,354:INFO:             mlxtend: Not installed
2023-03-22 14:09:19,354:INFO:       statsforecast: Not installed
2023-03-22 14:09:19,354:INFO:        tune_sklearn: Not installed
2023-03-22 14:09:19,354:INFO:                 ray: Not installed
2023-03-22 14:09:19,354:INFO:            hyperopt: Not installed
2023-03-22 14:09:19,354:INFO:              optuna: Not installed
2023-03-22 14:09:19,354:INFO:               skopt: Not installed
2023-03-22 14:09:19,355:INFO:              mlflow: Not installed
2023-03-22 14:09:19,355:INFO:              gradio: Not installed
2023-03-22 14:09:19,355:INFO:             fastapi: Not installed
2023-03-22 14:09:19,355:INFO:             uvicorn: Not installed
2023-03-22 14:09:19,355:INFO:              m2cgen: Not installed
2023-03-22 14:09:19,355:INFO:           evidently: Not installed
2023-03-22 14:09:19,355:INFO:               fugue: Not installed
2023-03-22 14:09:19,355:INFO:           streamlit: Not installed
2023-03-22 14:09:19,355:INFO:             prophet: Not installed
2023-03-22 14:09:19,355:INFO:None
2023-03-22 14:09:19,355:INFO:Set up data.
2023-03-22 14:09:19,374:INFO:Set up train/test split.
2023-03-22 14:09:19,378:INFO:Set up index.
2023-03-22 14:09:19,379:INFO:Set up folding strategy.
2023-03-22 14:09:19,379:INFO:Assigning column types.
2023-03-22 14:09:19,385:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-22 14:09:19,398:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,405:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,410:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,483:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,533:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,536:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:09:19,540:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:09:19,541:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,546:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,551:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,615:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,665:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,666:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:09:19,669:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:09:19,669:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-22 14:09:19,689:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,694:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,757:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,805:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,806:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:09:19,809:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:09:19,815:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,820:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,882:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,930:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:09:19,930:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:09:19,933:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:09:19,934:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-22 14:09:19,944:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 14:09:20,010:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:09:20,057:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:09:20,057:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:09:20,060:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:09:20,071:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 14:09:20,135:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:09:20,183:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:09:20,184:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:09:20,202:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:09:20,203:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-22 14:09:20,277:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:09:20,328:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:09:20,328:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:09:20,331:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:09:20,405:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:09:20,451:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:09:20,452:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:09:20,455:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:09:20,456:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-22 14:09:20,528:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:09:20,577:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:09:20,579:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:09:20,710:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:09:20,758:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:09:20,761:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:09:20,762:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-22 14:09:20,885:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:09:20,888:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:09:21,010:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:09:21,013:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:09:21,017:INFO:Preparing preprocessing pipeline...
2023-03-22 14:09:21,017:INFO:Set up simple imputation.
2023-03-22 14:09:21,019:INFO:Set up column name cleaning.
2023-03-22 14:09:21,111:INFO:Finished creating preprocessing pipeline.
2023-03-22 14:09:21,118:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\CHUKWUKA\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['onehotencoder__Item_Fat_Content_Low '
                                             'Fat',
                                             'onehotencoder__Item_Fat_Content_Regular',
                                             'onehotencoder__Outlet_Size_High',
                                             'onehotencoder__Outlet_Size_Medium',
                                             'onehotencoder__Outlet_Size_Small',
                                             'onehotencoder__Outlet_...
                                             'remainder__Item_Weight',
                                             'remainder__Item_Visibility',
                                             'remainder__Item_Type',
                                             'remainder__Item_MRP',
                                             'remainder__Outlet_Establishment_Year',
                                             'remainder__Item_Code'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-03-22 14:09:21,118:INFO:Creating final display dataframe.
2023-03-22 14:09:21,233:INFO:Setup _display_container:                     Description             Value
0                    Session id               666
1                        Target        Item_Sales
2                   Target type        Regression
3           Original data shape        (5966, 19)
4        Transformed data shape        (5966, 19)
5   Transformed train set shape        (4176, 19)
6    Transformed test set shape        (1790, 19)
7              Numeric features                18
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              239c
2023-03-22 14:09:21,381:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:09:21,384:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:09:21,513:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:09:21,515:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:09:21,518:INFO:setup() successfully completed in 2.42s...............
2023-03-22 14:10:04,554:INFO:Initializing compare_models()
2023-03-22 14:10:04,554:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, include=None, fold=None, round=4, cross_validation=True, sort=MSE, n_select=1, budget_time=10, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MSE', 'n_select': 1, 'budget_time': 10, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-22 14:10:04,555:INFO:Checking exceptions
2023-03-22 14:10:04,560:INFO:Preparing display monitor
2023-03-22 14:10:04,597:INFO:Time budget is 10 minutes
2023-03-22 14:10:04,598:INFO:Initializing Linear Regression
2023-03-22 14:10:04,598:INFO:Total runtime is 1.6717116038004556e-05 minutes
2023-03-22 14:10:04,602:INFO:SubProcess create_model() called ==================================
2023-03-22 14:10:04,603:INFO:Initializing create_model()
2023-03-22 14:10:04,603:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:10:04,603:INFO:Checking exceptions
2023-03-22 14:10:04,603:INFO:Importing libraries
2023-03-22 14:10:04,604:INFO:Copying training dataset
2023-03-22 14:10:04,610:INFO:Defining folds
2023-03-22 14:10:04,610:INFO:Declaring metric variables
2023-03-22 14:10:04,614:INFO:Importing untrained model
2023-03-22 14:10:04,619:INFO:Linear Regression Imported successfully
2023-03-22 14:10:04,631:INFO:Starting cross validation
2023-03-22 14:10:04,633:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:10:14,579:INFO:Calculating mean and std
2023-03-22 14:10:14,975:INFO:Creating metrics dataframe
2023-03-22 14:10:15,257:INFO:Uploading results into container
2023-03-22 14:10:15,258:INFO:Uploading model into container now
2023-03-22 14:10:15,259:INFO:_master_model_container: 1
2023-03-22 14:10:15,259:INFO:_display_container: 2
2023-03-22 14:10:15,259:INFO:LinearRegression(n_jobs=-1)
2023-03-22 14:10:15,259:INFO:create_model() successfully completed......................................
2023-03-22 14:10:19,917:INFO:SubProcess create_model() end ==================================
2023-03-22 14:10:19,918:INFO:Creating metrics dataframe
2023-03-22 14:10:19,950:INFO:Initializing Lasso Regression
2023-03-22 14:10:19,951:INFO:Total runtime is 0.25589638551076255 minutes
2023-03-22 14:10:19,955:INFO:SubProcess create_model() called ==================================
2023-03-22 14:10:19,955:INFO:Initializing create_model()
2023-03-22 14:10:19,955:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:10:19,955:INFO:Checking exceptions
2023-03-22 14:10:19,955:INFO:Importing libraries
2023-03-22 14:10:19,955:INFO:Copying training dataset
2023-03-22 14:10:19,962:INFO:Defining folds
2023-03-22 14:10:19,962:INFO:Declaring metric variables
2023-03-22 14:10:19,969:INFO:Importing untrained model
2023-03-22 14:10:19,975:INFO:Lasso Regression Imported successfully
2023-03-22 14:10:19,988:INFO:Starting cross validation
2023-03-22 14:10:19,989:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:10:21,719:INFO:Calculating mean and std
2023-03-22 14:10:21,720:INFO:Creating metrics dataframe
2023-03-22 14:10:21,995:INFO:Uploading results into container
2023-03-22 14:10:21,996:INFO:Uploading model into container now
2023-03-22 14:10:21,996:INFO:_master_model_container: 2
2023-03-22 14:10:21,997:INFO:_display_container: 2
2023-03-22 14:10:21,997:INFO:Lasso(random_state=666)
2023-03-22 14:10:21,997:INFO:create_model() successfully completed......................................
2023-03-22 14:10:22,123:INFO:SubProcess create_model() end ==================================
2023-03-22 14:10:22,123:INFO:Creating metrics dataframe
2023-03-22 14:10:22,151:INFO:Initializing Ridge Regression
2023-03-22 14:10:22,151:INFO:Total runtime is 0.29256967703501385 minutes
2023-03-22 14:10:22,155:INFO:SubProcess create_model() called ==================================
2023-03-22 14:10:22,156:INFO:Initializing create_model()
2023-03-22 14:10:22,156:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:10:22,157:INFO:Checking exceptions
2023-03-22 14:10:22,157:INFO:Importing libraries
2023-03-22 14:10:22,157:INFO:Copying training dataset
2023-03-22 14:10:22,165:INFO:Defining folds
2023-03-22 14:10:22,166:INFO:Declaring metric variables
2023-03-22 14:10:22,172:INFO:Importing untrained model
2023-03-22 14:10:22,178:INFO:Ridge Regression Imported successfully
2023-03-22 14:10:22,193:INFO:Starting cross validation
2023-03-22 14:10:22,195:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:10:24,034:INFO:Calculating mean and std
2023-03-22 14:10:24,036:INFO:Creating metrics dataframe
2023-03-22 14:10:24,276:INFO:Uploading results into container
2023-03-22 14:10:24,277:INFO:Uploading model into container now
2023-03-22 14:10:24,277:INFO:_master_model_container: 3
2023-03-22 14:10:24,277:INFO:_display_container: 2
2023-03-22 14:10:24,277:INFO:Ridge(random_state=666)
2023-03-22 14:10:24,277:INFO:create_model() successfully completed......................................
2023-03-22 14:10:24,407:INFO:SubProcess create_model() end ==================================
2023-03-22 14:10:24,407:INFO:Creating metrics dataframe
2023-03-22 14:10:24,450:INFO:Initializing Elastic Net
2023-03-22 14:10:24,450:INFO:Total runtime is 0.33088405529658 minutes
2023-03-22 14:10:24,455:INFO:SubProcess create_model() called ==================================
2023-03-22 14:10:24,455:INFO:Initializing create_model()
2023-03-22 14:10:24,455:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:10:24,455:INFO:Checking exceptions
2023-03-22 14:10:24,455:INFO:Importing libraries
2023-03-22 14:10:24,456:INFO:Copying training dataset
2023-03-22 14:10:24,461:INFO:Defining folds
2023-03-22 14:10:24,461:INFO:Declaring metric variables
2023-03-22 14:10:24,468:INFO:Importing untrained model
2023-03-22 14:10:24,473:INFO:Elastic Net Imported successfully
2023-03-22 14:10:24,482:INFO:Starting cross validation
2023-03-22 14:10:24,484:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:10:26,199:INFO:Calculating mean and std
2023-03-22 14:10:26,201:INFO:Creating metrics dataframe
2023-03-22 14:10:26,427:INFO:Uploading results into container
2023-03-22 14:10:26,428:INFO:Uploading model into container now
2023-03-22 14:10:26,429:INFO:_master_model_container: 4
2023-03-22 14:10:26,429:INFO:_display_container: 2
2023-03-22 14:10:26,429:INFO:ElasticNet(random_state=666)
2023-03-22 14:10:26,430:INFO:create_model() successfully completed......................................
2023-03-22 14:10:26,557:INFO:SubProcess create_model() end ==================================
2023-03-22 14:10:26,557:INFO:Creating metrics dataframe
2023-03-22 14:10:26,588:INFO:Initializing Least Angle Regression
2023-03-22 14:10:26,589:INFO:Total runtime is 0.3665335655212402 minutes
2023-03-22 14:10:26,593:INFO:SubProcess create_model() called ==================================
2023-03-22 14:10:26,593:INFO:Initializing create_model()
2023-03-22 14:10:26,594:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:10:26,594:INFO:Checking exceptions
2023-03-22 14:10:26,594:INFO:Importing libraries
2023-03-22 14:10:26,594:INFO:Copying training dataset
2023-03-22 14:10:26,602:INFO:Defining folds
2023-03-22 14:10:26,602:INFO:Declaring metric variables
2023-03-22 14:10:26,608:INFO:Importing untrained model
2023-03-22 14:10:26,617:INFO:Least Angle Regression Imported successfully
2023-03-22 14:10:26,629:INFO:Starting cross validation
2023-03-22 14:10:26,631:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:10:26,739:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:26,751:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.669e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:10:26,753:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:26,757:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.410e-02, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:10:26,759:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.747e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:10:26,759:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.438e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:10:26,768:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:26,783:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:26,800:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:26,814:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:26,827:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:26,850:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:27,185:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:27,189:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.839e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:10:27,190:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.382e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:10:27,191:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=8.997e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:10:27,192:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.442e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:10:27,194:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.397e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:10:27,207:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:28,377:INFO:Calculating mean and std
2023-03-22 14:10:28,378:INFO:Creating metrics dataframe
2023-03-22 14:10:28,632:INFO:Uploading results into container
2023-03-22 14:10:28,633:INFO:Uploading model into container now
2023-03-22 14:10:28,634:INFO:_master_model_container: 5
2023-03-22 14:10:28,634:INFO:_display_container: 2
2023-03-22 14:10:28,648:INFO:Lars(random_state=666)
2023-03-22 14:10:28,648:INFO:create_model() successfully completed......................................
2023-03-22 14:10:28,777:INFO:SubProcess create_model() end ==================================
2023-03-22 14:10:28,777:INFO:Creating metrics dataframe
2023-03-22 14:10:28,788:INFO:Initializing Lasso Least Angle Regression
2023-03-22 14:10:28,788:INFO:Total runtime is 0.4031825065612793 minutes
2023-03-22 14:10:28,792:INFO:SubProcess create_model() called ==================================
2023-03-22 14:10:28,793:INFO:Initializing create_model()
2023-03-22 14:10:28,793:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:10:28,793:INFO:Checking exceptions
2023-03-22 14:10:28,793:INFO:Importing libraries
2023-03-22 14:10:28,793:INFO:Copying training dataset
2023-03-22 14:10:28,801:INFO:Defining folds
2023-03-22 14:10:28,801:INFO:Declaring metric variables
2023-03-22 14:10:28,806:INFO:Importing untrained model
2023-03-22 14:10:28,810:INFO:Lasso Least Angle Regression Imported successfully
2023-03-22 14:10:28,818:INFO:Starting cross validation
2023-03-22 14:10:28,819:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:10:28,901:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:10:28,908:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:10:28,929:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:10:28,950:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:10:28,967:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:10:28,983:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:10:28,999:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:10:29,319:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:10:29,337:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:10:30,530:INFO:Calculating mean and std
2023-03-22 14:10:30,532:INFO:Creating metrics dataframe
2023-03-22 14:10:30,773:INFO:Uploading results into container
2023-03-22 14:10:30,774:INFO:Uploading model into container now
2023-03-22 14:10:30,775:INFO:_master_model_container: 6
2023-03-22 14:10:30,775:INFO:_display_container: 2
2023-03-22 14:10:30,775:INFO:LassoLars(random_state=666)
2023-03-22 14:10:30,776:INFO:create_model() successfully completed......................................
2023-03-22 14:10:30,906:INFO:SubProcess create_model() end ==================================
2023-03-22 14:10:30,906:INFO:Creating metrics dataframe
2023-03-22 14:10:30,941:INFO:Initializing Orthogonal Matching Pursuit
2023-03-22 14:10:30,941:INFO:Total runtime is 0.4390560706456502 minutes
2023-03-22 14:10:30,946:INFO:SubProcess create_model() called ==================================
2023-03-22 14:10:30,946:INFO:Initializing create_model()
2023-03-22 14:10:30,946:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:10:30,947:INFO:Checking exceptions
2023-03-22 14:10:30,947:INFO:Importing libraries
2023-03-22 14:10:30,947:INFO:Copying training dataset
2023-03-22 14:10:30,953:INFO:Defining folds
2023-03-22 14:10:30,953:INFO:Declaring metric variables
2023-03-22 14:10:30,958:INFO:Importing untrained model
2023-03-22 14:10:30,961:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-22 14:10:30,971:INFO:Starting cross validation
2023-03-22 14:10:30,972:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:10:31,044:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:31,062:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:31,073:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:31,092:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:31,109:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:31,131:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:31,141:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:31,155:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:31,486:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:31,488:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:10:32,703:INFO:Calculating mean and std
2023-03-22 14:10:32,704:INFO:Creating metrics dataframe
2023-03-22 14:10:32,972:INFO:Uploading results into container
2023-03-22 14:10:32,972:INFO:Uploading model into container now
2023-03-22 14:10:32,973:INFO:_master_model_container: 7
2023-03-22 14:10:32,973:INFO:_display_container: 2
2023-03-22 14:10:32,973:INFO:OrthogonalMatchingPursuit()
2023-03-22 14:10:32,973:INFO:create_model() successfully completed......................................
2023-03-22 14:10:33,101:INFO:SubProcess create_model() end ==================================
2023-03-22 14:10:33,101:INFO:Creating metrics dataframe
2023-03-22 14:10:33,115:INFO:Initializing Bayesian Ridge
2023-03-22 14:10:33,116:INFO:Total runtime is 0.47532035907109577 minutes
2023-03-22 14:10:33,119:INFO:SubProcess create_model() called ==================================
2023-03-22 14:10:33,120:INFO:Initializing create_model()
2023-03-22 14:10:33,120:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:10:33,120:INFO:Checking exceptions
2023-03-22 14:10:33,120:INFO:Importing libraries
2023-03-22 14:10:33,120:INFO:Copying training dataset
2023-03-22 14:10:33,126:INFO:Defining folds
2023-03-22 14:10:33,126:INFO:Declaring metric variables
2023-03-22 14:10:33,131:INFO:Importing untrained model
2023-03-22 14:10:33,136:INFO:Bayesian Ridge Imported successfully
2023-03-22 14:10:33,145:INFO:Starting cross validation
2023-03-22 14:10:33,147:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:10:34,850:INFO:Calculating mean and std
2023-03-22 14:10:34,851:INFO:Creating metrics dataframe
2023-03-22 14:10:35,090:INFO:Uploading results into container
2023-03-22 14:10:35,091:INFO:Uploading model into container now
2023-03-22 14:10:35,091:INFO:_master_model_container: 8
2023-03-22 14:10:35,091:INFO:_display_container: 2
2023-03-22 14:10:35,092:INFO:BayesianRidge()
2023-03-22 14:10:35,092:INFO:create_model() successfully completed......................................
2023-03-22 14:10:35,220:INFO:SubProcess create_model() end ==================================
2023-03-22 14:10:35,220:INFO:Creating metrics dataframe
2023-03-22 14:10:35,233:INFO:Initializing Passive Aggressive Regressor
2023-03-22 14:10:35,233:INFO:Total runtime is 0.5105928063392638 minutes
2023-03-22 14:10:35,237:INFO:SubProcess create_model() called ==================================
2023-03-22 14:10:35,238:INFO:Initializing create_model()
2023-03-22 14:10:35,238:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:10:35,238:INFO:Checking exceptions
2023-03-22 14:10:35,238:INFO:Importing libraries
2023-03-22 14:10:35,238:INFO:Copying training dataset
2023-03-22 14:10:35,243:INFO:Defining folds
2023-03-22 14:10:35,243:INFO:Declaring metric variables
2023-03-22 14:10:35,248:INFO:Importing untrained model
2023-03-22 14:10:35,252:INFO:Passive Aggressive Regressor Imported successfully
2023-03-22 14:10:35,259:INFO:Starting cross validation
2023-03-22 14:10:35,284:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:10:36,994:INFO:Calculating mean and std
2023-03-22 14:10:36,995:INFO:Creating metrics dataframe
2023-03-22 14:10:37,227:INFO:Uploading results into container
2023-03-22 14:10:37,228:INFO:Uploading model into container now
2023-03-22 14:10:37,229:INFO:_master_model_container: 9
2023-03-22 14:10:37,229:INFO:_display_container: 2
2023-03-22 14:10:37,229:INFO:PassiveAggressiveRegressor(random_state=666)
2023-03-22 14:10:37,229:INFO:create_model() successfully completed......................................
2023-03-22 14:10:37,355:INFO:SubProcess create_model() end ==================================
2023-03-22 14:10:37,355:INFO:Creating metrics dataframe
2023-03-22 14:10:37,370:INFO:Initializing Huber Regressor
2023-03-22 14:10:37,370:INFO:Total runtime is 0.5462121963500975 minutes
2023-03-22 14:10:37,374:INFO:SubProcess create_model() called ==================================
2023-03-22 14:10:37,374:INFO:Initializing create_model()
2023-03-22 14:10:37,374:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:10:37,374:INFO:Checking exceptions
2023-03-22 14:10:37,374:INFO:Importing libraries
2023-03-22 14:10:37,375:INFO:Copying training dataset
2023-03-22 14:10:37,382:INFO:Defining folds
2023-03-22 14:10:37,382:INFO:Declaring metric variables
2023-03-22 14:10:37,386:INFO:Importing untrained model
2023-03-22 14:10:37,390:INFO:Huber Regressor Imported successfully
2023-03-22 14:10:37,398:INFO:Starting cross validation
2023-03-22 14:10:37,399:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:10:37,793:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:10:37,808:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:10:37,825:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:10:37,841:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:10:37,868:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:10:37,887:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:10:37,902:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:10:37,915:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:10:38,424:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:10:38,430:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:10:39,537:INFO:Calculating mean and std
2023-03-22 14:10:39,538:INFO:Creating metrics dataframe
2023-03-22 14:10:39,803:INFO:Uploading results into container
2023-03-22 14:10:39,803:INFO:Uploading model into container now
2023-03-22 14:10:39,804:INFO:_master_model_container: 10
2023-03-22 14:10:39,804:INFO:_display_container: 2
2023-03-22 14:10:39,805:INFO:HuberRegressor()
2023-03-22 14:10:39,805:INFO:create_model() successfully completed......................................
2023-03-22 14:10:39,933:INFO:SubProcess create_model() end ==================================
2023-03-22 14:10:39,933:INFO:Creating metrics dataframe
2023-03-22 14:10:39,946:INFO:Initializing K Neighbors Regressor
2023-03-22 14:10:39,946:INFO:Total runtime is 0.5891464789708454 minutes
2023-03-22 14:10:39,950:INFO:SubProcess create_model() called ==================================
2023-03-22 14:10:39,950:INFO:Initializing create_model()
2023-03-22 14:10:39,950:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:10:39,950:INFO:Checking exceptions
2023-03-22 14:10:39,950:INFO:Importing libraries
2023-03-22 14:10:39,950:INFO:Copying training dataset
2023-03-22 14:10:39,956:INFO:Defining folds
2023-03-22 14:10:39,956:INFO:Declaring metric variables
2023-03-22 14:10:39,960:INFO:Importing untrained model
2023-03-22 14:10:39,965:INFO:K Neighbors Regressor Imported successfully
2023-03-22 14:10:39,972:INFO:Starting cross validation
2023-03-22 14:10:39,973:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:10:41,915:INFO:Calculating mean and std
2023-03-22 14:10:41,916:INFO:Creating metrics dataframe
2023-03-22 14:10:42,159:INFO:Uploading results into container
2023-03-22 14:10:42,159:INFO:Uploading model into container now
2023-03-22 14:10:42,160:INFO:_master_model_container: 11
2023-03-22 14:10:42,160:INFO:_display_container: 2
2023-03-22 14:10:42,160:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-22 14:10:42,161:INFO:create_model() successfully completed......................................
2023-03-22 14:10:42,285:INFO:SubProcess create_model() end ==================================
2023-03-22 14:10:42,285:INFO:Creating metrics dataframe
2023-03-22 14:10:42,300:INFO:Initializing Decision Tree Regressor
2023-03-22 14:10:42,300:INFO:Total runtime is 0.628385265668233 minutes
2023-03-22 14:10:42,303:INFO:SubProcess create_model() called ==================================
2023-03-22 14:10:42,304:INFO:Initializing create_model()
2023-03-22 14:10:42,304:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:10:42,304:INFO:Checking exceptions
2023-03-22 14:10:42,304:INFO:Importing libraries
2023-03-22 14:10:42,304:INFO:Copying training dataset
2023-03-22 14:10:42,313:INFO:Defining folds
2023-03-22 14:10:42,313:INFO:Declaring metric variables
2023-03-22 14:10:42,318:INFO:Importing untrained model
2023-03-22 14:10:42,321:INFO:Decision Tree Regressor Imported successfully
2023-03-22 14:10:42,329:INFO:Starting cross validation
2023-03-22 14:10:42,330:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:10:44,165:INFO:Calculating mean and std
2023-03-22 14:10:44,166:INFO:Creating metrics dataframe
2023-03-22 14:10:44,401:INFO:Uploading results into container
2023-03-22 14:10:44,401:INFO:Uploading model into container now
2023-03-22 14:10:44,402:INFO:_master_model_container: 12
2023-03-22 14:10:44,402:INFO:_display_container: 2
2023-03-22 14:10:44,402:INFO:DecisionTreeRegressor(random_state=666)
2023-03-22 14:10:44,403:INFO:create_model() successfully completed......................................
2023-03-22 14:10:44,529:INFO:SubProcess create_model() end ==================================
2023-03-22 14:10:44,529:INFO:Creating metrics dataframe
2023-03-22 14:10:44,545:INFO:Initializing Random Forest Regressor
2023-03-22 14:10:44,550:INFO:Total runtime is 0.665883966286977 minutes
2023-03-22 14:10:44,555:INFO:SubProcess create_model() called ==================================
2023-03-22 14:10:44,555:INFO:Initializing create_model()
2023-03-22 14:10:44,555:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:10:44,555:INFO:Checking exceptions
2023-03-22 14:10:44,555:INFO:Importing libraries
2023-03-22 14:10:44,555:INFO:Copying training dataset
2023-03-22 14:10:44,564:INFO:Defining folds
2023-03-22 14:10:44,564:INFO:Declaring metric variables
2023-03-22 14:10:44,568:INFO:Importing untrained model
2023-03-22 14:10:44,574:INFO:Random Forest Regressor Imported successfully
2023-03-22 14:10:44,584:INFO:Starting cross validation
2023-03-22 14:10:44,585:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:10:48,372:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 14:10:48,961:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 14:10:52,858:INFO:Calculating mean and std
2023-03-22 14:10:52,860:INFO:Creating metrics dataframe
2023-03-22 14:10:53,095:INFO:Uploading results into container
2023-03-22 14:10:53,096:INFO:Uploading model into container now
2023-03-22 14:10:53,097:INFO:_master_model_container: 13
2023-03-22 14:10:53,097:INFO:_display_container: 2
2023-03-22 14:10:53,098:INFO:RandomForestRegressor(n_jobs=-1, random_state=666)
2023-03-22 14:10:53,098:INFO:create_model() successfully completed......................................
2023-03-22 14:10:53,230:INFO:SubProcess create_model() end ==================================
2023-03-22 14:10:53,230:INFO:Creating metrics dataframe
2023-03-22 14:10:53,244:INFO:Initializing Extra Trees Regressor
2023-03-22 14:10:53,244:INFO:Total runtime is 0.8107777516047159 minutes
2023-03-22 14:10:53,248:INFO:SubProcess create_model() called ==================================
2023-03-22 14:10:53,248:INFO:Initializing create_model()
2023-03-22 14:10:53,249:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:10:53,249:INFO:Checking exceptions
2023-03-22 14:10:53,249:INFO:Importing libraries
2023-03-22 14:10:53,250:INFO:Copying training dataset
2023-03-22 14:10:53,255:INFO:Defining folds
2023-03-22 14:10:53,256:INFO:Declaring metric variables
2023-03-22 14:10:53,260:INFO:Importing untrained model
2023-03-22 14:10:53,264:INFO:Extra Trees Regressor Imported successfully
2023-03-22 14:10:53,272:INFO:Starting cross validation
2023-03-22 14:10:53,274:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:10:56,580:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 14:10:57,054:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.12s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 14:10:57,435:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 14:10:57,940:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 14:10:57,940:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 14:10:57,940:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 14:10:57,940:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 14:10:59,283:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-22 14:11:01,466:INFO:Calculating mean and std
2023-03-22 14:11:01,468:INFO:Creating metrics dataframe
2023-03-22 14:11:01,765:INFO:Uploading results into container
2023-03-22 14:11:01,766:INFO:Uploading model into container now
2023-03-22 14:11:01,767:INFO:_master_model_container: 14
2023-03-22 14:11:01,767:INFO:_display_container: 2
2023-03-22 14:11:01,768:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=666)
2023-03-22 14:11:01,769:INFO:create_model() successfully completed......................................
2023-03-22 14:11:01,916:INFO:SubProcess create_model() end ==================================
2023-03-22 14:11:01,916:INFO:Creating metrics dataframe
2023-03-22 14:11:02,071:INFO:Initializing AdaBoost Regressor
2023-03-22 14:11:02,071:INFO:Total runtime is 0.9578991055488586 minutes
2023-03-22 14:11:02,075:INFO:SubProcess create_model() called ==================================
2023-03-22 14:11:02,075:INFO:Initializing create_model()
2023-03-22 14:11:02,075:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:11:02,075:INFO:Checking exceptions
2023-03-22 14:11:02,075:INFO:Importing libraries
2023-03-22 14:11:02,075:INFO:Copying training dataset
2023-03-22 14:11:02,082:INFO:Defining folds
2023-03-22 14:11:02,082:INFO:Declaring metric variables
2023-03-22 14:11:02,086:INFO:Importing untrained model
2023-03-22 14:11:02,090:INFO:AdaBoost Regressor Imported successfully
2023-03-22 14:11:02,098:INFO:Starting cross validation
2023-03-22 14:11:02,099:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:11:05,238:INFO:Calculating mean and std
2023-03-22 14:11:05,240:INFO:Creating metrics dataframe
2023-03-22 14:11:05,776:INFO:Uploading results into container
2023-03-22 14:11:05,777:INFO:Uploading model into container now
2023-03-22 14:11:05,777:INFO:_master_model_container: 15
2023-03-22 14:11:05,778:INFO:_display_container: 2
2023-03-22 14:11:05,778:INFO:AdaBoostRegressor(random_state=666)
2023-03-22 14:11:05,778:INFO:create_model() successfully completed......................................
2023-03-22 14:11:05,898:INFO:SubProcess create_model() end ==================================
2023-03-22 14:11:05,898:INFO:Creating metrics dataframe
2023-03-22 14:11:07,061:INFO:Initializing Gradient Boosting Regressor
2023-03-22 14:11:07,061:INFO:Total runtime is 1.0410595615704854 minutes
2023-03-22 14:11:07,065:INFO:SubProcess create_model() called ==================================
2023-03-22 14:11:07,066:INFO:Initializing create_model()
2023-03-22 14:11:07,066:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:11:07,066:INFO:Checking exceptions
2023-03-22 14:11:07,066:INFO:Importing libraries
2023-03-22 14:11:07,066:INFO:Copying training dataset
2023-03-22 14:11:07,072:INFO:Defining folds
2023-03-22 14:11:07,072:INFO:Declaring metric variables
2023-03-22 14:11:07,076:INFO:Importing untrained model
2023-03-22 14:11:07,079:INFO:Gradient Boosting Regressor Imported successfully
2023-03-22 14:11:07,087:INFO:Starting cross validation
2023-03-22 14:11:07,087:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:11:10,292:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 14:11:11,404:INFO:Calculating mean and std
2023-03-22 14:11:11,405:INFO:Creating metrics dataframe
2023-03-22 14:11:11,683:INFO:Uploading results into container
2023-03-22 14:11:11,685:INFO:Uploading model into container now
2023-03-22 14:11:11,686:INFO:_master_model_container: 16
2023-03-22 14:11:11,686:INFO:_display_container: 2
2023-03-22 14:11:11,686:INFO:GradientBoostingRegressor(random_state=666)
2023-03-22 14:11:11,687:INFO:create_model() successfully completed......................................
2023-03-22 14:11:11,812:INFO:SubProcess create_model() end ==================================
2023-03-22 14:11:11,812:INFO:Creating metrics dataframe
2023-03-22 14:11:11,872:INFO:Initializing Extreme Gradient Boosting
2023-03-22 14:11:11,872:INFO:Total runtime is 1.1212398886680603 minutes
2023-03-22 14:11:11,876:INFO:SubProcess create_model() called ==================================
2023-03-22 14:11:11,876:INFO:Initializing create_model()
2023-03-22 14:11:11,876:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:11:11,877:INFO:Checking exceptions
2023-03-22 14:11:11,877:INFO:Importing libraries
2023-03-22 14:11:11,877:INFO:Copying training dataset
2023-03-22 14:11:11,884:INFO:Defining folds
2023-03-22 14:11:11,885:INFO:Declaring metric variables
2023-03-22 14:11:11,890:INFO:Importing untrained model
2023-03-22 14:11:11,894:INFO:Extreme Gradient Boosting Imported successfully
2023-03-22 14:11:11,902:INFO:Starting cross validation
2023-03-22 14:11:11,903:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:11:17,495:INFO:Calculating mean and std
2023-03-22 14:11:17,496:INFO:Creating metrics dataframe
2023-03-22 14:11:17,791:INFO:Uploading results into container
2023-03-22 14:11:17,792:INFO:Uploading model into container now
2023-03-22 14:11:17,792:INFO:_master_model_container: 17
2023-03-22 14:11:17,792:INFO:_display_container: 2
2023-03-22 14:11:17,793:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=666, ...)
2023-03-22 14:11:17,794:INFO:create_model() successfully completed......................................
2023-03-22 14:11:17,921:INFO:SubProcess create_model() end ==================================
2023-03-22 14:11:17,921:INFO:Creating metrics dataframe
2023-03-22 14:11:17,935:INFO:Initializing Light Gradient Boosting Machine
2023-03-22 14:11:17,935:INFO:Total runtime is 1.2222976843516031 minutes
2023-03-22 14:11:17,939:INFO:SubProcess create_model() called ==================================
2023-03-22 14:11:17,939:INFO:Initializing create_model()
2023-03-22 14:11:17,939:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:11:17,939:INFO:Checking exceptions
2023-03-22 14:11:17,939:INFO:Importing libraries
2023-03-22 14:11:17,939:INFO:Copying training dataset
2023-03-22 14:11:17,945:INFO:Defining folds
2023-03-22 14:11:17,945:INFO:Declaring metric variables
2023-03-22 14:11:17,950:INFO:Importing untrained model
2023-03-22 14:11:17,954:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-22 14:11:17,961:INFO:Starting cross validation
2023-03-22 14:11:17,962:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:11:28,409:INFO:Calculating mean and std
2023-03-22 14:11:28,410:INFO:Creating metrics dataframe
2023-03-22 14:11:28,697:INFO:Uploading results into container
2023-03-22 14:11:28,698:INFO:Uploading model into container now
2023-03-22 14:11:28,699:INFO:_master_model_container: 18
2023-03-22 14:11:28,699:INFO:_display_container: 2
2023-03-22 14:11:28,699:INFO:LGBMRegressor(random_state=666)
2023-03-22 14:11:28,699:INFO:create_model() successfully completed......................................
2023-03-22 14:11:28,825:INFO:SubProcess create_model() end ==================================
2023-03-22 14:11:28,825:INFO:Creating metrics dataframe
2023-03-22 14:11:28,841:INFO:Initializing CatBoost Regressor
2023-03-22 14:11:28,841:INFO:Total runtime is 1.4040673613548278 minutes
2023-03-22 14:11:28,846:INFO:SubProcess create_model() called ==================================
2023-03-22 14:11:28,846:INFO:Initializing create_model()
2023-03-22 14:11:28,847:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:11:28,847:INFO:Checking exceptions
2023-03-22 14:11:28,847:INFO:Importing libraries
2023-03-22 14:11:28,847:INFO:Copying training dataset
2023-03-22 14:11:28,853:INFO:Defining folds
2023-03-22 14:11:28,853:INFO:Declaring metric variables
2023-03-22 14:11:28,857:INFO:Importing untrained model
2023-03-22 14:11:28,933:INFO:CatBoost Regressor Imported successfully
2023-03-22 14:11:28,940:INFO:Starting cross validation
2023-03-22 14:11:28,941:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:11:49,458:INFO:Calculating mean and std
2023-03-22 14:11:49,459:INFO:Creating metrics dataframe
2023-03-22 14:11:49,812:INFO:Uploading results into container
2023-03-22 14:11:49,813:INFO:Uploading model into container now
2023-03-22 14:11:49,813:INFO:_master_model_container: 19
2023-03-22 14:11:49,813:INFO:_display_container: 2
2023-03-22 14:11:49,814:INFO:<catboost.core.CatBoostRegressor object at 0x00000164FD49F7F0>
2023-03-22 14:11:49,814:INFO:create_model() successfully completed......................................
2023-03-22 14:11:49,949:INFO:SubProcess create_model() end ==================================
2023-03-22 14:11:49,949:INFO:Creating metrics dataframe
2023-03-22 14:11:49,965:INFO:Initializing Dummy Regressor
2023-03-22 14:11:49,965:INFO:Total runtime is 1.7561335881551106 minutes
2023-03-22 14:11:49,969:INFO:SubProcess create_model() called ==================================
2023-03-22 14:11:49,969:INFO:Initializing create_model()
2023-03-22 14:11:49,969:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD9622B0>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:11:49,970:INFO:Checking exceptions
2023-03-22 14:11:49,970:INFO:Importing libraries
2023-03-22 14:11:49,970:INFO:Copying training dataset
2023-03-22 14:11:49,978:INFO:Defining folds
2023-03-22 14:11:49,978:INFO:Declaring metric variables
2023-03-22 14:11:49,982:INFO:Importing untrained model
2023-03-22 14:11:49,988:INFO:Dummy Regressor Imported successfully
2023-03-22 14:11:50,003:INFO:Starting cross validation
2023-03-22 14:11:50,006:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:11:52,402:INFO:Calculating mean and std
2023-03-22 14:11:52,403:INFO:Creating metrics dataframe
2023-03-22 14:11:52,691:INFO:Uploading results into container
2023-03-22 14:11:52,692:INFO:Uploading model into container now
2023-03-22 14:11:52,692:INFO:_master_model_container: 20
2023-03-22 14:11:52,692:INFO:_display_container: 2
2023-03-22 14:11:52,693:INFO:DummyRegressor()
2023-03-22 14:11:52,693:INFO:create_model() successfully completed......................................
2023-03-22 14:11:52,813:INFO:SubProcess create_model() end ==================================
2023-03-22 14:11:52,813:INFO:Creating metrics dataframe
2023-03-22 14:11:52,841:INFO:Initializing create_model()
2023-03-22 14:11:52,841:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=GradientBoostingRegressor(random_state=666), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:11:52,841:INFO:Checking exceptions
2023-03-22 14:11:52,843:INFO:Importing libraries
2023-03-22 14:11:52,843:INFO:Copying training dataset
2023-03-22 14:11:52,849:INFO:Defining folds
2023-03-22 14:11:52,849:INFO:Declaring metric variables
2023-03-22 14:11:52,850:INFO:Importing untrained model
2023-03-22 14:11:52,850:INFO:Declaring custom model
2023-03-22 14:11:52,851:INFO:Gradient Boosting Regressor Imported successfully
2023-03-22 14:11:52,852:INFO:Cross validation set to False
2023-03-22 14:11:52,852:INFO:Fitting Model
2023-03-22 14:11:54,089:INFO:GradientBoostingRegressor(random_state=666)
2023-03-22 14:11:54,089:INFO:create_model() successfully completed......................................
2023-03-22 14:11:54,288:INFO:_master_model_container: 20
2023-03-22 14:11:54,288:INFO:_display_container: 2
2023-03-22 14:11:54,289:INFO:GradientBoostingRegressor(random_state=666)
2023-03-22 14:11:54,289:INFO:compare_models() successfully completed......................................
2023-03-22 14:14:36,938:INFO:Initializing predict_model()
2023-03-22 14:14:36,938:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000164FDC0DC70>, estimator=GradientBoostingRegressor(random_state=666), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x00000164F43861F0>)
2023-03-22 14:14:36,938:INFO:Checking exceptions
2023-03-22 14:14:36,939:INFO:Preloading libraries
2023-03-22 14:14:36,940:INFO:Set up data.
2023-03-22 14:14:36,977:INFO:Set up index.
2023-03-22 14:45:49,383:INFO:PyCaret RegressionExperiment
2023-03-22 14:45:49,383:INFO:Logging name: reg-default-name
2023-03-22 14:45:49,383:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-22 14:45:49,383:INFO:version 3.0.0
2023-03-22 14:45:49,384:INFO:Initializing setup()
2023-03-22 14:45:49,384:INFO:self.USI: ad5e
2023-03-22 14:45:49,384:INFO:self._variable_keys: {'gpu_param', '_available_plots', 'X', 'html_param', '_ml_usecase', 'target_param', 'gpu_n_jobs_param', 'USI', 'X_test', 'logging_param', 'idx', 'data', 'fold_groups_param', 'memory', 'pipeline', 'log_plots_param', 'n_jobs_param', 'y_test', 'X_train', 'fold_generator', 'exp_id', 'transform_target_param', 'exp_name_log', 'seed', 'y_train', 'fold_shuffle_param', 'y'}
2023-03-22 14:45:49,384:INFO:Checking environment
2023-03-22 14:45:49,384:INFO:python_version: 3.9.12
2023-03-22 14:45:49,384:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-22 14:45:49,384:INFO:machine: AMD64
2023-03-22 14:45:49,384:INFO:platform: Windows-10-10.0.19045-SP0
2023-03-22 14:45:49,384:INFO:Memory: svmem(total=8496553984, available=1974542336, percent=76.8, used=6522011648, free=1974542336)
2023-03-22 14:45:49,384:INFO:Physical Core: 4
2023-03-22 14:45:49,384:INFO:Logical Core: 8
2023-03-22 14:45:49,384:INFO:Checking libraries
2023-03-22 14:45:49,384:INFO:System:
2023-03-22 14:45:49,384:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-22 14:45:49,384:INFO:executable: C:\Users\CHUKWUKA\anaconda3\python.exe
2023-03-22 14:45:49,384:INFO:   machine: Windows-10-10.0.19045-SP0
2023-03-22 14:45:49,384:INFO:PyCaret required dependencies:
2023-03-22 14:45:49,384:INFO:                 pip: 22.3.1
2023-03-22 14:45:49,385:INFO:          setuptools: 65.5.0
2023-03-22 14:45:49,385:INFO:             pycaret: 3.0.0
2023-03-22 14:45:49,385:INFO:             IPython: 8.7.0
2023-03-22 14:45:49,385:INFO:          ipywidgets: 7.6.5
2023-03-22 14:45:49,385:INFO:                tqdm: 4.64.1
2023-03-22 14:45:49,385:INFO:               numpy: 1.21.5
2023-03-22 14:45:49,385:INFO:              pandas: 1.4.4
2023-03-22 14:45:49,385:INFO:              jinja2: 2.11.3
2023-03-22 14:45:49,385:INFO:               scipy: 1.9.3
2023-03-22 14:45:49,385:INFO:              joblib: 1.2.0
2023-03-22 14:45:49,385:INFO:             sklearn: 1.0.2
2023-03-22 14:45:49,385:INFO:                pyod: 1.0.9
2023-03-22 14:45:49,385:INFO:            imblearn: 0.10.1
2023-03-22 14:45:49,385:INFO:   category_encoders: 2.6.0
2023-03-22 14:45:49,385:INFO:            lightgbm: 3.3.5
2023-03-22 14:45:49,385:INFO:               numba: 0.56.4
2023-03-22 14:45:49,385:INFO:            requests: 2.27.1
2023-03-22 14:45:49,385:INFO:          matplotlib: 3.6.2
2023-03-22 14:45:49,385:INFO:          scikitplot: 0.3.7
2023-03-22 14:45:49,385:INFO:         yellowbrick: 1.5
2023-03-22 14:45:49,385:INFO:              plotly: 5.9.0
2023-03-22 14:45:49,385:INFO:             kaleido: 0.2.1
2023-03-22 14:45:49,385:INFO:         statsmodels: 0.13.2
2023-03-22 14:45:49,386:INFO:              sktime: 0.16.1
2023-03-22 14:45:49,386:INFO:               tbats: 1.1.2
2023-03-22 14:45:49,386:INFO:            pmdarima: 2.0.3
2023-03-22 14:45:49,386:INFO:              psutil: 5.9.0
2023-03-22 14:45:49,386:INFO:PyCaret optional dependencies:
2023-03-22 14:45:49,386:INFO:                shap: Not installed
2023-03-22 14:45:49,386:INFO:           interpret: Not installed
2023-03-22 14:45:49,386:INFO:                umap: Not installed
2023-03-22 14:45:49,386:INFO:    pandas_profiling: Not installed
2023-03-22 14:45:49,386:INFO:  explainerdashboard: Not installed
2023-03-22 14:45:49,386:INFO:             autoviz: Not installed
2023-03-22 14:45:49,386:INFO:           fairlearn: Not installed
2023-03-22 14:45:49,386:INFO:             xgboost: 1.7.1
2023-03-22 14:45:49,386:INFO:            catboost: 1.0.6
2023-03-22 14:45:49,386:INFO:              kmodes: Not installed
2023-03-22 14:45:49,386:INFO:             mlxtend: Not installed
2023-03-22 14:45:49,386:INFO:       statsforecast: Not installed
2023-03-22 14:45:49,386:INFO:        tune_sklearn: Not installed
2023-03-22 14:45:49,386:INFO:                 ray: Not installed
2023-03-22 14:45:49,386:INFO:            hyperopt: Not installed
2023-03-22 14:45:49,386:INFO:              optuna: Not installed
2023-03-22 14:45:49,387:INFO:               skopt: Not installed
2023-03-22 14:45:49,387:INFO:              mlflow: Not installed
2023-03-22 14:45:49,387:INFO:              gradio: Not installed
2023-03-22 14:45:49,387:INFO:             fastapi: Not installed
2023-03-22 14:45:49,387:INFO:             uvicorn: Not installed
2023-03-22 14:45:49,387:INFO:              m2cgen: Not installed
2023-03-22 14:45:49,387:INFO:           evidently: Not installed
2023-03-22 14:45:49,387:INFO:               fugue: Not installed
2023-03-22 14:45:49,387:INFO:           streamlit: Not installed
2023-03-22 14:45:49,387:INFO:             prophet: Not installed
2023-03-22 14:45:49,387:INFO:None
2023-03-22 14:45:49,387:INFO:Set up data.
2023-03-22 14:45:49,397:INFO:Set up train/test split.
2023-03-22 14:45:49,397:INFO:Set up data.
2023-03-22 14:45:49,405:INFO:Set up index.
2023-03-22 14:45:49,406:INFO:Set up folding strategy.
2023-03-22 14:45:49,406:INFO:Assigning column types.
2023-03-22 14:45:49,412:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-22 14:45:49,412:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,417:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,422:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,497:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,547:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,547:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:45:49,550:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:45:49,551:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,556:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,561:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,634:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,682:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,683:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:45:49,685:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:45:49,686:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-22 14:45:49,692:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,697:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,767:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,815:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,816:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:45:49,818:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:45:49,825:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,830:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,906:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,954:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:45:49,955:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:45:49,957:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:45:49,958:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-22 14:45:49,967:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 14:45:50,040:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:45:50,089:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:45:50,090:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:45:50,093:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:45:50,102:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 14:45:50,175:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:45:50,226:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:45:50,227:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:45:50,230:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:45:50,230:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-22 14:45:50,313:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:45:50,360:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:45:50,361:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:45:50,364:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:45:50,446:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:45:50,494:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 14:45:50,494:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:45:50,497:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:45:50,498:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-22 14:45:50,577:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:45:50,624:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:45:50,627:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:45:50,708:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 14:45:50,755:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:45:50,758:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:45:50,758:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-22 14:45:50,887:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:45:50,890:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:45:51,017:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:45:51,019:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:45:51,021:INFO:Preparing preprocessing pipeline...
2023-03-22 14:45:51,021:INFO:Set up simple imputation.
2023-03-22 14:45:51,022:INFO:Set up column name cleaning.
2023-03-22 14:45:51,308:INFO:Finished creating preprocessing pipeline.
2023-03-22 14:45:51,313:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\CHUKWUKA\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['onehotencoder__Item_Fat_Content_Low '
                                             'Fat',
                                             'onehotencoder__Item_Fat_Content_Regular',
                                             'onehotencoder__Outlet_Size_High',
                                             'onehotencoder__Outlet_Size_Medium',
                                             'onehotencoder__Outlet_Size_Small',
                                             'onehotencoder__Outlet_...
                                             'remainder__Item_Weight',
                                             'remainder__Item_Visibility',
                                             'remainder__Item_Type',
                                             'remainder__Item_MRP',
                                             'remainder__Outlet_Establishment_Year',
                                             'remainder__Item_Code'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-03-22 14:45:51,313:INFO:Creating final display dataframe.
2023-03-22 14:45:51,432:INFO:Setup _display_container:                     Description              Value
0                    Session id                666
1                        Target  Item_Outlet_Sales
2                   Target type         Regression
3           Original data shape         (8523, 19)
4        Transformed data shape        (13637, 19)
5   Transformed train set shape         (8523, 19)
6    Transformed test set shape         (5114, 19)
7              Numeric features                 18
8                    Preprocess               True
9               Imputation type             simple
10           Numeric imputation               mean
11       Categorical imputation               mode
12               Fold Generator              KFold
13                  Fold Number                 10
14                     CPU Jobs                 -1
15                      Use GPU              False
16               Log Experiment              False
17              Experiment Name   reg-default-name
18                          USI               ad5e
2023-03-22 14:45:51,583:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:45:51,586:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:45:51,713:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 14:45:51,716:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 14:45:51,717:INFO:setup() successfully completed in 2.53s...............
2023-03-22 14:46:22,442:INFO:Initializing compare_models()
2023-03-22 14:46:22,442:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, include=None, fold=None, round=4, cross_validation=True, sort=MSE, n_select=1, budget_time=10, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MSE', 'n_select': 1, 'budget_time': 10, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-22 14:46:22,442:INFO:Checking exceptions
2023-03-22 14:46:22,455:INFO:Preparing display monitor
2023-03-22 14:46:22,491:INFO:Time budget is 10 minutes
2023-03-22 14:46:22,491:INFO:Initializing Linear Regression
2023-03-22 14:46:22,491:INFO:Total runtime is 0.0 minutes
2023-03-22 14:46:22,500:INFO:SubProcess create_model() called ==================================
2023-03-22 14:46:22,500:INFO:Initializing create_model()
2023-03-22 14:46:22,500:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:46:22,500:INFO:Checking exceptions
2023-03-22 14:46:22,501:INFO:Importing libraries
2023-03-22 14:46:22,501:INFO:Copying training dataset
2023-03-22 14:46:22,525:INFO:Defining folds
2023-03-22 14:46:22,525:INFO:Declaring metric variables
2023-03-22 14:46:22,529:INFO:Importing untrained model
2023-03-22 14:46:22,534:INFO:Linear Regression Imported successfully
2023-03-22 14:46:22,547:INFO:Starting cross validation
2023-03-22 14:46:22,548:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:46:32,755:INFO:Calculating mean and std
2023-03-22 14:46:32,757:INFO:Creating metrics dataframe
2023-03-22 14:46:33,263:INFO:Uploading results into container
2023-03-22 14:46:33,264:INFO:Uploading model into container now
2023-03-22 14:46:33,264:INFO:_master_model_container: 1
2023-03-22 14:46:33,265:INFO:_display_container: 2
2023-03-22 14:46:33,265:INFO:LinearRegression(n_jobs=-1)
2023-03-22 14:46:33,265:INFO:create_model() successfully completed......................................
2023-03-22 14:46:33,514:INFO:SubProcess create_model() end ==================================
2023-03-22 14:46:33,599:INFO:Creating metrics dataframe
2023-03-22 14:46:33,610:INFO:Initializing Lasso Regression
2023-03-22 14:46:33,610:INFO:Total runtime is 0.18532197078069051 minutes
2023-03-22 14:46:33,613:INFO:SubProcess create_model() called ==================================
2023-03-22 14:46:33,614:INFO:Initializing create_model()
2023-03-22 14:46:33,614:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:46:33,614:INFO:Checking exceptions
2023-03-22 14:46:33,614:INFO:Importing libraries
2023-03-22 14:46:33,614:INFO:Copying training dataset
2023-03-22 14:46:33,639:INFO:Defining folds
2023-03-22 14:46:33,640:INFO:Declaring metric variables
2023-03-22 14:46:33,649:INFO:Importing untrained model
2023-03-22 14:46:33,655:INFO:Lasso Regression Imported successfully
2023-03-22 14:46:33,665:INFO:Starting cross validation
2023-03-22 14:46:33,666:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:46:35,862:INFO:Calculating mean and std
2023-03-22 14:46:35,863:INFO:Creating metrics dataframe
2023-03-22 14:46:36,150:INFO:Uploading results into container
2023-03-22 14:46:36,239:INFO:Uploading model into container now
2023-03-22 14:46:36,240:INFO:_master_model_container: 2
2023-03-22 14:46:36,240:INFO:_display_container: 2
2023-03-22 14:46:36,241:INFO:Lasso(random_state=666)
2023-03-22 14:46:36,241:INFO:create_model() successfully completed......................................
2023-03-22 14:46:36,469:INFO:SubProcess create_model() end ==================================
2023-03-22 14:46:36,469:INFO:Creating metrics dataframe
2023-03-22 14:46:36,480:INFO:Initializing Ridge Regression
2023-03-22 14:46:36,480:INFO:Total runtime is 0.2331589420636495 minutes
2023-03-22 14:46:36,484:INFO:SubProcess create_model() called ==================================
2023-03-22 14:46:36,484:INFO:Initializing create_model()
2023-03-22 14:46:36,484:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:46:36,484:INFO:Checking exceptions
2023-03-22 14:46:36,484:INFO:Importing libraries
2023-03-22 14:46:36,485:INFO:Copying training dataset
2023-03-22 14:46:36,511:INFO:Defining folds
2023-03-22 14:46:36,511:INFO:Declaring metric variables
2023-03-22 14:46:36,516:INFO:Importing untrained model
2023-03-22 14:46:36,520:INFO:Ridge Regression Imported successfully
2023-03-22 14:46:36,529:INFO:Starting cross validation
2023-03-22 14:46:36,530:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:46:36,598:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.21194e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 14:46:36,613:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.21251e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 14:46:36,645:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.20768e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 14:46:36,682:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.21278e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 14:46:36,704:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.19649e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 14:46:36,725:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.22579e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 14:46:36,729:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.21263e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 14:46:36,741:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.21773e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 14:46:37,128:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.21104e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 14:46:37,172:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.22263e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 14:46:38,649:INFO:Calculating mean and std
2023-03-22 14:46:38,651:INFO:Creating metrics dataframe
2023-03-22 14:46:38,938:INFO:Uploading results into container
2023-03-22 14:46:38,938:INFO:Uploading model into container now
2023-03-22 14:46:38,939:INFO:_master_model_container: 3
2023-03-22 14:46:38,939:INFO:_display_container: 2
2023-03-22 14:46:38,941:INFO:Ridge(random_state=666)
2023-03-22 14:46:38,942:INFO:create_model() successfully completed......................................
2023-03-22 14:46:39,177:INFO:SubProcess create_model() end ==================================
2023-03-22 14:46:39,177:INFO:Creating metrics dataframe
2023-03-22 14:46:39,187:INFO:Initializing Elastic Net
2023-03-22 14:46:39,187:INFO:Total runtime is 0.278267506758372 minutes
2023-03-22 14:46:39,192:INFO:SubProcess create_model() called ==================================
2023-03-22 14:46:39,192:INFO:Initializing create_model()
2023-03-22 14:46:39,192:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:46:39,193:INFO:Checking exceptions
2023-03-22 14:46:39,193:INFO:Importing libraries
2023-03-22 14:46:39,193:INFO:Copying training dataset
2023-03-22 14:46:39,223:INFO:Defining folds
2023-03-22 14:46:39,224:INFO:Declaring metric variables
2023-03-22 14:46:39,229:INFO:Importing untrained model
2023-03-22 14:46:39,234:INFO:Elastic Net Imported successfully
2023-03-22 14:46:39,245:INFO:Starting cross validation
2023-03-22 14:46:39,246:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:46:41,293:INFO:Calculating mean and std
2023-03-22 14:46:41,294:INFO:Creating metrics dataframe
2023-03-22 14:46:41,579:INFO:Uploading results into container
2023-03-22 14:46:41,580:INFO:Uploading model into container now
2023-03-22 14:46:41,580:INFO:_master_model_container: 4
2023-03-22 14:46:41,581:INFO:_display_container: 2
2023-03-22 14:46:41,582:INFO:ElasticNet(random_state=666)
2023-03-22 14:46:41,582:INFO:create_model() successfully completed......................................
2023-03-22 14:46:41,811:INFO:SubProcess create_model() end ==================================
2023-03-22 14:46:41,811:INFO:Creating metrics dataframe
2023-03-22 14:46:41,822:INFO:Initializing Least Angle Regression
2023-03-22 14:46:41,823:INFO:Total runtime is 0.322202452023824 minutes
2023-03-22 14:46:41,830:INFO:SubProcess create_model() called ==================================
2023-03-22 14:46:41,830:INFO:Initializing create_model()
2023-03-22 14:46:41,830:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:46:41,831:INFO:Checking exceptions
2023-03-22 14:46:41,831:INFO:Importing libraries
2023-03-22 14:46:41,831:INFO:Copying training dataset
2023-03-22 14:46:41,852:INFO:Defining folds
2023-03-22 14:46:41,852:INFO:Declaring metric variables
2023-03-22 14:46:41,859:INFO:Importing untrained model
2023-03-22 14:46:41,865:INFO:Least Angle Regression Imported successfully
2023-03-22 14:46:41,879:INFO:Starting cross validation
2023-03-22 14:46:41,880:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:46:41,964:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:41,974:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:41,993:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:42,005:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:42,020:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:42,032:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.092e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,034:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.127e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,034:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.998e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,036:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:42,051:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=7.208e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,051:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.048e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,052:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.771e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,053:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.537e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,059:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:42,072:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=6.450e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,073:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.493e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,075:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:42,092:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.442e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,498:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:42,502:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:42,508:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=6.907e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,508:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.323e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,508:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.635e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,508:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.623e-02, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,509:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.088e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,510:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.581e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:42,510:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=5.262e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 14:46:43,961:INFO:Calculating mean and std
2023-03-22 14:46:43,962:INFO:Creating metrics dataframe
2023-03-22 14:46:44,257:INFO:Uploading results into container
2023-03-22 14:46:44,289:INFO:Uploading model into container now
2023-03-22 14:46:44,289:INFO:_master_model_container: 5
2023-03-22 14:46:44,289:INFO:_display_container: 2
2023-03-22 14:46:44,290:INFO:Lars(random_state=666)
2023-03-22 14:46:44,290:INFO:create_model() successfully completed......................................
2023-03-22 14:46:44,516:INFO:SubProcess create_model() end ==================================
2023-03-22 14:46:44,516:INFO:Creating metrics dataframe
2023-03-22 14:46:44,584:INFO:Initializing Lasso Least Angle Regression
2023-03-22 14:46:44,584:INFO:Total runtime is 0.368220051129659 minutes
2023-03-22 14:46:44,588:INFO:SubProcess create_model() called ==================================
2023-03-22 14:46:44,589:INFO:Initializing create_model()
2023-03-22 14:46:44,589:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:46:44,589:INFO:Checking exceptions
2023-03-22 14:46:44,589:INFO:Importing libraries
2023-03-22 14:46:44,590:INFO:Copying training dataset
2023-03-22 14:46:44,614:INFO:Defining folds
2023-03-22 14:46:44,614:INFO:Declaring metric variables
2023-03-22 14:46:44,620:INFO:Importing untrained model
2023-03-22 14:46:44,627:INFO:Lasso Least Angle Regression Imported successfully
2023-03-22 14:46:44,636:INFO:Starting cross validation
2023-03-22 14:46:44,637:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:46:44,724:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:46:44,733:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:46:44,749:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:46:44,778:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:46:44,791:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:46:44,813:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:46:44,825:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:46:44,838:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:46:45,273:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:46:45,274:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 14:46:46,727:INFO:Calculating mean and std
2023-03-22 14:46:46,729:INFO:Creating metrics dataframe
2023-03-22 14:46:47,018:INFO:Uploading results into container
2023-03-22 14:46:47,019:INFO:Uploading model into container now
2023-03-22 14:46:47,020:INFO:_master_model_container: 6
2023-03-22 14:46:47,020:INFO:_display_container: 2
2023-03-22 14:46:47,021:INFO:LassoLars(random_state=666)
2023-03-22 14:46:47,022:INFO:create_model() successfully completed......................................
2023-03-22 14:46:47,255:INFO:SubProcess create_model() end ==================================
2023-03-22 14:46:47,255:INFO:Creating metrics dataframe
2023-03-22 14:46:47,266:INFO:Initializing Orthogonal Matching Pursuit
2023-03-22 14:46:47,266:INFO:Total runtime is 0.4129291415214538 minutes
2023-03-22 14:46:47,270:INFO:SubProcess create_model() called ==================================
2023-03-22 14:46:47,271:INFO:Initializing create_model()
2023-03-22 14:46:47,271:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:46:47,271:INFO:Checking exceptions
2023-03-22 14:46:47,271:INFO:Importing libraries
2023-03-22 14:46:47,271:INFO:Copying training dataset
2023-03-22 14:46:47,297:INFO:Defining folds
2023-03-22 14:46:47,297:INFO:Declaring metric variables
2023-03-22 14:46:47,302:INFO:Importing untrained model
2023-03-22 14:46:47,311:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-22 14:46:47,318:INFO:Starting cross validation
2023-03-22 14:46:47,320:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:46:47,393:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:47,408:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:47,415:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:47,439:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:47,447:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:47,469:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:47,501:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:47,520:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:47,911:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:47,921:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 14:46:49,383:INFO:Calculating mean and std
2023-03-22 14:46:49,384:INFO:Creating metrics dataframe
2023-03-22 14:46:49,674:INFO:Uploading results into container
2023-03-22 14:46:49,674:INFO:Uploading model into container now
2023-03-22 14:46:49,701:INFO:_master_model_container: 7
2023-03-22 14:46:49,701:INFO:_display_container: 2
2023-03-22 14:46:49,701:INFO:OrthogonalMatchingPursuit()
2023-03-22 14:46:49,701:INFO:create_model() successfully completed......................................
2023-03-22 14:46:49,930:INFO:SubProcess create_model() end ==================================
2023-03-22 14:46:49,930:INFO:Creating metrics dataframe
2023-03-22 14:46:49,989:INFO:Initializing Bayesian Ridge
2023-03-22 14:46:49,989:INFO:Total runtime is 0.4583108027776082 minutes
2023-03-22 14:46:49,993:INFO:SubProcess create_model() called ==================================
2023-03-22 14:46:49,993:INFO:Initializing create_model()
2023-03-22 14:46:49,993:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:46:49,994:INFO:Checking exceptions
2023-03-22 14:46:49,994:INFO:Importing libraries
2023-03-22 14:46:49,994:INFO:Copying training dataset
2023-03-22 14:46:50,018:INFO:Defining folds
2023-03-22 14:46:50,019:INFO:Declaring metric variables
2023-03-22 14:46:50,026:INFO:Importing untrained model
2023-03-22 14:46:50,030:INFO:Bayesian Ridge Imported successfully
2023-03-22 14:46:50,039:INFO:Starting cross validation
2023-03-22 14:46:50,041:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:46:52,107:INFO:Calculating mean and std
2023-03-22 14:46:52,108:INFO:Creating metrics dataframe
2023-03-22 14:46:52,400:INFO:Uploading results into container
2023-03-22 14:46:52,402:INFO:Uploading model into container now
2023-03-22 14:46:52,456:INFO:_master_model_container: 8
2023-03-22 14:46:52,456:INFO:_display_container: 2
2023-03-22 14:46:52,457:INFO:BayesianRidge()
2023-03-22 14:46:52,457:INFO:create_model() successfully completed......................................
2023-03-22 14:46:52,683:INFO:SubProcess create_model() end ==================================
2023-03-22 14:46:52,683:INFO:Creating metrics dataframe
2023-03-22 14:46:52,696:INFO:Initializing Passive Aggressive Regressor
2023-03-22 14:46:52,696:INFO:Total runtime is 0.5034306685129801 minutes
2023-03-22 14:46:52,699:INFO:SubProcess create_model() called ==================================
2023-03-22 14:46:52,700:INFO:Initializing create_model()
2023-03-22 14:46:52,700:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:46:52,700:INFO:Checking exceptions
2023-03-22 14:46:52,700:INFO:Importing libraries
2023-03-22 14:46:52,700:INFO:Copying training dataset
2023-03-22 14:46:52,723:INFO:Defining folds
2023-03-22 14:46:52,723:INFO:Declaring metric variables
2023-03-22 14:46:52,727:INFO:Importing untrained model
2023-03-22 14:46:52,731:INFO:Passive Aggressive Regressor Imported successfully
2023-03-22 14:46:52,740:INFO:Starting cross validation
2023-03-22 14:46:52,742:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:46:54,885:INFO:Calculating mean and std
2023-03-22 14:46:54,887:INFO:Creating metrics dataframe
2023-03-22 14:46:55,167:INFO:Uploading results into container
2023-03-22 14:46:55,169:INFO:Uploading model into container now
2023-03-22 14:46:55,170:INFO:_master_model_container: 9
2023-03-22 14:46:55,170:INFO:_display_container: 2
2023-03-22 14:46:55,170:INFO:PassiveAggressiveRegressor(random_state=666)
2023-03-22 14:46:55,170:INFO:create_model() successfully completed......................................
2023-03-22 14:46:55,400:INFO:SubProcess create_model() end ==================================
2023-03-22 14:46:55,400:INFO:Creating metrics dataframe
2023-03-22 14:46:55,413:INFO:Initializing Huber Regressor
2023-03-22 14:46:55,413:INFO:Total runtime is 0.5487053791681925 minutes
2023-03-22 14:46:55,417:INFO:SubProcess create_model() called ==================================
2023-03-22 14:46:55,417:INFO:Initializing create_model()
2023-03-22 14:46:55,417:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:46:55,418:INFO:Checking exceptions
2023-03-22 14:46:55,418:INFO:Importing libraries
2023-03-22 14:46:55,418:INFO:Copying training dataset
2023-03-22 14:46:55,439:INFO:Defining folds
2023-03-22 14:46:55,439:INFO:Declaring metric variables
2023-03-22 14:46:55,443:INFO:Importing untrained model
2023-03-22 14:46:55,447:INFO:Huber Regressor Imported successfully
2023-03-22 14:46:55,458:INFO:Starting cross validation
2023-03-22 14:46:55,459:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:46:56,260:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:46:56,287:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:46:56,335:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:46:56,354:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:46:56,384:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:46:56,387:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:46:56,415:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:46:57,159:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:46:57,200:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 14:46:58,441:INFO:Calculating mean and std
2023-03-22 14:46:58,442:INFO:Creating metrics dataframe
2023-03-22 14:46:58,737:INFO:Uploading results into container
2023-03-22 14:46:58,738:INFO:Uploading model into container now
2023-03-22 14:46:58,739:INFO:_master_model_container: 10
2023-03-22 14:46:58,739:INFO:_display_container: 2
2023-03-22 14:46:58,739:INFO:HuberRegressor()
2023-03-22 14:46:58,739:INFO:create_model() successfully completed......................................
2023-03-22 14:46:58,966:INFO:SubProcess create_model() end ==================================
2023-03-22 14:46:58,966:INFO:Creating metrics dataframe
2023-03-22 14:46:58,981:INFO:Initializing K Neighbors Regressor
2023-03-22 14:46:58,981:INFO:Total runtime is 0.6081708153088887 minutes
2023-03-22 14:46:58,987:INFO:SubProcess create_model() called ==================================
2023-03-22 14:46:58,987:INFO:Initializing create_model()
2023-03-22 14:46:58,987:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:46:58,987:INFO:Checking exceptions
2023-03-22 14:46:58,987:INFO:Importing libraries
2023-03-22 14:46:58,987:INFO:Copying training dataset
2023-03-22 14:46:59,012:INFO:Defining folds
2023-03-22 14:46:59,012:INFO:Declaring metric variables
2023-03-22 14:46:59,018:INFO:Importing untrained model
2023-03-22 14:46:59,023:INFO:K Neighbors Regressor Imported successfully
2023-03-22 14:46:59,031:INFO:Starting cross validation
2023-03-22 14:46:59,032:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:47:02,211:INFO:Calculating mean and std
2023-03-22 14:47:02,213:INFO:Creating metrics dataframe
2023-03-22 14:47:02,503:INFO:Uploading results into container
2023-03-22 14:47:02,504:INFO:Uploading model into container now
2023-03-22 14:47:02,505:INFO:_master_model_container: 11
2023-03-22 14:47:02,505:INFO:_display_container: 2
2023-03-22 14:47:02,506:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-22 14:47:02,506:INFO:create_model() successfully completed......................................
2023-03-22 14:47:02,744:INFO:SubProcess create_model() end ==================================
2023-03-22 14:47:02,744:INFO:Creating metrics dataframe
2023-03-22 14:47:02,757:INFO:Initializing Decision Tree Regressor
2023-03-22 14:47:02,757:INFO:Total runtime is 0.6711009383201598 minutes
2023-03-22 14:47:02,763:INFO:SubProcess create_model() called ==================================
2023-03-22 14:47:02,764:INFO:Initializing create_model()
2023-03-22 14:47:02,764:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:47:02,764:INFO:Checking exceptions
2023-03-22 14:47:02,764:INFO:Importing libraries
2023-03-22 14:47:02,764:INFO:Copying training dataset
2023-03-22 14:47:02,793:INFO:Defining folds
2023-03-22 14:47:02,794:INFO:Declaring metric variables
2023-03-22 14:47:02,801:INFO:Importing untrained model
2023-03-22 14:47:02,807:INFO:Decision Tree Regressor Imported successfully
2023-03-22 14:47:02,819:INFO:Starting cross validation
2023-03-22 14:47:02,820:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:47:05,153:INFO:Calculating mean and std
2023-03-22 14:47:05,154:INFO:Creating metrics dataframe
2023-03-22 14:47:05,456:INFO:Uploading results into container
2023-03-22 14:47:05,457:INFO:Uploading model into container now
2023-03-22 14:47:05,458:INFO:_master_model_container: 12
2023-03-22 14:47:05,458:INFO:_display_container: 2
2023-03-22 14:47:05,458:INFO:DecisionTreeRegressor(random_state=666)
2023-03-22 14:47:05,458:INFO:create_model() successfully completed......................................
2023-03-22 14:47:05,689:INFO:SubProcess create_model() end ==================================
2023-03-22 14:47:05,689:INFO:Creating metrics dataframe
2023-03-22 14:47:05,705:INFO:Initializing Random Forest Regressor
2023-03-22 14:47:05,705:INFO:Total runtime is 0.7202320893605549 minutes
2023-03-22 14:47:05,710:INFO:SubProcess create_model() called ==================================
2023-03-22 14:47:05,710:INFO:Initializing create_model()
2023-03-22 14:47:05,710:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:47:05,710:INFO:Checking exceptions
2023-03-22 14:47:05,710:INFO:Importing libraries
2023-03-22 14:47:05,711:INFO:Copying training dataset
2023-03-22 14:47:05,739:INFO:Defining folds
2023-03-22 14:47:05,739:INFO:Declaring metric variables
2023-03-22 14:47:05,744:INFO:Importing untrained model
2023-03-22 14:47:05,753:INFO:Random Forest Regressor Imported successfully
2023-03-22 14:47:05,761:INFO:Starting cross validation
2023-03-22 14:47:05,762:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:47:09,722:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.90s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 14:47:10,054:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 14:47:11,487:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.85s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-22 14:47:16,348:INFO:Calculating mean and std
2023-03-22 14:47:16,350:INFO:Creating metrics dataframe
2023-03-22 14:47:16,727:INFO:Uploading results into container
2023-03-22 14:47:16,728:INFO:Uploading model into container now
2023-03-22 14:47:16,728:INFO:_master_model_container: 13
2023-03-22 14:47:16,729:INFO:_display_container: 2
2023-03-22 14:47:16,729:INFO:RandomForestRegressor(n_jobs=-1, random_state=666)
2023-03-22 14:47:16,729:INFO:create_model() successfully completed......................................
2023-03-22 14:47:16,975:INFO:SubProcess create_model() end ==================================
2023-03-22 14:47:16,975:INFO:Creating metrics dataframe
2023-03-22 14:47:16,992:INFO:Initializing Extra Trees Regressor
2023-03-22 14:47:16,992:INFO:Total runtime is 0.9083492318789164 minutes
2023-03-22 14:47:16,998:INFO:SubProcess create_model() called ==================================
2023-03-22 14:47:16,999:INFO:Initializing create_model()
2023-03-22 14:47:16,999:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:47:16,999:INFO:Checking exceptions
2023-03-22 14:47:16,999:INFO:Importing libraries
2023-03-22 14:47:16,999:INFO:Copying training dataset
2023-03-22 14:47:17,023:INFO:Defining folds
2023-03-22 14:47:17,023:INFO:Declaring metric variables
2023-03-22 14:47:17,027:INFO:Importing untrained model
2023-03-22 14:47:17,033:INFO:Extra Trees Regressor Imported successfully
2023-03-22 14:47:17,041:INFO:Starting cross validation
2023-03-22 14:47:17,042:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:47:22,370:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 14:47:26,895:INFO:Calculating mean and std
2023-03-22 14:47:26,897:INFO:Creating metrics dataframe
2023-03-22 14:47:27,220:INFO:Uploading results into container
2023-03-22 14:47:27,221:INFO:Uploading model into container now
2023-03-22 14:47:27,222:INFO:_master_model_container: 14
2023-03-22 14:47:27,222:INFO:_display_container: 2
2023-03-22 14:47:27,223:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=666)
2023-03-22 14:47:27,223:INFO:create_model() successfully completed......................................
2023-03-22 14:47:27,471:INFO:SubProcess create_model() end ==================================
2023-03-22 14:47:27,471:INFO:Creating metrics dataframe
2023-03-22 14:47:27,484:INFO:Initializing AdaBoost Regressor
2023-03-22 14:47:27,485:INFO:Total runtime is 1.0832388520240783 minutes
2023-03-22 14:47:27,516:INFO:SubProcess create_model() called ==================================
2023-03-22 14:47:27,517:INFO:Initializing create_model()
2023-03-22 14:47:27,517:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:47:27,517:INFO:Checking exceptions
2023-03-22 14:47:27,517:INFO:Importing libraries
2023-03-22 14:47:27,517:INFO:Copying training dataset
2023-03-22 14:47:27,543:INFO:Defining folds
2023-03-22 14:47:27,544:INFO:Declaring metric variables
2023-03-22 14:47:27,549:INFO:Importing untrained model
2023-03-22 14:47:27,565:INFO:AdaBoost Regressor Imported successfully
2023-03-22 14:47:27,573:INFO:Starting cross validation
2023-03-22 14:47:27,579:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:47:30,958:INFO:Calculating mean and std
2023-03-22 14:47:30,960:INFO:Creating metrics dataframe
2023-03-22 14:47:31,281:INFO:Uploading results into container
2023-03-22 14:47:31,282:INFO:Uploading model into container now
2023-03-22 14:47:31,283:INFO:_master_model_container: 15
2023-03-22 14:47:31,283:INFO:_display_container: 2
2023-03-22 14:47:31,283:INFO:AdaBoostRegressor(random_state=666)
2023-03-22 14:47:31,283:INFO:create_model() successfully completed......................................
2023-03-22 14:47:31,511:INFO:SubProcess create_model() end ==================================
2023-03-22 14:47:31,511:INFO:Creating metrics dataframe
2023-03-22 14:47:31,526:INFO:Initializing Gradient Boosting Regressor
2023-03-22 14:47:31,526:INFO:Total runtime is 1.1505878925323485 minutes
2023-03-22 14:47:31,531:INFO:SubProcess create_model() called ==================================
2023-03-22 14:47:31,531:INFO:Initializing create_model()
2023-03-22 14:47:31,531:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:47:31,531:INFO:Checking exceptions
2023-03-22 14:47:31,531:INFO:Importing libraries
2023-03-22 14:47:31,532:INFO:Copying training dataset
2023-03-22 14:47:31,556:INFO:Defining folds
2023-03-22 14:47:31,557:INFO:Declaring metric variables
2023-03-22 14:47:31,562:INFO:Importing untrained model
2023-03-22 14:47:31,567:INFO:Gradient Boosting Regressor Imported successfully
2023-03-22 14:47:31,583:INFO:Starting cross validation
2023-03-22 14:47:31,584:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:47:36,546:INFO:Calculating mean and std
2023-03-22 14:47:36,547:INFO:Creating metrics dataframe
2023-03-22 14:47:36,878:INFO:Uploading results into container
2023-03-22 14:47:36,879:INFO:Uploading model into container now
2023-03-22 14:47:36,879:INFO:_master_model_container: 16
2023-03-22 14:47:36,879:INFO:_display_container: 2
2023-03-22 14:47:36,880:INFO:GradientBoostingRegressor(random_state=666)
2023-03-22 14:47:36,880:INFO:create_model() successfully completed......................................
2023-03-22 14:47:37,111:INFO:SubProcess create_model() end ==================================
2023-03-22 14:47:37,112:INFO:Creating metrics dataframe
2023-03-22 14:47:37,127:INFO:Initializing Extreme Gradient Boosting
2023-03-22 14:47:37,127:INFO:Total runtime is 1.243935704231262 minutes
2023-03-22 14:47:37,131:INFO:SubProcess create_model() called ==================================
2023-03-22 14:47:37,132:INFO:Initializing create_model()
2023-03-22 14:47:37,132:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:47:37,132:INFO:Checking exceptions
2023-03-22 14:47:37,132:INFO:Importing libraries
2023-03-22 14:47:37,132:INFO:Copying training dataset
2023-03-22 14:47:37,153:INFO:Defining folds
2023-03-22 14:47:37,153:INFO:Declaring metric variables
2023-03-22 14:47:37,159:INFO:Importing untrained model
2023-03-22 14:47:37,164:INFO:Extreme Gradient Boosting Imported successfully
2023-03-22 14:47:37,171:INFO:Starting cross validation
2023-03-22 14:47:37,172:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:47:43,260:INFO:Calculating mean and std
2023-03-22 14:47:43,262:INFO:Creating metrics dataframe
2023-03-22 14:47:43,606:INFO:Uploading results into container
2023-03-22 14:47:43,607:INFO:Uploading model into container now
2023-03-22 14:47:43,608:INFO:_master_model_container: 17
2023-03-22 14:47:43,608:INFO:_display_container: 2
2023-03-22 14:47:43,609:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=666, ...)
2023-03-22 14:47:43,609:INFO:create_model() successfully completed......................................
2023-03-22 14:47:43,845:INFO:SubProcess create_model() end ==================================
2023-03-22 14:47:43,845:INFO:Creating metrics dataframe
2023-03-22 14:47:43,861:INFO:Initializing Light Gradient Boosting Machine
2023-03-22 14:47:43,862:INFO:Total runtime is 1.3561886628468829 minutes
2023-03-22 14:47:43,866:INFO:SubProcess create_model() called ==================================
2023-03-22 14:47:43,867:INFO:Initializing create_model()
2023-03-22 14:47:43,867:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:47:43,867:INFO:Checking exceptions
2023-03-22 14:47:43,867:INFO:Importing libraries
2023-03-22 14:47:43,867:INFO:Copying training dataset
2023-03-22 14:47:43,890:INFO:Defining folds
2023-03-22 14:47:43,890:INFO:Declaring metric variables
2023-03-22 14:47:43,895:INFO:Importing untrained model
2023-03-22 14:47:43,915:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-22 14:47:43,927:INFO:Starting cross validation
2023-03-22 14:47:43,928:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:47:48,804:INFO:Calculating mean and std
2023-03-22 14:47:48,805:INFO:Creating metrics dataframe
2023-03-22 14:47:49,152:INFO:Uploading results into container
2023-03-22 14:47:49,153:INFO:Uploading model into container now
2023-03-22 14:47:49,153:INFO:_master_model_container: 18
2023-03-22 14:47:49,153:INFO:_display_container: 2
2023-03-22 14:47:49,154:INFO:LGBMRegressor(random_state=666)
2023-03-22 14:47:49,154:INFO:create_model() successfully completed......................................
2023-03-22 14:47:49,382:INFO:SubProcess create_model() end ==================================
2023-03-22 14:47:49,382:INFO:Creating metrics dataframe
2023-03-22 14:47:49,397:INFO:Initializing CatBoost Regressor
2023-03-22 14:47:49,397:INFO:Total runtime is 1.4484450658162433 minutes
2023-03-22 14:47:49,401:INFO:SubProcess create_model() called ==================================
2023-03-22 14:47:49,402:INFO:Initializing create_model()
2023-03-22 14:47:49,402:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:47:49,402:INFO:Checking exceptions
2023-03-22 14:47:49,402:INFO:Importing libraries
2023-03-22 14:47:49,403:INFO:Copying training dataset
2023-03-22 14:47:49,423:INFO:Defining folds
2023-03-22 14:47:49,423:INFO:Declaring metric variables
2023-03-22 14:47:49,426:INFO:Importing untrained model
2023-03-22 14:47:49,430:INFO:CatBoost Regressor Imported successfully
2023-03-22 14:47:49,439:INFO:Starting cross validation
2023-03-22 14:47:49,440:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:48:13,053:INFO:Calculating mean and std
2023-03-22 14:48:13,054:INFO:Creating metrics dataframe
2023-03-22 14:48:13,404:INFO:Uploading results into container
2023-03-22 14:48:13,404:INFO:Uploading model into container now
2023-03-22 14:48:13,405:INFO:_master_model_container: 19
2023-03-22 14:48:13,405:INFO:_display_container: 2
2023-03-22 14:48:13,405:INFO:<catboost.core.CatBoostRegressor object at 0x0000016483B284C0>
2023-03-22 14:48:13,405:INFO:create_model() successfully completed......................................
2023-03-22 14:48:13,637:INFO:SubProcess create_model() end ==================================
2023-03-22 14:48:13,638:INFO:Creating metrics dataframe
2023-03-22 14:48:13,652:INFO:Initializing Dummy Regressor
2023-03-22 14:48:13,652:INFO:Total runtime is 1.8526818752288816 minutes
2023-03-22 14:48:13,656:INFO:SubProcess create_model() called ==================================
2023-03-22 14:48:13,657:INFO:Initializing create_model()
2023-03-22 14:48:13,657:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164FD45AF70>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:48:13,657:INFO:Checking exceptions
2023-03-22 14:48:13,657:INFO:Importing libraries
2023-03-22 14:48:13,657:INFO:Copying training dataset
2023-03-22 14:48:13,680:INFO:Defining folds
2023-03-22 14:48:13,680:INFO:Declaring metric variables
2023-03-22 14:48:13,684:INFO:Importing untrained model
2023-03-22 14:48:13,689:INFO:Dummy Regressor Imported successfully
2023-03-22 14:48:13,696:INFO:Starting cross validation
2023-03-22 14:48:13,697:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 14:48:16,533:INFO:Calculating mean and std
2023-03-22 14:48:16,534:INFO:Creating metrics dataframe
2023-03-22 14:48:16,899:INFO:Uploading results into container
2023-03-22 14:48:16,900:INFO:Uploading model into container now
2023-03-22 14:48:16,900:INFO:_master_model_container: 20
2023-03-22 14:48:16,900:INFO:_display_container: 2
2023-03-22 14:48:16,900:INFO:DummyRegressor()
2023-03-22 14:48:16,901:INFO:create_model() successfully completed......................................
2023-03-22 14:48:17,132:INFO:SubProcess create_model() end ==================================
2023-03-22 14:48:17,132:INFO:Creating metrics dataframe
2023-03-22 14:48:17,163:INFO:Initializing create_model()
2023-03-22 14:48:17,163:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483112850>, estimator=GradientBoostingRegressor(random_state=666), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-22 14:48:17,215:INFO:Checking exceptions
2023-03-22 14:48:17,218:INFO:Importing libraries
2023-03-22 14:48:17,218:INFO:Copying training dataset
2023-03-22 14:48:17,246:INFO:Defining folds
2023-03-22 14:48:17,247:INFO:Declaring metric variables
2023-03-22 14:48:17,247:INFO:Importing untrained model
2023-03-22 14:48:17,247:INFO:Declaring custom model
2023-03-22 14:48:17,248:INFO:Gradient Boosting Regressor Imported successfully
2023-03-22 14:48:17,248:INFO:Cross validation set to False
2023-03-22 14:48:17,248:INFO:Fitting Model
2023-03-22 14:48:18,856:INFO:GradientBoostingRegressor(random_state=666)
2023-03-22 14:48:18,857:INFO:create_model() successfully completed......................................
2023-03-22 14:48:19,131:INFO:_master_model_container: 20
2023-03-22 14:48:19,131:INFO:_display_container: 2
2023-03-22 14:48:19,132:INFO:GradientBoostingRegressor(random_state=666)
2023-03-22 14:48:19,132:INFO:compare_models() successfully completed......................................
2023-03-22 20:10:19,367:INFO:PyCaret RegressionExperiment
2023-03-22 20:10:19,501:INFO:Logging name: reg-default-name
2023-03-22 20:10:19,501:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-22 20:10:19,501:INFO:version 3.0.0
2023-03-22 20:10:19,501:INFO:Initializing setup()
2023-03-22 20:10:19,501:INFO:self.USI: 72cd
2023-03-22 20:10:19,501:INFO:self._variable_keys: {'gpu_param', '_available_plots', 'X', 'html_param', '_ml_usecase', 'target_param', 'gpu_n_jobs_param', 'USI', 'X_test', 'logging_param', 'idx', 'data', 'fold_groups_param', 'memory', 'pipeline', 'log_plots_param', 'n_jobs_param', 'y_test', 'X_train', 'fold_generator', 'exp_id', 'transform_target_param', 'exp_name_log', 'seed', 'y_train', 'fold_shuffle_param', 'y'}
2023-03-22 20:10:19,502:INFO:Checking environment
2023-03-22 20:10:19,603:INFO:python_version: 3.9.12
2023-03-22 20:10:19,603:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-22 20:10:19,637:INFO:machine: AMD64
2023-03-22 20:10:19,663:INFO:platform: Windows-10-10.0.19045-SP0
2023-03-22 20:10:19,815:INFO:Memory: svmem(total=8496553984, available=1216417792, percent=85.7, used=7280136192, free=1216417792)
2023-03-22 20:10:19,816:INFO:Physical Core: 4
2023-03-22 20:10:19,843:INFO:Logical Core: 8
2023-03-22 20:10:19,843:INFO:Checking libraries
2023-03-22 20:10:19,871:INFO:System:
2023-03-22 20:10:19,961:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-22 20:10:19,961:INFO:executable: C:\Users\CHUKWUKA\anaconda3\python.exe
2023-03-22 20:10:19,961:INFO:   machine: Windows-10-10.0.19045-SP0
2023-03-22 20:10:19,961:INFO:PyCaret required dependencies:
2023-03-22 20:10:20,108:INFO:                 pip: 22.3.1
2023-03-22 20:10:20,108:INFO:          setuptools: 65.5.0
2023-03-22 20:10:20,108:INFO:             pycaret: 3.0.0
2023-03-22 20:10:20,108:INFO:             IPython: 8.7.0
2023-03-22 20:10:20,108:INFO:          ipywidgets: 7.6.5
2023-03-22 20:10:20,108:INFO:                tqdm: 4.64.1
2023-03-22 20:10:20,108:INFO:               numpy: 1.21.5
2023-03-22 20:10:20,108:INFO:              pandas: 1.4.4
2023-03-22 20:10:20,108:INFO:              jinja2: 2.11.3
2023-03-22 20:10:20,108:INFO:               scipy: 1.9.3
2023-03-22 20:10:20,108:INFO:              joblib: 1.2.0
2023-03-22 20:10:20,109:INFO:             sklearn: 1.0.2
2023-03-22 20:10:20,109:INFO:                pyod: 1.0.9
2023-03-22 20:10:20,109:INFO:            imblearn: 0.10.1
2023-03-22 20:10:20,109:INFO:   category_encoders: 2.6.0
2023-03-22 20:10:20,109:INFO:            lightgbm: 3.3.5
2023-03-22 20:10:20,109:INFO:               numba: 0.56.4
2023-03-22 20:10:20,109:INFO:            requests: 2.27.1
2023-03-22 20:10:20,109:INFO:          matplotlib: 3.6.2
2023-03-22 20:10:20,109:INFO:          scikitplot: 0.3.7
2023-03-22 20:10:20,109:INFO:         yellowbrick: 1.5
2023-03-22 20:10:20,109:INFO:              plotly: 5.9.0
2023-03-22 20:10:20,109:INFO:             kaleido: 0.2.1
2023-03-22 20:10:20,109:INFO:         statsmodels: 0.13.2
2023-03-22 20:10:20,109:INFO:              sktime: 0.16.1
2023-03-22 20:10:20,109:INFO:               tbats: 1.1.2
2023-03-22 20:10:20,109:INFO:            pmdarima: 2.0.3
2023-03-22 20:10:20,109:INFO:              psutil: 5.9.0
2023-03-22 20:10:20,109:INFO:PyCaret optional dependencies:
2023-03-22 20:10:20,109:INFO:                shap: Not installed
2023-03-22 20:10:20,109:INFO:           interpret: Not installed
2023-03-22 20:10:20,109:INFO:                umap: Not installed
2023-03-22 20:10:20,109:INFO:    pandas_profiling: Not installed
2023-03-22 20:10:20,110:INFO:  explainerdashboard: Not installed
2023-03-22 20:10:20,110:INFO:             autoviz: Not installed
2023-03-22 20:10:20,110:INFO:           fairlearn: Not installed
2023-03-22 20:10:20,110:INFO:             xgboost: 1.7.1
2023-03-22 20:10:20,110:INFO:            catboost: 1.0.6
2023-03-22 20:10:20,110:INFO:              kmodes: Not installed
2023-03-22 20:10:20,110:INFO:             mlxtend: Not installed
2023-03-22 20:10:20,110:INFO:       statsforecast: Not installed
2023-03-22 20:10:20,110:INFO:        tune_sklearn: Not installed
2023-03-22 20:10:20,110:INFO:                 ray: Not installed
2023-03-22 20:10:20,110:INFO:            hyperopt: Not installed
2023-03-22 20:10:20,110:INFO:              optuna: Not installed
2023-03-22 20:10:20,110:INFO:               skopt: Not installed
2023-03-22 20:10:20,110:INFO:              mlflow: Not installed
2023-03-22 20:10:20,110:INFO:              gradio: Not installed
2023-03-22 20:10:20,110:INFO:             fastapi: Not installed
2023-03-22 20:10:20,110:INFO:             uvicorn: Not installed
2023-03-22 20:10:20,110:INFO:              m2cgen: Not installed
2023-03-22 20:10:20,110:INFO:           evidently: Not installed
2023-03-22 20:10:20,110:INFO:               fugue: Not installed
2023-03-22 20:10:20,110:INFO:           streamlit: Not installed
2023-03-22 20:10:20,110:INFO:             prophet: Not installed
2023-03-22 20:10:20,110:INFO:None
2023-03-22 20:10:20,136:INFO:Set up data.
2023-03-22 20:10:20,738:INFO:Set up train/test split.
2023-03-22 20:10:20,738:INFO:Set up data.
2023-03-22 20:10:20,811:INFO:Set up index.
2023-03-22 20:10:20,811:INFO:Set up folding strategy.
2023-03-22 20:10:20,818:INFO:Assigning column types.
2023-03-22 20:10:21,152:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-22 20:10:21,379:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-22 20:10:21,413:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 20:10:21,417:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 20:10:21,825:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 20:10:21,872:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 20:10:21,931:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 20:10:22,081:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 20:10:22,141:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-22 20:10:22,146:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 20:10:22,150:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 20:10:52,672:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 20:10:52,717:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 20:10:52,717:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 20:10:52,720:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 20:10:52,721:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-22 20:10:52,725:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 20:10:52,730:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 20:10:52,807:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 20:10:52,865:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 20:10:52,866:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 20:10:52,869:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 20:10:52,875:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-22 20:10:52,879:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 20:10:52,948:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 20:10:52,992:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 20:10:52,993:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 20:10:52,996:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 20:10:52,996:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-22 20:10:53,006:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 20:10:53,074:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 20:10:53,119:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 20:10:53,120:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 20:10:53,123:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 20:10:53,132:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-22 20:10:53,202:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 20:10:53,248:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 20:10:53,248:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 20:10:53,251:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 20:10:53,252:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-22 20:10:53,338:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 20:10:53,384:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 20:10:53,385:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 20:10:53,387:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 20:10:53,466:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 20:10:53,510:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-22 20:10:53,511:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 20:10:53,514:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 20:10:53,514:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-22 20:10:53,593:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 20:10:53,639:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 20:10:53,642:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 20:10:53,720:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-22 20:10:53,765:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 20:10:53,768:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 20:10:53,768:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-22 20:10:53,891:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 20:10:53,893:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 20:10:54,018:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 20:10:54,021:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 20:10:54,071:INFO:Preparing preprocessing pipeline...
2023-03-22 20:10:54,071:INFO:Set up simple imputation.
2023-03-22 20:10:54,072:INFO:Set up column name cleaning.
2023-03-22 20:10:54,231:INFO:Finished creating preprocessing pipeline.
2023-03-22 20:10:54,243:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\CHUKWUKA\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['onehotencoder__Item_Fat_Content_Low '
                                             'Fat',
                                             'onehotencoder__Item_Fat_Content_Regular',
                                             'onehotencoder__Outlet_Size_High',
                                             'onehotencoder__Outlet_Size_Medium',
                                             'onehotencoder__Outlet_Size_Small',
                                             'onehotencoder__Outlet_...
                                             'remainder__Item_Weight',
                                             'remainder__Item_Visibility',
                                             'remainder__Item_Type',
                                             'remainder__Item_MRP',
                                             'remainder__Outlet_Establishment_Year',
                                             'remainder__Item_Code'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-03-22 20:10:54,243:INFO:Creating final display dataframe.
2023-03-22 20:10:54,373:INFO:Setup _display_container:                     Description              Value
0                    Session id                666
1                        Target  Item_Outlet_Sales
2                   Target type         Regression
3           Original data shape         (8523, 19)
4        Transformed data shape        (13637, 19)
5   Transformed train set shape         (8523, 19)
6    Transformed test set shape         (5114, 19)
7              Numeric features                 18
8                    Preprocess               True
9               Imputation type             simple
10           Numeric imputation               mean
11       Categorical imputation               mode
12               Fold Generator              KFold
13                  Fold Number                 10
14                     CPU Jobs                 -1
15                      Use GPU              False
16               Log Experiment              False
17              Experiment Name   reg-default-name
18                          USI               72cd
2023-03-22 20:10:54,579:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 20:10:54,583:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 20:10:54,737:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-22 20:10:54,740:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-22 20:10:54,741:INFO:setup() successfully completed in 39.86s...............
2023-03-22 20:10:55,292:INFO:Initializing compare_models()
2023-03-22 20:10:55,292:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, include=None, fold=None, round=4, cross_validation=True, sort=MSE, n_select=1, budget_time=10, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MSE', 'n_select': 1, 'budget_time': 10, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-22 20:10:55,292:INFO:Checking exceptions
2023-03-22 20:10:55,455:INFO:Preparing display monitor
2023-03-22 20:10:55,500:INFO:Time budget is 10 minutes
2023-03-22 20:10:55,501:INFO:Initializing Linear Regression
2023-03-22 20:10:55,502:INFO:Total runtime is 3.440380096435547e-05 minutes
2023-03-22 20:10:55,506:INFO:SubProcess create_model() called ==================================
2023-03-22 20:10:55,506:INFO:Initializing create_model()
2023-03-22 20:10:55,506:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:10:55,506:INFO:Checking exceptions
2023-03-22 20:10:55,506:INFO:Importing libraries
2023-03-22 20:10:55,506:INFO:Copying training dataset
2023-03-22 20:10:55,536:INFO:Defining folds
2023-03-22 20:10:55,537:INFO:Declaring metric variables
2023-03-22 20:10:55,540:INFO:Importing untrained model
2023-03-22 20:10:55,544:INFO:Linear Regression Imported successfully
2023-03-22 20:10:55,559:INFO:Starting cross validation
2023-03-22 20:10:55,561:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:11:17,459:INFO:Calculating mean and std
2023-03-22 20:11:17,460:INFO:Creating metrics dataframe
2023-03-22 20:11:17,842:INFO:Uploading results into container
2023-03-22 20:11:17,849:INFO:Uploading model into container now
2023-03-22 20:11:17,849:INFO:_master_model_container: 1
2023-03-22 20:11:17,850:INFO:_display_container: 2
2023-03-22 20:11:17,850:INFO:LinearRegression(n_jobs=-1)
2023-03-22 20:11:17,850:INFO:create_model() successfully completed......................................
2023-03-22 20:11:18,188:INFO:SubProcess create_model() end ==================================
2023-03-22 20:11:18,189:INFO:Creating metrics dataframe
2023-03-22 20:11:18,266:INFO:Initializing Lasso Regression
2023-03-22 20:11:18,266:INFO:Total runtime is 0.37943873008092244 minutes
2023-03-22 20:11:18,270:INFO:SubProcess create_model() called ==================================
2023-03-22 20:11:18,270:INFO:Initializing create_model()
2023-03-22 20:11:18,270:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:11:18,270:INFO:Checking exceptions
2023-03-22 20:11:18,270:INFO:Importing libraries
2023-03-22 20:11:18,270:INFO:Copying training dataset
2023-03-22 20:11:18,291:INFO:Defining folds
2023-03-22 20:11:18,292:INFO:Declaring metric variables
2023-03-22 20:11:18,295:INFO:Importing untrained model
2023-03-22 20:11:18,300:INFO:Lasso Regression Imported successfully
2023-03-22 20:11:18,308:INFO:Starting cross validation
2023-03-22 20:11:18,309:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:11:20,828:INFO:Calculating mean and std
2023-03-22 20:11:20,829:INFO:Creating metrics dataframe
2023-03-22 20:11:21,213:INFO:Uploading results into container
2023-03-22 20:11:21,214:INFO:Uploading model into container now
2023-03-22 20:11:21,214:INFO:_master_model_container: 2
2023-03-22 20:11:21,216:INFO:_display_container: 2
2023-03-22 20:11:21,217:INFO:Lasso(random_state=666)
2023-03-22 20:11:21,217:INFO:create_model() successfully completed......................................
2023-03-22 20:11:21,536:INFO:SubProcess create_model() end ==================================
2023-03-22 20:11:21,542:INFO:Creating metrics dataframe
2023-03-22 20:11:21,565:INFO:Initializing Ridge Regression
2023-03-22 20:11:21,565:INFO:Total runtime is 0.43443135023117063 minutes
2023-03-22 20:11:21,568:INFO:SubProcess create_model() called ==================================
2023-03-22 20:11:21,569:INFO:Initializing create_model()
2023-03-22 20:11:21,569:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:11:21,569:INFO:Checking exceptions
2023-03-22 20:11:21,569:INFO:Importing libraries
2023-03-22 20:11:21,569:INFO:Copying training dataset
2023-03-22 20:11:21,590:INFO:Defining folds
2023-03-22 20:11:21,590:INFO:Declaring metric variables
2023-03-22 20:11:21,594:INFO:Importing untrained model
2023-03-22 20:11:21,598:INFO:Ridge Regression Imported successfully
2023-03-22 20:11:21,608:INFO:Starting cross validation
2023-03-22 20:11:21,609:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:11:21,681:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.22133e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 20:11:21,696:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.22445e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 20:11:21,697:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.23028e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 20:11:21,719:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.21593e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 20:11:21,739:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.22845e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 20:11:21,758:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.23131e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 20:11:21,777:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.24168e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 20:11:21,801:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.23441e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 20:11:22,266:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.22479e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 20:11:22,274:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.23824e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-22 20:11:24,053:INFO:Calculating mean and std
2023-03-22 20:11:24,054:INFO:Creating metrics dataframe
2023-03-22 20:11:24,398:INFO:Uploading results into container
2023-03-22 20:11:24,399:INFO:Uploading model into container now
2023-03-22 20:11:24,400:INFO:_master_model_container: 3
2023-03-22 20:11:24,400:INFO:_display_container: 2
2023-03-22 20:11:24,400:INFO:Ridge(random_state=666)
2023-03-22 20:11:24,400:INFO:create_model() successfully completed......................................
2023-03-22 20:11:24,712:INFO:SubProcess create_model() end ==================================
2023-03-22 20:11:24,712:INFO:Creating metrics dataframe
2023-03-22 20:11:24,733:INFO:Initializing Elastic Net
2023-03-22 20:11:24,735:INFO:Total runtime is 0.4872538129488627 minutes
2023-03-22 20:11:24,751:INFO:SubProcess create_model() called ==================================
2023-03-22 20:11:24,752:INFO:Initializing create_model()
2023-03-22 20:11:24,752:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:11:24,752:INFO:Checking exceptions
2023-03-22 20:11:24,752:INFO:Importing libraries
2023-03-22 20:11:24,752:INFO:Copying training dataset
2023-03-22 20:11:24,802:INFO:Defining folds
2023-03-22 20:11:24,802:INFO:Declaring metric variables
2023-03-22 20:11:24,811:INFO:Importing untrained model
2023-03-22 20:11:24,831:INFO:Elastic Net Imported successfully
2023-03-22 20:11:24,847:INFO:Starting cross validation
2023-03-22 20:11:24,849:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:11:27,355:INFO:Calculating mean and std
2023-03-22 20:11:27,356:INFO:Creating metrics dataframe
2023-03-22 20:11:27,693:INFO:Uploading results into container
2023-03-22 20:11:27,693:INFO:Uploading model into container now
2023-03-22 20:11:27,694:INFO:_master_model_container: 4
2023-03-22 20:11:27,695:INFO:_display_container: 2
2023-03-22 20:11:27,695:INFO:ElasticNet(random_state=666)
2023-03-22 20:11:27,695:INFO:create_model() successfully completed......................................
2023-03-22 20:11:27,997:INFO:SubProcess create_model() end ==================================
2023-03-22 20:11:27,998:INFO:Creating metrics dataframe
2023-03-22 20:11:28,010:INFO:Initializing Least Angle Regression
2023-03-22 20:11:28,010:INFO:Total runtime is 0.5418351014455159 minutes
2023-03-22 20:11:28,014:INFO:SubProcess create_model() called ==================================
2023-03-22 20:11:28,015:INFO:Initializing create_model()
2023-03-22 20:11:28,015:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:11:28,015:INFO:Checking exceptions
2023-03-22 20:11:28,015:INFO:Importing libraries
2023-03-22 20:11:28,015:INFO:Copying training dataset
2023-03-22 20:11:28,036:INFO:Defining folds
2023-03-22 20:11:28,036:INFO:Declaring metric variables
2023-03-22 20:11:28,041:INFO:Importing untrained model
2023-03-22 20:11:28,045:INFO:Least Angle Regression Imported successfully
2023-03-22 20:11:28,053:INFO:Starting cross validation
2023-03-22 20:11:28,054:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:11:28,118:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:28,130:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:28,132:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:739: RuntimeWarning: overflow encountered in true_divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-03-22 20:11:28,145:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:28,160:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:28,180:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:28,190:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.521e-02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 20:11:28,201:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=8.262e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 20:11:28,201:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:28,202:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.195e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 20:11:28,217:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:28,226:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.863e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 20:11:28,227:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.402e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 20:11:28,228:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.279e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 20:11:28,229:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.044e-02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 20:11:28,230:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.343e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 20:11:28,231:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.056e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 20:11:28,237:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:28,249:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=7.148e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 20:11:28,249:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=3.295e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 20:11:28,712:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:28,723:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.146e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 20:11:28,724:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=8.537e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-22 20:11:28,740:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:28,748:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:776: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-03-22 20:11:28,749:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:776: RuntimeWarning: invalid value encountered in add
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-03-22 20:11:28,749:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:739: RuntimeWarning: overflow encountered in true_divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-03-22 20:11:28,749:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:744: RuntimeWarning: overflow encountered in true_divide
  z = -coef[active] / (least_squares + tiny32)

2023-03-22 20:11:28,767:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\utils\extmath.py:153: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-03-22 20:11:28,773:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 264, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 191, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\metrics\_regression.py", line 96, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 800, in check_array
    _assert_all_finite(array, allow_nan=force_all_finite == "allow-nan")
  File "C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 114, in _assert_all_finite
    raise ValueError(
ValueError: Input contains NaN, infinity or a value too large for dtype('float32').

  warnings.warn(

2023-03-22 20:11:30,592:INFO:Calculating mean and std
2023-03-22 20:11:30,593:INFO:Creating metrics dataframe
2023-03-22 20:11:30,966:INFO:Uploading results into container
2023-03-22 20:11:30,966:INFO:Uploading model into container now
2023-03-22 20:11:30,967:INFO:_master_model_container: 5
2023-03-22 20:11:30,967:INFO:_display_container: 2
2023-03-22 20:11:31,026:INFO:Lars(random_state=666)
2023-03-22 20:11:31,027:INFO:create_model() successfully completed......................................
2023-03-22 20:11:31,362:INFO:SubProcess create_model() end ==================================
2023-03-22 20:11:31,362:INFO:Creating metrics dataframe
2023-03-22 20:11:31,375:INFO:Initializing Lasso Least Angle Regression
2023-03-22 20:11:31,376:INFO:Total runtime is 0.5979355335235595 minutes
2023-03-22 20:11:31,380:INFO:SubProcess create_model() called ==================================
2023-03-22 20:11:31,381:INFO:Initializing create_model()
2023-03-22 20:11:31,381:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:11:31,381:INFO:Checking exceptions
2023-03-22 20:11:31,381:INFO:Importing libraries
2023-03-22 20:11:31,381:INFO:Copying training dataset
2023-03-22 20:11:31,404:INFO:Defining folds
2023-03-22 20:11:31,405:INFO:Declaring metric variables
2023-03-22 20:11:31,409:INFO:Importing untrained model
2023-03-22 20:11:31,413:INFO:Lasso Least Angle Regression Imported successfully
2023-03-22 20:11:31,424:INFO:Starting cross validation
2023-03-22 20:11:31,425:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:11:31,496:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 20:11:31,513:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 20:11:31,521:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 20:11:31,551:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 20:11:31,562:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 20:11:31,594:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 20:11:31,619:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 20:11:31,664:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 20:11:32,246:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 20:11:32,277:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-22 20:11:34,059:INFO:Calculating mean and std
2023-03-22 20:11:34,060:INFO:Creating metrics dataframe
2023-03-22 20:11:34,385:INFO:Uploading results into container
2023-03-22 20:11:34,386:INFO:Uploading model into container now
2023-03-22 20:11:34,386:INFO:_master_model_container: 6
2023-03-22 20:11:34,387:INFO:_display_container: 2
2023-03-22 20:11:34,387:INFO:LassoLars(random_state=666)
2023-03-22 20:11:34,387:INFO:create_model() successfully completed......................................
2023-03-22 20:11:34,672:INFO:SubProcess create_model() end ==================================
2023-03-22 20:11:34,672:INFO:Creating metrics dataframe
2023-03-22 20:11:34,684:INFO:Initializing Orthogonal Matching Pursuit
2023-03-22 20:11:34,684:INFO:Total runtime is 0.6530765891075134 minutes
2023-03-22 20:11:34,688:INFO:SubProcess create_model() called ==================================
2023-03-22 20:11:34,688:INFO:Initializing create_model()
2023-03-22 20:11:34,688:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:11:34,688:INFO:Checking exceptions
2023-03-22 20:11:34,688:INFO:Importing libraries
2023-03-22 20:11:34,688:INFO:Copying training dataset
2023-03-22 20:11:34,708:INFO:Defining folds
2023-03-22 20:11:34,708:INFO:Declaring metric variables
2023-03-22 20:11:34,712:INFO:Importing untrained model
2023-03-22 20:11:34,715:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-22 20:11:34,723:INFO:Starting cross validation
2023-03-22 20:11:34,724:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:11:34,783:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:34,799:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:34,812:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:34,828:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:34,842:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:34,862:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:34,879:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:34,896:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:35,404:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:35,404:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-22 20:11:37,131:INFO:Calculating mean and std
2023-03-22 20:11:37,133:INFO:Creating metrics dataframe
2023-03-22 20:11:37,461:INFO:Uploading results into container
2023-03-22 20:11:37,461:INFO:Uploading model into container now
2023-03-22 20:11:37,462:INFO:_master_model_container: 7
2023-03-22 20:11:37,462:INFO:_display_container: 2
2023-03-22 20:11:37,462:INFO:OrthogonalMatchingPursuit()
2023-03-22 20:11:37,463:INFO:create_model() successfully completed......................................
2023-03-22 20:11:37,750:INFO:SubProcess create_model() end ==================================
2023-03-22 20:11:37,751:INFO:Creating metrics dataframe
2023-03-22 20:11:37,761:INFO:Initializing Bayesian Ridge
2023-03-22 20:11:37,762:INFO:Total runtime is 0.7043808023134868 minutes
2023-03-22 20:11:37,765:INFO:SubProcess create_model() called ==================================
2023-03-22 20:11:37,766:INFO:Initializing create_model()
2023-03-22 20:11:37,766:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:11:37,766:INFO:Checking exceptions
2023-03-22 20:11:37,766:INFO:Importing libraries
2023-03-22 20:11:37,766:INFO:Copying training dataset
2023-03-22 20:11:37,786:INFO:Defining folds
2023-03-22 20:11:37,786:INFO:Declaring metric variables
2023-03-22 20:11:37,790:INFO:Importing untrained model
2023-03-22 20:11:37,793:INFO:Bayesian Ridge Imported successfully
2023-03-22 20:11:37,800:INFO:Starting cross validation
2023-03-22 20:11:37,801:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:11:40,191:INFO:Calculating mean and std
2023-03-22 20:11:40,192:INFO:Creating metrics dataframe
2023-03-22 20:11:40,519:INFO:Uploading results into container
2023-03-22 20:11:40,520:INFO:Uploading model into container now
2023-03-22 20:11:40,520:INFO:_master_model_container: 8
2023-03-22 20:11:40,521:INFO:_display_container: 2
2023-03-22 20:11:40,521:INFO:BayesianRidge()
2023-03-22 20:11:40,521:INFO:create_model() successfully completed......................................
2023-03-22 20:11:40,809:INFO:SubProcess create_model() end ==================================
2023-03-22 20:11:40,815:INFO:Creating metrics dataframe
2023-03-22 20:11:40,826:INFO:Initializing Passive Aggressive Regressor
2023-03-22 20:11:40,826:INFO:Total runtime is 0.7554479241371155 minutes
2023-03-22 20:11:40,830:INFO:SubProcess create_model() called ==================================
2023-03-22 20:11:40,830:INFO:Initializing create_model()
2023-03-22 20:11:40,830:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:11:40,831:INFO:Checking exceptions
2023-03-22 20:11:40,831:INFO:Importing libraries
2023-03-22 20:11:40,831:INFO:Copying training dataset
2023-03-22 20:11:40,853:INFO:Defining folds
2023-03-22 20:11:40,853:INFO:Declaring metric variables
2023-03-22 20:11:40,857:INFO:Importing untrained model
2023-03-22 20:11:40,861:INFO:Passive Aggressive Regressor Imported successfully
2023-03-22 20:11:40,869:INFO:Starting cross validation
2023-03-22 20:11:40,870:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:11:43,277:INFO:Calculating mean and std
2023-03-22 20:11:43,278:INFO:Creating metrics dataframe
2023-03-22 20:11:43,602:INFO:Uploading results into container
2023-03-22 20:11:43,603:INFO:Uploading model into container now
2023-03-22 20:11:43,604:INFO:_master_model_container: 9
2023-03-22 20:11:43,604:INFO:_display_container: 2
2023-03-22 20:11:43,604:INFO:PassiveAggressiveRegressor(random_state=666)
2023-03-22 20:11:43,604:INFO:create_model() successfully completed......................................
2023-03-22 20:11:43,891:INFO:SubProcess create_model() end ==================================
2023-03-22 20:11:43,891:INFO:Creating metrics dataframe
2023-03-22 20:11:43,903:INFO:Initializing Huber Regressor
2023-03-22 20:11:43,903:INFO:Total runtime is 0.8067315975824992 minutes
2023-03-22 20:11:43,907:INFO:SubProcess create_model() called ==================================
2023-03-22 20:11:43,907:INFO:Initializing create_model()
2023-03-22 20:11:43,908:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:11:43,908:INFO:Checking exceptions
2023-03-22 20:11:43,908:INFO:Importing libraries
2023-03-22 20:11:43,908:INFO:Copying training dataset
2023-03-22 20:11:43,929:INFO:Defining folds
2023-03-22 20:11:43,929:INFO:Declaring metric variables
2023-03-22 20:11:43,932:INFO:Importing untrained model
2023-03-22 20:11:43,936:INFO:Huber Regressor Imported successfully
2023-03-22 20:11:43,943:INFO:Starting cross validation
2023-03-22 20:11:43,944:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:11:44,607:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 20:11:44,656:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 20:11:44,702:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 20:11:44,704:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 20:11:44,744:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 20:11:44,758:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 20:11:44,798:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 20:11:44,836:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 20:11:45,541:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 20:11:45,555:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-22 20:11:47,065:INFO:Calculating mean and std
2023-03-22 20:11:47,066:INFO:Creating metrics dataframe
2023-03-22 20:11:47,401:INFO:Uploading results into container
2023-03-22 20:11:47,402:INFO:Uploading model into container now
2023-03-22 20:11:47,402:INFO:_master_model_container: 10
2023-03-22 20:11:47,402:INFO:_display_container: 2
2023-03-22 20:11:47,403:INFO:HuberRegressor()
2023-03-22 20:11:47,403:INFO:create_model() successfully completed......................................
2023-03-22 20:11:47,686:INFO:SubProcess create_model() end ==================================
2023-03-22 20:11:47,686:INFO:Creating metrics dataframe
2023-03-22 20:11:47,698:INFO:Initializing K Neighbors Regressor
2023-03-22 20:11:47,699:INFO:Total runtime is 0.8699904441833497 minutes
2023-03-22 20:11:47,703:INFO:SubProcess create_model() called ==================================
2023-03-22 20:11:47,703:INFO:Initializing create_model()
2023-03-22 20:11:47,703:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:11:47,703:INFO:Checking exceptions
2023-03-22 20:11:47,703:INFO:Importing libraries
2023-03-22 20:11:47,704:INFO:Copying training dataset
2023-03-22 20:11:47,723:INFO:Defining folds
2023-03-22 20:11:47,723:INFO:Declaring metric variables
2023-03-22 20:11:47,727:INFO:Importing untrained model
2023-03-22 20:11:47,731:INFO:K Neighbors Regressor Imported successfully
2023-03-22 20:11:47,739:INFO:Starting cross validation
2023-03-22 20:11:47,741:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:11:50,903:INFO:Calculating mean and std
2023-03-22 20:11:50,904:INFO:Creating metrics dataframe
2023-03-22 20:11:51,233:INFO:Uploading results into container
2023-03-22 20:11:51,234:INFO:Uploading model into container now
2023-03-22 20:11:51,234:INFO:_master_model_container: 11
2023-03-22 20:11:51,234:INFO:_display_container: 2
2023-03-22 20:11:51,235:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-22 20:11:51,235:INFO:create_model() successfully completed......................................
2023-03-22 20:11:51,519:INFO:SubProcess create_model() end ==================================
2023-03-22 20:11:51,519:INFO:Creating metrics dataframe
2023-03-22 20:11:51,555:INFO:Initializing Decision Tree Regressor
2023-03-22 20:11:51,555:INFO:Total runtime is 0.9342634320259096 minutes
2023-03-22 20:11:51,559:INFO:SubProcess create_model() called ==================================
2023-03-22 20:11:51,559:INFO:Initializing create_model()
2023-03-22 20:11:51,560:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:11:51,560:INFO:Checking exceptions
2023-03-22 20:11:51,560:INFO:Importing libraries
2023-03-22 20:11:51,560:INFO:Copying training dataset
2023-03-22 20:11:51,580:INFO:Defining folds
2023-03-22 20:11:51,580:INFO:Declaring metric variables
2023-03-22 20:11:51,584:INFO:Importing untrained model
2023-03-22 20:11:51,588:INFO:Decision Tree Regressor Imported successfully
2023-03-22 20:11:51,596:INFO:Starting cross validation
2023-03-22 20:11:51,596:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:11:54,212:INFO:Calculating mean and std
2023-03-22 20:11:54,214:INFO:Creating metrics dataframe
2023-03-22 20:11:54,594:INFO:Uploading results into container
2023-03-22 20:11:54,595:INFO:Uploading model into container now
2023-03-22 20:11:54,595:INFO:_master_model_container: 12
2023-03-22 20:11:54,596:INFO:_display_container: 2
2023-03-22 20:11:54,596:INFO:DecisionTreeRegressor(random_state=666)
2023-03-22 20:11:54,596:INFO:create_model() successfully completed......................................
2023-03-22 20:11:54,914:INFO:SubProcess create_model() end ==================================
2023-03-22 20:11:54,914:INFO:Creating metrics dataframe
2023-03-22 20:11:54,928:INFO:Initializing Random Forest Regressor
2023-03-22 20:11:54,928:INFO:Total runtime is 0.9904724160830182 minutes
2023-03-22 20:11:54,934:INFO:SubProcess create_model() called ==================================
2023-03-22 20:11:54,934:INFO:Initializing create_model()
2023-03-22 20:11:54,934:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:11:54,934:INFO:Checking exceptions
2023-03-22 20:11:54,935:INFO:Importing libraries
2023-03-22 20:11:54,935:INFO:Copying training dataset
2023-03-22 20:11:54,956:INFO:Defining folds
2023-03-22 20:11:54,956:INFO:Declaring metric variables
2023-03-22 20:11:54,961:INFO:Importing untrained model
2023-03-22 20:11:54,967:INFO:Random Forest Regressor Imported successfully
2023-03-22 20:11:54,975:INFO:Starting cross validation
2023-03-22 20:11:54,977:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:12:02,656:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 20:12:07,938:INFO:Calculating mean and std
2023-03-22 20:12:07,939:INFO:Creating metrics dataframe
2023-03-22 20:12:08,301:INFO:Uploading results into container
2023-03-22 20:12:08,302:INFO:Uploading model into container now
2023-03-22 20:12:08,303:INFO:_master_model_container: 13
2023-03-22 20:12:08,303:INFO:_display_container: 2
2023-03-22 20:12:08,304:INFO:RandomForestRegressor(n_jobs=-1, random_state=666)
2023-03-22 20:12:08,304:INFO:create_model() successfully completed......................................
2023-03-22 20:12:08,629:INFO:SubProcess create_model() end ==================================
2023-03-22 20:12:08,630:INFO:Creating metrics dataframe
2023-03-22 20:12:08,652:INFO:Initializing Extra Trees Regressor
2023-03-22 20:12:08,652:INFO:Total runtime is 1.2192136128743492 minutes
2023-03-22 20:12:08,656:INFO:SubProcess create_model() called ==================================
2023-03-22 20:12:08,657:INFO:Initializing create_model()
2023-03-22 20:12:08,657:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:12:08,657:INFO:Checking exceptions
2023-03-22 20:12:08,657:INFO:Importing libraries
2023-03-22 20:12:08,657:INFO:Copying training dataset
2023-03-22 20:12:08,679:INFO:Defining folds
2023-03-22 20:12:08,679:INFO:Declaring metric variables
2023-03-22 20:12:08,684:INFO:Importing untrained model
2023-03-22 20:12:08,688:INFO:Extra Trees Regressor Imported successfully
2023-03-22 20:12:08,696:INFO:Starting cross validation
2023-03-22 20:12:08,698:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:12:14,382:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 20:12:14,464:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 20:12:19,664:INFO:Calculating mean and std
2023-03-22 20:12:19,666:INFO:Creating metrics dataframe
2023-03-22 20:12:20,042:INFO:Uploading results into container
2023-03-22 20:12:20,043:INFO:Uploading model into container now
2023-03-22 20:12:20,043:INFO:_master_model_container: 14
2023-03-22 20:12:20,043:INFO:_display_container: 2
2023-03-22 20:12:20,044:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=666)
2023-03-22 20:12:20,045:INFO:create_model() successfully completed......................................
2023-03-22 20:12:20,363:INFO:SubProcess create_model() end ==================================
2023-03-22 20:12:20,363:INFO:Creating metrics dataframe
2023-03-22 20:12:20,403:INFO:Initializing AdaBoost Regressor
2023-03-22 20:12:20,403:INFO:Total runtime is 1.4150607426961266 minutes
2023-03-22 20:12:20,407:INFO:SubProcess create_model() called ==================================
2023-03-22 20:12:20,408:INFO:Initializing create_model()
2023-03-22 20:12:20,408:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:12:20,408:INFO:Checking exceptions
2023-03-22 20:12:20,408:INFO:Importing libraries
2023-03-22 20:12:20,408:INFO:Copying training dataset
2023-03-22 20:12:20,429:INFO:Defining folds
2023-03-22 20:12:20,430:INFO:Declaring metric variables
2023-03-22 20:12:20,434:INFO:Importing untrained model
2023-03-22 20:12:20,438:INFO:AdaBoost Regressor Imported successfully
2023-03-22 20:12:20,447:INFO:Starting cross validation
2023-03-22 20:12:20,448:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:12:24,445:INFO:Calculating mean and std
2023-03-22 20:12:24,447:INFO:Creating metrics dataframe
2023-03-22 20:12:24,829:INFO:Uploading results into container
2023-03-22 20:12:24,830:INFO:Uploading model into container now
2023-03-22 20:12:24,830:INFO:_master_model_container: 15
2023-03-22 20:12:24,830:INFO:_display_container: 2
2023-03-22 20:12:24,830:INFO:AdaBoostRegressor(random_state=666)
2023-03-22 20:12:24,831:INFO:create_model() successfully completed......................................
2023-03-22 20:12:25,134:INFO:SubProcess create_model() end ==================================
2023-03-22 20:12:25,134:INFO:Creating metrics dataframe
2023-03-22 20:12:25,347:INFO:Initializing Gradient Boosting Regressor
2023-03-22 20:12:25,347:INFO:Total runtime is 1.4974539637565616 minutes
2023-03-22 20:12:25,377:INFO:SubProcess create_model() called ==================================
2023-03-22 20:12:25,377:INFO:Initializing create_model()
2023-03-22 20:12:25,377:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:12:25,378:INFO:Checking exceptions
2023-03-22 20:12:25,378:INFO:Importing libraries
2023-03-22 20:12:25,378:INFO:Copying training dataset
2023-03-22 20:12:25,399:INFO:Defining folds
2023-03-22 20:12:25,399:INFO:Declaring metric variables
2023-03-22 20:12:25,403:INFO:Importing untrained model
2023-03-22 20:12:25,408:INFO:Gradient Boosting Regressor Imported successfully
2023-03-22 20:12:25,417:INFO:Starting cross validation
2023-03-22 20:12:25,419:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:12:31,267:INFO:Calculating mean and std
2023-03-22 20:12:31,268:INFO:Creating metrics dataframe
2023-03-22 20:12:31,660:INFO:Uploading results into container
2023-03-22 20:12:31,661:INFO:Uploading model into container now
2023-03-22 20:12:31,662:INFO:_master_model_container: 16
2023-03-22 20:12:31,662:INFO:_display_container: 2
2023-03-22 20:12:31,663:INFO:GradientBoostingRegressor(random_state=666)
2023-03-22 20:12:31,663:INFO:create_model() successfully completed......................................
2023-03-22 20:12:31,966:INFO:SubProcess create_model() end ==================================
2023-03-22 20:12:31,967:INFO:Creating metrics dataframe
2023-03-22 20:12:31,982:INFO:Initializing Extreme Gradient Boosting
2023-03-22 20:12:31,983:INFO:Total runtime is 1.6080605546633406 minutes
2023-03-22 20:12:31,988:INFO:SubProcess create_model() called ==================================
2023-03-22 20:12:31,989:INFO:Initializing create_model()
2023-03-22 20:12:31,989:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:12:31,989:INFO:Checking exceptions
2023-03-22 20:12:31,989:INFO:Importing libraries
2023-03-22 20:12:31,989:INFO:Copying training dataset
2023-03-22 20:12:32,010:INFO:Defining folds
2023-03-22 20:12:32,010:INFO:Declaring metric variables
2023-03-22 20:12:32,014:INFO:Importing untrained model
2023-03-22 20:12:32,021:INFO:Extreme Gradient Boosting Imported successfully
2023-03-22 20:12:32,028:INFO:Starting cross validation
2023-03-22 20:12:32,029:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:12:40,363:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 20:12:41,016:INFO:Calculating mean and std
2023-03-22 20:12:41,018:INFO:Creating metrics dataframe
2023-03-22 20:12:41,381:INFO:Uploading results into container
2023-03-22 20:12:41,382:INFO:Uploading model into container now
2023-03-22 20:12:41,382:INFO:_master_model_container: 17
2023-03-22 20:12:41,382:INFO:_display_container: 2
2023-03-22 20:12:41,383:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=666, ...)
2023-03-22 20:12:41,383:INFO:create_model() successfully completed......................................
2023-03-22 20:12:41,692:INFO:SubProcess create_model() end ==================================
2023-03-22 20:12:41,692:INFO:Creating metrics dataframe
2023-03-22 20:12:41,708:INFO:Initializing Light Gradient Boosting Machine
2023-03-22 20:12:41,708:INFO:Total runtime is 1.770134286085765 minutes
2023-03-22 20:12:41,712:INFO:SubProcess create_model() called ==================================
2023-03-22 20:12:41,712:INFO:Initializing create_model()
2023-03-22 20:12:41,712:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:12:41,713:INFO:Checking exceptions
2023-03-22 20:12:41,713:INFO:Importing libraries
2023-03-22 20:12:41,713:INFO:Copying training dataset
2023-03-22 20:12:41,735:INFO:Defining folds
2023-03-22 20:12:41,735:INFO:Declaring metric variables
2023-03-22 20:12:41,740:INFO:Importing untrained model
2023-03-22 20:12:41,744:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-22 20:12:41,752:INFO:Starting cross validation
2023-03-22 20:12:41,753:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:12:48,986:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 20:12:48,988:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 20:12:48,989:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-22 20:12:52,745:INFO:Calculating mean and std
2023-03-22 20:12:52,746:INFO:Creating metrics dataframe
2023-03-22 20:12:53,181:INFO:Uploading results into container
2023-03-22 20:12:53,182:INFO:Uploading model into container now
2023-03-22 20:12:53,182:INFO:_master_model_container: 18
2023-03-22 20:12:53,182:INFO:_display_container: 2
2023-03-22 20:12:53,183:INFO:LGBMRegressor(random_state=666)
2023-03-22 20:12:53,183:INFO:create_model() successfully completed......................................
2023-03-22 20:12:53,533:INFO:SubProcess create_model() end ==================================
2023-03-22 20:12:53,533:INFO:Creating metrics dataframe
2023-03-22 20:12:53,548:INFO:Initializing CatBoost Regressor
2023-03-22 20:12:53,548:INFO:Total runtime is 1.9674732208251957 minutes
2023-03-22 20:12:53,552:INFO:SubProcess create_model() called ==================================
2023-03-22 20:12:53,553:INFO:Initializing create_model()
2023-03-22 20:12:53,553:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:12:53,553:INFO:Checking exceptions
2023-03-22 20:12:53,553:INFO:Importing libraries
2023-03-22 20:12:53,553:INFO:Copying training dataset
2023-03-22 20:12:53,583:INFO:Defining folds
2023-03-22 20:12:53,583:INFO:Declaring metric variables
2023-03-22 20:12:53,591:INFO:Importing untrained model
2023-03-22 20:12:53,720:INFO:CatBoost Regressor Imported successfully
2023-03-22 20:12:53,732:INFO:Starting cross validation
2023-03-22 20:12:53,733:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:13:19,119:INFO:Calculating mean and std
2023-03-22 20:13:19,121:INFO:Creating metrics dataframe
2023-03-22 20:13:19,559:INFO:Uploading results into container
2023-03-22 20:13:19,560:INFO:Uploading model into container now
2023-03-22 20:13:19,560:INFO:_master_model_container: 19
2023-03-22 20:13:19,560:INFO:_display_container: 2
2023-03-22 20:13:19,561:INFO:<catboost.core.CatBoostRegressor object at 0x0000016486BE8F40>
2023-03-22 20:13:19,561:INFO:create_model() successfully completed......................................
2023-03-22 20:13:19,889:INFO:SubProcess create_model() end ==================================
2023-03-22 20:13:19,890:INFO:Creating metrics dataframe
2023-03-22 20:13:19,904:INFO:Initializing Dummy Regressor
2023-03-22 20:13:19,904:INFO:Total runtime is 2.4067437728246057 minutes
2023-03-22 20:13:19,908:INFO:SubProcess create_model() called ==================================
2023-03-22 20:13:19,908:INFO:Initializing create_model()
2023-03-22 20:13:19,909:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000164844D1F10>, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:13:19,909:INFO:Checking exceptions
2023-03-22 20:13:19,909:INFO:Importing libraries
2023-03-22 20:13:19,909:INFO:Copying training dataset
2023-03-22 20:13:19,931:INFO:Defining folds
2023-03-22 20:13:19,931:INFO:Declaring metric variables
2023-03-22 20:13:19,935:INFO:Importing untrained model
2023-03-22 20:13:19,940:INFO:Dummy Regressor Imported successfully
2023-03-22 20:13:19,947:INFO:Starting cross validation
2023-03-22 20:13:19,948:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-22 20:13:23,240:INFO:Calculating mean and std
2023-03-22 20:13:23,241:INFO:Creating metrics dataframe
2023-03-22 20:13:23,637:INFO:Uploading results into container
2023-03-22 20:13:23,638:INFO:Uploading model into container now
2023-03-22 20:13:23,639:INFO:_master_model_container: 20
2023-03-22 20:13:23,639:INFO:_display_container: 2
2023-03-22 20:13:23,639:INFO:DummyRegressor()
2023-03-22 20:13:23,639:INFO:create_model() successfully completed......................................
2023-03-22 20:13:23,940:INFO:SubProcess create_model() end ==================================
2023-03-22 20:13:23,941:INFO:Creating metrics dataframe
2023-03-22 20:13:23,968:INFO:Initializing create_model()
2023-03-22 20:13:23,968:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016483B28BB0>, estimator=GradientBoostingRegressor(random_state=666), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-22 20:13:23,969:INFO:Checking exceptions
2023-03-22 20:13:23,971:INFO:Importing libraries
2023-03-22 20:13:23,971:INFO:Copying training dataset
2023-03-22 20:13:23,990:INFO:Defining folds
2023-03-22 20:13:23,990:INFO:Declaring metric variables
2023-03-22 20:13:23,990:INFO:Importing untrained model
2023-03-22 20:13:23,990:INFO:Declaring custom model
2023-03-22 20:13:23,991:INFO:Gradient Boosting Regressor Imported successfully
2023-03-22 20:13:23,992:INFO:Cross validation set to False
2023-03-22 20:13:23,992:INFO:Fitting Model
2023-03-22 20:13:27,218:INFO:GradientBoostingRegressor(random_state=666)
2023-03-22 20:13:27,219:INFO:create_model() successfully completed......................................
2023-03-22 20:13:27,576:INFO:_master_model_container: 20
2023-03-22 20:13:27,577:INFO:_display_container: 2
2023-03-22 20:13:27,577:INFO:GradientBoostingRegressor(random_state=666)
2023-03-22 20:13:27,577:INFO:compare_models() successfully completed......................................
2023-03-23 17:02:41,103:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-23 17:02:41,127:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-23 17:02:41,127:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-23 17:02:41,127:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-23 17:02:43,458:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-23 17:02:44,633:INFO:PyCaret RegressionExperiment
2023-03-23 17:02:44,633:INFO:Logging name: reg-default-name
2023-03-23 17:02:44,633:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-23 17:02:44,633:INFO:version 3.0.0
2023-03-23 17:02:44,633:INFO:Initializing setup()
2023-03-23 17:02:44,633:INFO:self.USI: bbe5
2023-03-23 17:02:44,634:INFO:self._variable_keys: {'memory', 'gpu_n_jobs_param', 'exp_id', 'html_param', 'y_test', 'seed', 'X_test', 'n_jobs_param', 'data', 'gpu_param', 'X', 'target_param', 'y', 'USI', 'log_plots_param', 'y_train', 'idx', 'logging_param', '_ml_usecase', '_available_plots', 'fold_shuffle_param', 'fold_generator', 'X_train', 'transform_target_param', 'fold_groups_param', 'pipeline', 'exp_name_log'}
2023-03-23 17:02:44,634:INFO:Checking environment
2023-03-23 17:02:44,634:INFO:python_version: 3.9.12
2023-03-23 17:02:44,634:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-23 17:02:44,634:INFO:machine: AMD64
2023-03-23 17:02:44,634:INFO:platform: Windows-10-10.0.19045-SP0
2023-03-23 17:02:44,634:INFO:Memory: svmem(total=8496553984, available=1050333184, percent=87.6, used=7446220800, free=1050333184)
2023-03-23 17:02:44,634:INFO:Physical Core: 4
2023-03-23 17:02:44,634:INFO:Logical Core: 8
2023-03-23 17:02:44,634:INFO:Checking libraries
2023-03-23 17:02:44,634:INFO:System:
2023-03-23 17:02:44,634:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-23 17:02:44,634:INFO:executable: C:\Users\CHUKWUKA\anaconda3\python.exe
2023-03-23 17:02:44,634:INFO:   machine: Windows-10-10.0.19045-SP0
2023-03-23 17:02:44,634:INFO:PyCaret required dependencies:
2023-03-23 17:02:44,634:INFO:                 pip: 22.3.1
2023-03-23 17:02:44,634:INFO:          setuptools: 65.5.0
2023-03-23 17:02:44,634:INFO:             pycaret: 3.0.0
2023-03-23 17:02:44,635:INFO:             IPython: 8.7.0
2023-03-23 17:02:44,635:INFO:          ipywidgets: 7.6.5
2023-03-23 17:02:44,635:INFO:                tqdm: 4.64.1
2023-03-23 17:02:44,635:INFO:               numpy: 1.21.5
2023-03-23 17:02:44,635:INFO:              pandas: 1.4.4
2023-03-23 17:02:44,635:INFO:              jinja2: 2.11.3
2023-03-23 17:02:44,635:INFO:               scipy: 1.9.3
2023-03-23 17:02:44,635:INFO:              joblib: 1.2.0
2023-03-23 17:02:44,635:INFO:             sklearn: 1.0.2
2023-03-23 17:02:44,635:INFO:                pyod: 1.0.9
2023-03-23 17:02:44,635:INFO:            imblearn: 0.10.1
2023-03-23 17:02:44,635:INFO:   category_encoders: 2.6.0
2023-03-23 17:02:44,635:INFO:            lightgbm: 3.3.5
2023-03-23 17:02:44,635:INFO:               numba: 0.56.4
2023-03-23 17:02:44,635:INFO:            requests: 2.27.1
2023-03-23 17:02:44,635:INFO:          matplotlib: 3.6.2
2023-03-23 17:02:44,635:INFO:          scikitplot: 0.3.7
2023-03-23 17:02:44,635:INFO:         yellowbrick: 1.5
2023-03-23 17:02:44,635:INFO:              plotly: 5.9.0
2023-03-23 17:02:44,635:INFO:             kaleido: 0.2.1
2023-03-23 17:02:44,636:INFO:         statsmodels: 0.13.2
2023-03-23 17:02:44,636:INFO:              sktime: 0.16.1
2023-03-23 17:02:44,636:INFO:               tbats: 1.1.2
2023-03-23 17:02:44,636:INFO:            pmdarima: 2.0.3
2023-03-23 17:02:44,636:INFO:              psutil: 5.9.0
2023-03-23 17:02:44,636:INFO:PyCaret optional dependencies:
2023-03-23 17:02:44,649:INFO:                shap: Not installed
2023-03-23 17:02:44,649:INFO:           interpret: Not installed
2023-03-23 17:02:44,649:INFO:                umap: Not installed
2023-03-23 17:02:44,649:INFO:    pandas_profiling: Not installed
2023-03-23 17:02:44,649:INFO:  explainerdashboard: Not installed
2023-03-23 17:02:44,650:INFO:             autoviz: Not installed
2023-03-23 17:02:44,650:INFO:           fairlearn: Not installed
2023-03-23 17:02:44,650:INFO:             xgboost: 1.7.1
2023-03-23 17:02:44,650:INFO:            catboost: 1.0.6
2023-03-23 17:02:44,650:INFO:              kmodes: Not installed
2023-03-23 17:02:44,650:INFO:             mlxtend: Not installed
2023-03-23 17:02:44,650:INFO:       statsforecast: Not installed
2023-03-23 17:02:44,650:INFO:        tune_sklearn: Not installed
2023-03-23 17:02:44,650:INFO:                 ray: Not installed
2023-03-23 17:02:44,650:INFO:            hyperopt: Not installed
2023-03-23 17:02:44,650:INFO:              optuna: Not installed
2023-03-23 17:02:44,650:INFO:               skopt: Not installed
2023-03-23 17:02:44,650:INFO:              mlflow: Not installed
2023-03-23 17:02:44,650:INFO:              gradio: Not installed
2023-03-23 17:02:44,650:INFO:             fastapi: Not installed
2023-03-23 17:02:44,650:INFO:             uvicorn: Not installed
2023-03-23 17:02:44,650:INFO:              m2cgen: Not installed
2023-03-23 17:02:44,650:INFO:           evidently: Not installed
2023-03-23 17:02:44,650:INFO:               fugue: Not installed
2023-03-23 17:02:44,650:INFO:           streamlit: Not installed
2023-03-23 17:02:44,650:INFO:             prophet: Not installed
2023-03-23 17:02:44,650:INFO:None
2023-03-23 17:02:44,651:INFO:Set up data.
2023-03-23 17:02:44,660:INFO:Set up train/test split.
2023-03-23 17:02:44,660:INFO:Set up data.
2023-03-23 17:02:44,668:INFO:Set up index.
2023-03-23 17:02:44,668:INFO:Set up folding strategy.
2023-03-23 17:02:44,668:INFO:Assigning column types.
2023-03-23 17:02:44,674:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-23 17:02:44,674:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-23 17:02:44,678:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 17:02:44,683:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 17:02:44,753:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:02:44,798:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:02:44,799:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:02:45,325:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:02:45,574:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-23 17:02:45,579:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 17:02:45,584:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 17:02:45,653:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:02:45,699:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:02:45,699:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:02:45,702:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:02:45,702:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-23 17:02:45,707:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 17:02:45,712:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 17:02:45,779:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:02:45,824:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:02:45,825:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:02:45,827:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:02:45,833:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 17:02:45,837:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 17:02:45,905:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:02:45,952:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:02:45,952:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:02:45,955:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:02:45,956:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-23 17:02:45,965:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 17:02:46,034:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:02:46,081:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:02:46,082:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:02:46,085:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:02:46,095:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 17:02:46,166:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:02:46,212:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:02:46,213:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:02:46,215:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:02:46,216:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-23 17:02:46,302:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:02:46,351:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:02:46,352:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:02:46,355:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:02:46,435:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:02:46,482:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:02:46,482:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:02:46,485:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:02:46,485:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-23 17:02:46,563:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:02:46,610:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:02:46,613:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:02:46,692:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:02:46,738:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:02:46,740:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:02:46,741:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-23 17:02:46,872:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:02:46,875:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:02:47,013:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:02:47,017:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:02:47,020:INFO:Preparing preprocessing pipeline...
2023-03-23 17:02:47,020:INFO:Set up simple imputation.
2023-03-23 17:02:47,021:INFO:Set up column name cleaning.
2023-03-23 17:02:47,082:INFO:Finished creating preprocessing pipeline.
2023-03-23 17:02:47,089:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\CHUKWUKA\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['onehotencoder__Item_Fat_Content_Low '
                                             'Fat',
                                             'onehotencoder__Item_Fat_Content_Regular',
                                             'onehotencoder__Outlet_Size_High',
                                             'onehotencoder__Outlet_Size_Medium',
                                             'onehotencoder__Outlet_Size_Small',
                                             'onehotencoder__Outlet_...
                                             'remainder__Item_Weight',
                                             'remainder__Item_Visibility',
                                             'remainder__Item_Type',
                                             'remainder__Item_MRP',
                                             'remainder__Item_Type_Category',
                                             'remainder__Outlet_Age'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-03-23 17:02:47,089:INFO:Creating final display dataframe.
2023-03-23 17:02:47,205:INFO:Setup _display_container:                     Description              Value
0                    Session id                666
1                        Target  Item_Outlet_Sales
2                   Target type         Regression
3           Original data shape         (8523, 19)
4        Transformed data shape        (13637, 19)
5   Transformed train set shape         (8523, 19)
6    Transformed test set shape         (5114, 19)
7              Numeric features                 18
8                    Preprocess               True
9               Imputation type             simple
10           Numeric imputation               mean
11       Categorical imputation               mode
12               Fold Generator              KFold
13                  Fold Number                 10
14                     CPU Jobs                 -1
15                      Use GPU              False
16               Log Experiment              False
17              Experiment Name   reg-default-name
18                          USI               bbe5
2023-03-23 17:02:47,347:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:02:47,351:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:02:47,489:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:02:47,492:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:02:47,493:INFO:setup() successfully completed in 3.37s...............
2023-03-23 17:03:04,512:INFO:Initializing compare_models()
2023-03-23 17:03:04,512:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, include=None, fold=None, round=4, cross_validation=True, sort=MSE, n_select=1, budget_time=10, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MSE', 'n_select': 1, 'budget_time': 10, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-23 17:03:04,512:INFO:Checking exceptions
2023-03-23 17:03:04,519:INFO:Preparing display monitor
2023-03-23 17:03:04,584:INFO:Time budget is 10 minutes
2023-03-23 17:03:04,584:INFO:Initializing Linear Regression
2023-03-23 17:03:04,584:INFO:Total runtime is 1.6144911448160808e-05 minutes
2023-03-23 17:03:04,587:INFO:SubProcess create_model() called ==================================
2023-03-23 17:03:04,589:INFO:Initializing create_model()
2023-03-23 17:03:04,589:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:03:04,589:INFO:Checking exceptions
2023-03-23 17:03:04,589:INFO:Importing libraries
2023-03-23 17:03:04,589:INFO:Copying training dataset
2023-03-23 17:03:04,607:INFO:Defining folds
2023-03-23 17:03:04,607:INFO:Declaring metric variables
2023-03-23 17:03:04,613:INFO:Importing untrained model
2023-03-23 17:03:04,617:INFO:Linear Regression Imported successfully
2023-03-23 17:03:04,624:INFO:Starting cross validation
2023-03-23 17:03:04,632:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:03:21,984:INFO:Calculating mean and std
2023-03-23 17:03:21,985:INFO:Creating metrics dataframe
2023-03-23 17:03:22,375:INFO:Uploading results into container
2023-03-23 17:03:22,376:INFO:Uploading model into container now
2023-03-23 17:03:22,376:INFO:_master_model_container: 1
2023-03-23 17:03:22,376:INFO:_display_container: 2
2023-03-23 17:03:22,377:INFO:LinearRegression(n_jobs=-1)
2023-03-23 17:03:22,377:INFO:create_model() successfully completed......................................
2023-03-23 17:03:22,668:INFO:SubProcess create_model() end ==================================
2023-03-23 17:03:22,668:INFO:Creating metrics dataframe
2023-03-23 17:03:22,677:INFO:Initializing Lasso Regression
2023-03-23 17:03:22,677:INFO:Total runtime is 0.30156558354695634 minutes
2023-03-23 17:03:22,681:INFO:SubProcess create_model() called ==================================
2023-03-23 17:03:22,681:INFO:Initializing create_model()
2023-03-23 17:03:22,681:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:03:22,682:INFO:Checking exceptions
2023-03-23 17:03:22,682:INFO:Importing libraries
2023-03-23 17:03:22,682:INFO:Copying training dataset
2023-03-23 17:03:22,702:INFO:Defining folds
2023-03-23 17:03:22,702:INFO:Declaring metric variables
2023-03-23 17:03:22,707:INFO:Importing untrained model
2023-03-23 17:03:22,712:INFO:Lasso Regression Imported successfully
2023-03-23 17:03:22,721:INFO:Starting cross validation
2023-03-23 17:03:22,723:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:03:25,549:INFO:Calculating mean and std
2023-03-23 17:03:25,550:INFO:Creating metrics dataframe
2023-03-23 17:03:25,940:INFO:Uploading results into container
2023-03-23 17:03:25,941:INFO:Uploading model into container now
2023-03-23 17:03:25,941:INFO:_master_model_container: 2
2023-03-23 17:03:25,941:INFO:_display_container: 2
2023-03-23 17:03:25,942:INFO:Lasso(random_state=666)
2023-03-23 17:03:25,942:INFO:create_model() successfully completed......................................
2023-03-23 17:03:26,215:INFO:SubProcess create_model() end ==================================
2023-03-23 17:03:26,215:INFO:Creating metrics dataframe
2023-03-23 17:03:26,225:INFO:Initializing Ridge Regression
2023-03-23 17:03:26,225:INFO:Total runtime is 0.36069285472234086 minutes
2023-03-23 17:03:26,230:INFO:SubProcess create_model() called ==================================
2023-03-23 17:03:26,230:INFO:Initializing create_model()
2023-03-23 17:03:26,230:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:03:26,230:INFO:Checking exceptions
2023-03-23 17:03:26,230:INFO:Importing libraries
2023-03-23 17:03:26,231:INFO:Copying training dataset
2023-03-23 17:03:26,252:INFO:Defining folds
2023-03-23 17:03:26,256:INFO:Declaring metric variables
2023-03-23 17:03:26,264:INFO:Importing untrained model
2023-03-23 17:03:26,272:INFO:Ridge Regression Imported successfully
2023-03-23 17:03:26,279:INFO:Starting cross validation
2023-03-23 17:03:26,280:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:03:26,370:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.19994e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:03:26,372:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17704e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:03:26,382:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17152e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:03:26,398:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.18403e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:03:26,418:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17938e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:03:26,431:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.2047e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:03:26,469:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.19028e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:03:26,477:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.20487e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:03:27,104:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17503e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:03:27,134:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.18623e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:03:29,431:INFO:Calculating mean and std
2023-03-23 17:03:29,432:INFO:Creating metrics dataframe
2023-03-23 17:03:29,919:INFO:Uploading results into container
2023-03-23 17:03:29,920:INFO:Uploading model into container now
2023-03-23 17:03:29,921:INFO:_master_model_container: 3
2023-03-23 17:03:29,921:INFO:_display_container: 2
2023-03-23 17:03:29,923:INFO:Ridge(random_state=666)
2023-03-23 17:03:29,924:INFO:create_model() successfully completed......................................
2023-03-23 17:03:30,242:INFO:SubProcess create_model() end ==================================
2023-03-23 17:03:30,242:INFO:Creating metrics dataframe
2023-03-23 17:03:30,252:INFO:Initializing Elastic Net
2023-03-23 17:03:30,253:INFO:Total runtime is 0.4278066237767537 minutes
2023-03-23 17:03:30,256:INFO:SubProcess create_model() called ==================================
2023-03-23 17:03:30,257:INFO:Initializing create_model()
2023-03-23 17:03:30,257:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:03:30,257:INFO:Checking exceptions
2023-03-23 17:03:30,257:INFO:Importing libraries
2023-03-23 17:03:30,257:INFO:Copying training dataset
2023-03-23 17:03:30,275:INFO:Defining folds
2023-03-23 17:03:30,276:INFO:Declaring metric variables
2023-03-23 17:03:30,280:INFO:Importing untrained model
2023-03-23 17:03:30,283:INFO:Elastic Net Imported successfully
2023-03-23 17:03:30,291:INFO:Starting cross validation
2023-03-23 17:03:30,292:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:03:33,294:INFO:Calculating mean and std
2023-03-23 17:03:33,554:INFO:Creating metrics dataframe
2023-03-23 17:03:33,818:INFO:Uploading results into container
2023-03-23 17:03:33,818:INFO:Uploading model into container now
2023-03-23 17:03:33,819:INFO:_master_model_container: 4
2023-03-23 17:03:33,819:INFO:_display_container: 2
2023-03-23 17:03:33,819:INFO:ElasticNet(random_state=666)
2023-03-23 17:03:33,819:INFO:create_model() successfully completed......................................
2023-03-23 17:03:34,087:INFO:SubProcess create_model() end ==================================
2023-03-23 17:03:34,088:INFO:Creating metrics dataframe
2023-03-23 17:03:34,234:INFO:Initializing Least Angle Regression
2023-03-23 17:03:34,234:INFO:Total runtime is 0.4941765745480855 minutes
2023-03-23 17:03:34,238:INFO:SubProcess create_model() called ==================================
2023-03-23 17:03:34,238:INFO:Initializing create_model()
2023-03-23 17:03:34,239:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:03:34,239:INFO:Checking exceptions
2023-03-23 17:03:34,239:INFO:Importing libraries
2023-03-23 17:03:34,239:INFO:Copying training dataset
2023-03-23 17:03:34,261:INFO:Defining folds
2023-03-23 17:03:34,261:INFO:Declaring metric variables
2023-03-23 17:03:34,265:INFO:Importing untrained model
2023-03-23 17:03:34,270:INFO:Least Angle Regression Imported successfully
2023-03-23 17:03:34,277:INFO:Starting cross validation
2023-03-23 17:03:34,278:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:03:34,347:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:34,348:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:34,374:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:34,377:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:34,395:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:34,413:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:34,423:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.330e-02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:03:34,424:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.934e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:03:34,426:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:34,435:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=8.345e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:03:34,443:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.611e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:03:34,444:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.258e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:03:34,447:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:34,978:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:34,984:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=9.870e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:03:34,991:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:37,094:INFO:Calculating mean and std
2023-03-23 17:03:37,095:INFO:Creating metrics dataframe
2023-03-23 17:03:37,481:INFO:Uploading results into container
2023-03-23 17:03:37,483:INFO:Uploading model into container now
2023-03-23 17:03:37,483:INFO:_master_model_container: 5
2023-03-23 17:03:37,483:INFO:_display_container: 2
2023-03-23 17:03:37,483:INFO:Lars(random_state=666)
2023-03-23 17:03:37,484:INFO:create_model() successfully completed......................................
2023-03-23 17:03:37,751:INFO:SubProcess create_model() end ==================================
2023-03-23 17:03:37,751:INFO:Creating metrics dataframe
2023-03-23 17:03:37,762:INFO:Initializing Lasso Least Angle Regression
2023-03-23 17:03:37,763:INFO:Total runtime is 0.5529833833376566 minutes
2023-03-23 17:03:37,766:INFO:SubProcess create_model() called ==================================
2023-03-23 17:03:37,766:INFO:Initializing create_model()
2023-03-23 17:03:37,766:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:03:37,766:INFO:Checking exceptions
2023-03-23 17:03:37,766:INFO:Importing libraries
2023-03-23 17:03:37,766:INFO:Copying training dataset
2023-03-23 17:03:37,785:INFO:Defining folds
2023-03-23 17:03:37,786:INFO:Declaring metric variables
2023-03-23 17:03:37,789:INFO:Importing untrained model
2023-03-23 17:03:37,793:INFO:Lasso Least Angle Regression Imported successfully
2023-03-23 17:03:37,801:INFO:Starting cross validation
2023-03-23 17:03:37,802:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:03:37,889:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:03:37,903:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:03:37,920:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:03:37,932:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:03:37,959:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:03:37,960:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:03:37,975:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:03:37,988:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:03:38,678:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:03:38,726:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:03:40,691:INFO:Calculating mean and std
2023-03-23 17:03:40,693:INFO:Creating metrics dataframe
2023-03-23 17:03:41,155:INFO:Uploading results into container
2023-03-23 17:03:41,156:INFO:Uploading model into container now
2023-03-23 17:03:41,156:INFO:_master_model_container: 6
2023-03-23 17:03:41,156:INFO:_display_container: 2
2023-03-23 17:03:41,157:INFO:LassoLars(random_state=666)
2023-03-23 17:03:41,157:INFO:create_model() successfully completed......................................
2023-03-23 17:03:41,443:INFO:SubProcess create_model() end ==================================
2023-03-23 17:03:41,443:INFO:Creating metrics dataframe
2023-03-23 17:03:41,454:INFO:Initializing Orthogonal Matching Pursuit
2023-03-23 17:03:41,454:INFO:Total runtime is 0.6145112037658691 minutes
2023-03-23 17:03:41,458:INFO:SubProcess create_model() called ==================================
2023-03-23 17:03:41,458:INFO:Initializing create_model()
2023-03-23 17:03:41,458:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:03:41,458:INFO:Checking exceptions
2023-03-23 17:03:41,459:INFO:Importing libraries
2023-03-23 17:03:41,459:INFO:Copying training dataset
2023-03-23 17:03:41,477:INFO:Defining folds
2023-03-23 17:03:41,477:INFO:Declaring metric variables
2023-03-23 17:03:41,481:INFO:Importing untrained model
2023-03-23 17:03:41,485:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-23 17:03:41,492:INFO:Starting cross validation
2023-03-23 17:03:41,493:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:03:41,561:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:41,574:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:41,588:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:41,593:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:41,624:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:41,639:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:41,652:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:41,669:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:42,247:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:42,267:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:03:44,422:INFO:Calculating mean and std
2023-03-23 17:03:44,423:INFO:Creating metrics dataframe
2023-03-23 17:03:44,815:INFO:Uploading results into container
2023-03-23 17:03:44,816:INFO:Uploading model into container now
2023-03-23 17:03:44,816:INFO:_master_model_container: 7
2023-03-23 17:03:44,816:INFO:_display_container: 2
2023-03-23 17:03:44,817:INFO:OrthogonalMatchingPursuit()
2023-03-23 17:03:44,817:INFO:create_model() successfully completed......................................
2023-03-23 17:03:45,094:INFO:SubProcess create_model() end ==================================
2023-03-23 17:03:45,094:INFO:Creating metrics dataframe
2023-03-23 17:03:45,105:INFO:Initializing Bayesian Ridge
2023-03-23 17:03:45,105:INFO:Total runtime is 0.6753665328025817 minutes
2023-03-23 17:03:45,110:INFO:SubProcess create_model() called ==================================
2023-03-23 17:03:45,110:INFO:Initializing create_model()
2023-03-23 17:03:45,111:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:03:45,111:INFO:Checking exceptions
2023-03-23 17:03:45,111:INFO:Importing libraries
2023-03-23 17:03:45,111:INFO:Copying training dataset
2023-03-23 17:03:45,132:INFO:Defining folds
2023-03-23 17:03:45,132:INFO:Declaring metric variables
2023-03-23 17:03:45,137:INFO:Importing untrained model
2023-03-23 17:03:45,141:INFO:Bayesian Ridge Imported successfully
2023-03-23 17:03:45,153:INFO:Starting cross validation
2023-03-23 17:03:45,153:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:03:47,965:INFO:Calculating mean and std
2023-03-23 17:03:47,966:INFO:Creating metrics dataframe
2023-03-23 17:03:48,360:INFO:Uploading results into container
2023-03-23 17:03:48,361:INFO:Uploading model into container now
2023-03-23 17:03:48,361:INFO:_master_model_container: 8
2023-03-23 17:03:48,361:INFO:_display_container: 2
2023-03-23 17:03:48,362:INFO:BayesianRidge()
2023-03-23 17:03:48,362:INFO:create_model() successfully completed......................................
2023-03-23 17:03:48,625:INFO:SubProcess create_model() end ==================================
2023-03-23 17:03:48,625:INFO:Creating metrics dataframe
2023-03-23 17:03:48,636:INFO:Initializing Passive Aggressive Regressor
2023-03-23 17:03:48,636:INFO:Total runtime is 0.7342099666595459 minutes
2023-03-23 17:03:48,640:INFO:SubProcess create_model() called ==================================
2023-03-23 17:03:48,640:INFO:Initializing create_model()
2023-03-23 17:03:48,640:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:03:48,640:INFO:Checking exceptions
2023-03-23 17:03:48,641:INFO:Importing libraries
2023-03-23 17:03:48,641:INFO:Copying training dataset
2023-03-23 17:03:48,659:INFO:Defining folds
2023-03-23 17:03:48,659:INFO:Declaring metric variables
2023-03-23 17:03:48,662:INFO:Importing untrained model
2023-03-23 17:03:48,666:INFO:Passive Aggressive Regressor Imported successfully
2023-03-23 17:03:48,673:INFO:Starting cross validation
2023-03-23 17:03:48,674:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:03:51,501:INFO:Calculating mean and std
2023-03-23 17:03:51,502:INFO:Creating metrics dataframe
2023-03-23 17:03:51,898:INFO:Uploading results into container
2023-03-23 17:03:51,899:INFO:Uploading model into container now
2023-03-23 17:03:51,899:INFO:_master_model_container: 9
2023-03-23 17:03:51,899:INFO:_display_container: 2
2023-03-23 17:03:51,900:INFO:PassiveAggressiveRegressor(random_state=666)
2023-03-23 17:03:51,900:INFO:create_model() successfully completed......................................
2023-03-23 17:03:52,165:INFO:SubProcess create_model() end ==================================
2023-03-23 17:03:52,165:INFO:Creating metrics dataframe
2023-03-23 17:03:52,176:INFO:Initializing Huber Regressor
2023-03-23 17:03:52,176:INFO:Total runtime is 0.7932018717130025 minutes
2023-03-23 17:03:52,181:INFO:SubProcess create_model() called ==================================
2023-03-23 17:03:52,181:INFO:Initializing create_model()
2023-03-23 17:03:52,181:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:03:52,181:INFO:Checking exceptions
2023-03-23 17:03:52,181:INFO:Importing libraries
2023-03-23 17:03:52,182:INFO:Copying training dataset
2023-03-23 17:03:52,199:INFO:Defining folds
2023-03-23 17:03:52,199:INFO:Declaring metric variables
2023-03-23 17:03:52,203:INFO:Importing untrained model
2023-03-23 17:03:52,207:INFO:Huber Regressor Imported successfully
2023-03-23 17:03:52,214:INFO:Starting cross validation
2023-03-23 17:03:52,215:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:03:52,894:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:03:53,025:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:03:53,029:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:03:53,040:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:03:53,061:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:03:53,073:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:03:53,090:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:03:53,119:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:03:53,876:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:03:53,959:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:03:55,764:INFO:Calculating mean and std
2023-03-23 17:03:55,765:INFO:Creating metrics dataframe
2023-03-23 17:03:56,152:INFO:Uploading results into container
2023-03-23 17:03:56,153:INFO:Uploading model into container now
2023-03-23 17:03:56,153:INFO:_master_model_container: 10
2023-03-23 17:03:56,153:INFO:_display_container: 2
2023-03-23 17:03:56,154:INFO:HuberRegressor()
2023-03-23 17:03:56,154:INFO:create_model() successfully completed......................................
2023-03-23 17:03:56,418:INFO:SubProcess create_model() end ==================================
2023-03-23 17:03:56,419:INFO:Creating metrics dataframe
2023-03-23 17:03:56,431:INFO:Initializing K Neighbors Regressor
2023-03-23 17:03:56,431:INFO:Total runtime is 0.8641265432039896 minutes
2023-03-23 17:03:56,435:INFO:SubProcess create_model() called ==================================
2023-03-23 17:03:56,435:INFO:Initializing create_model()
2023-03-23 17:03:56,435:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:03:56,435:INFO:Checking exceptions
2023-03-23 17:03:56,435:INFO:Importing libraries
2023-03-23 17:03:56,435:INFO:Copying training dataset
2023-03-23 17:03:56,454:INFO:Defining folds
2023-03-23 17:03:56,454:INFO:Declaring metric variables
2023-03-23 17:03:56,458:INFO:Importing untrained model
2023-03-23 17:03:56,462:INFO:K Neighbors Regressor Imported successfully
2023-03-23 17:03:56,470:INFO:Starting cross validation
2023-03-23 17:03:56,471:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:04:00,210:INFO:Calculating mean and std
2023-03-23 17:04:00,212:INFO:Creating metrics dataframe
2023-03-23 17:04:00,601:INFO:Uploading results into container
2023-03-23 17:04:00,601:INFO:Uploading model into container now
2023-03-23 17:04:00,602:INFO:_master_model_container: 11
2023-03-23 17:04:00,602:INFO:_display_container: 2
2023-03-23 17:04:00,602:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-23 17:04:00,603:INFO:create_model() successfully completed......................................
2023-03-23 17:04:00,866:INFO:SubProcess create_model() end ==================================
2023-03-23 17:04:00,867:INFO:Creating metrics dataframe
2023-03-23 17:04:00,891:INFO:Initializing Decision Tree Regressor
2023-03-23 17:04:00,891:INFO:Total runtime is 0.9384558200836182 minutes
2023-03-23 17:04:00,895:INFO:SubProcess create_model() called ==================================
2023-03-23 17:04:00,896:INFO:Initializing create_model()
2023-03-23 17:04:00,896:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:04:00,896:INFO:Checking exceptions
2023-03-23 17:04:00,896:INFO:Importing libraries
2023-03-23 17:04:00,896:INFO:Copying training dataset
2023-03-23 17:04:00,914:INFO:Defining folds
2023-03-23 17:04:00,915:INFO:Declaring metric variables
2023-03-23 17:04:00,919:INFO:Importing untrained model
2023-03-23 17:04:00,923:INFO:Decision Tree Regressor Imported successfully
2023-03-23 17:04:00,935:INFO:Starting cross validation
2023-03-23 17:04:00,936:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:04:03,935:INFO:Calculating mean and std
2023-03-23 17:04:03,936:INFO:Creating metrics dataframe
2023-03-23 17:04:04,324:INFO:Uploading results into container
2023-03-23 17:04:04,325:INFO:Uploading model into container now
2023-03-23 17:04:04,325:INFO:_master_model_container: 12
2023-03-23 17:04:04,326:INFO:_display_container: 2
2023-03-23 17:04:04,326:INFO:DecisionTreeRegressor(random_state=666)
2023-03-23 17:04:04,326:INFO:create_model() successfully completed......................................
2023-03-23 17:04:04,589:INFO:SubProcess create_model() end ==================================
2023-03-23 17:04:04,589:INFO:Creating metrics dataframe
2023-03-23 17:04:04,602:INFO:Initializing Random Forest Regressor
2023-03-23 17:04:04,603:INFO:Total runtime is 1.000333054860433 minutes
2023-03-23 17:04:04,607:INFO:SubProcess create_model() called ==================================
2023-03-23 17:04:04,607:INFO:Initializing create_model()
2023-03-23 17:04:04,607:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:04:04,607:INFO:Checking exceptions
2023-03-23 17:04:04,607:INFO:Importing libraries
2023-03-23 17:04:04,608:INFO:Copying training dataset
2023-03-23 17:04:04,630:INFO:Defining folds
2023-03-23 17:04:04,630:INFO:Declaring metric variables
2023-03-23 17:04:04,635:INFO:Importing untrained model
2023-03-23 17:04:04,641:INFO:Random Forest Regressor Imported successfully
2023-03-23 17:04:04,652:INFO:Starting cross validation
2023-03-23 17:04:04,652:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:04:10,381:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.46s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-23 17:04:10,542:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.34s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-23 17:04:12,241:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.96s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-23 17:04:12,283:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-03-23 17:04:17,731:INFO:Calculating mean and std
2023-03-23 17:04:17,734:INFO:Creating metrics dataframe
2023-03-23 17:04:18,131:INFO:Uploading results into container
2023-03-23 17:04:18,131:INFO:Uploading model into container now
2023-03-23 17:04:18,132:INFO:_master_model_container: 13
2023-03-23 17:04:18,132:INFO:_display_container: 2
2023-03-23 17:04:18,133:INFO:RandomForestRegressor(n_jobs=-1, random_state=666)
2023-03-23 17:04:18,133:INFO:create_model() successfully completed......................................
2023-03-23 17:04:18,399:INFO:SubProcess create_model() end ==================================
2023-03-23 17:04:18,399:INFO:Creating metrics dataframe
2023-03-23 17:04:18,412:INFO:Initializing Extra Trees Regressor
2023-03-23 17:04:18,412:INFO:Total runtime is 1.230474054813385 minutes
2023-03-23 17:04:18,416:INFO:SubProcess create_model() called ==================================
2023-03-23 17:04:18,417:INFO:Initializing create_model()
2023-03-23 17:04:18,417:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:04:18,417:INFO:Checking exceptions
2023-03-23 17:04:18,417:INFO:Importing libraries
2023-03-23 17:04:18,417:INFO:Copying training dataset
2023-03-23 17:04:18,437:INFO:Defining folds
2023-03-23 17:04:18,437:INFO:Declaring metric variables
2023-03-23 17:04:18,442:INFO:Importing untrained model
2023-03-23 17:04:18,446:INFO:Extra Trees Regressor Imported successfully
2023-03-23 17:04:18,454:INFO:Starting cross validation
2023-03-23 17:04:18,455:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:04:23,627:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-23 17:04:23,817:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.89s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-23 17:04:28,799:INFO:Calculating mean and std
2023-03-23 17:04:28,800:INFO:Creating metrics dataframe
2023-03-23 17:04:29,194:INFO:Uploading results into container
2023-03-23 17:04:29,195:INFO:Uploading model into container now
2023-03-23 17:04:29,195:INFO:_master_model_container: 14
2023-03-23 17:04:29,195:INFO:_display_container: 2
2023-03-23 17:04:29,196:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=666)
2023-03-23 17:04:29,196:INFO:create_model() successfully completed......................................
2023-03-23 17:04:29,459:INFO:SubProcess create_model() end ==================================
2023-03-23 17:04:29,459:INFO:Creating metrics dataframe
2023-03-23 17:04:29,638:INFO:Initializing AdaBoost Regressor
2023-03-23 17:04:29,638:INFO:Total runtime is 1.4175787925720216 minutes
2023-03-23 17:04:29,642:INFO:SubProcess create_model() called ==================================
2023-03-23 17:04:29,643:INFO:Initializing create_model()
2023-03-23 17:04:29,643:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:04:29,643:INFO:Checking exceptions
2023-03-23 17:04:29,643:INFO:Importing libraries
2023-03-23 17:04:29,643:INFO:Copying training dataset
2023-03-23 17:04:29,662:INFO:Defining folds
2023-03-23 17:04:29,662:INFO:Declaring metric variables
2023-03-23 17:04:29,665:INFO:Importing untrained model
2023-03-23 17:04:29,670:INFO:AdaBoost Regressor Imported successfully
2023-03-23 17:04:29,678:INFO:Starting cross validation
2023-03-23 17:04:29,679:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:04:33,897:INFO:Calculating mean and std
2023-03-23 17:04:33,899:INFO:Creating metrics dataframe
2023-03-23 17:04:34,308:INFO:Uploading results into container
2023-03-23 17:04:34,309:INFO:Uploading model into container now
2023-03-23 17:04:34,309:INFO:_master_model_container: 15
2023-03-23 17:04:34,310:INFO:_display_container: 2
2023-03-23 17:04:34,310:INFO:AdaBoostRegressor(random_state=666)
2023-03-23 17:04:34,311:INFO:create_model() successfully completed......................................
2023-03-23 17:04:34,584:INFO:SubProcess create_model() end ==================================
2023-03-23 17:04:34,584:INFO:Creating metrics dataframe
2023-03-23 17:04:34,661:INFO:Initializing Gradient Boosting Regressor
2023-03-23 17:04:34,661:INFO:Total runtime is 1.5012893875439963 minutes
2023-03-23 17:04:34,665:INFO:SubProcess create_model() called ==================================
2023-03-23 17:04:34,665:INFO:Initializing create_model()
2023-03-23 17:04:34,665:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:04:34,665:INFO:Checking exceptions
2023-03-23 17:04:34,665:INFO:Importing libraries
2023-03-23 17:04:34,665:INFO:Copying training dataset
2023-03-23 17:04:34,684:INFO:Defining folds
2023-03-23 17:04:34,685:INFO:Declaring metric variables
2023-03-23 17:04:34,689:INFO:Importing untrained model
2023-03-23 17:04:34,693:INFO:Gradient Boosting Regressor Imported successfully
2023-03-23 17:04:34,701:INFO:Starting cross validation
2023-03-23 17:04:34,702:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:04:41,317:INFO:Calculating mean and std
2023-03-23 17:04:41,318:INFO:Creating metrics dataframe
2023-03-23 17:04:41,739:INFO:Uploading results into container
2023-03-23 17:04:41,740:INFO:Uploading model into container now
2023-03-23 17:04:41,740:INFO:_master_model_container: 16
2023-03-23 17:04:41,740:INFO:_display_container: 2
2023-03-23 17:04:41,740:INFO:GradientBoostingRegressor(random_state=666)
2023-03-23 17:04:41,741:INFO:create_model() successfully completed......................................
2023-03-23 17:04:42,057:INFO:SubProcess create_model() end ==================================
2023-03-23 17:04:42,057:INFO:Creating metrics dataframe
2023-03-23 17:04:42,072:INFO:Initializing Extreme Gradient Boosting
2023-03-23 17:04:42,073:INFO:Total runtime is 1.624827492237091 minutes
2023-03-23 17:04:42,077:INFO:SubProcess create_model() called ==================================
2023-03-23 17:04:42,077:INFO:Initializing create_model()
2023-03-23 17:04:42,077:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:04:42,146:INFO:Checking exceptions
2023-03-23 17:04:42,146:INFO:Importing libraries
2023-03-23 17:04:42,146:INFO:Copying training dataset
2023-03-23 17:04:42,167:INFO:Defining folds
2023-03-23 17:04:42,168:INFO:Declaring metric variables
2023-03-23 17:04:42,172:INFO:Importing untrained model
2023-03-23 17:04:42,177:INFO:Extreme Gradient Boosting Imported successfully
2023-03-23 17:04:42,186:INFO:Starting cross validation
2023-03-23 17:04:42,187:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:04:50,179:INFO:Calculating mean and std
2023-03-23 17:04:50,180:INFO:Creating metrics dataframe
2023-03-23 17:04:50,631:INFO:Uploading results into container
2023-03-23 17:04:50,632:INFO:Uploading model into container now
2023-03-23 17:04:50,632:INFO:_master_model_container: 17
2023-03-23 17:04:50,632:INFO:_display_container: 2
2023-03-23 17:04:50,633:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=666, ...)
2023-03-23 17:04:50,634:INFO:create_model() successfully completed......................................
2023-03-23 17:04:50,907:INFO:SubProcess create_model() end ==================================
2023-03-23 17:04:50,907:INFO:Creating metrics dataframe
2023-03-23 17:04:50,920:INFO:Initializing Light Gradient Boosting Machine
2023-03-23 17:04:50,920:INFO:Total runtime is 1.7722788651784263 minutes
2023-03-23 17:04:50,925:INFO:SubProcess create_model() called ==================================
2023-03-23 17:04:50,925:INFO:Initializing create_model()
2023-03-23 17:04:50,925:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:04:50,925:INFO:Checking exceptions
2023-03-23 17:04:50,925:INFO:Importing libraries
2023-03-23 17:04:50,926:INFO:Copying training dataset
2023-03-23 17:04:50,945:INFO:Defining folds
2023-03-23 17:04:50,945:INFO:Declaring metric variables
2023-03-23 17:04:50,949:INFO:Importing untrained model
2023-03-23 17:04:50,953:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-23 17:04:50,961:INFO:Starting cross validation
2023-03-23 17:04:50,962:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:04:58,856:INFO:Calculating mean and std
2023-03-23 17:04:58,857:INFO:Creating metrics dataframe
2023-03-23 17:04:59,282:INFO:Uploading results into container
2023-03-23 17:04:59,283:INFO:Uploading model into container now
2023-03-23 17:04:59,284:INFO:_master_model_container: 18
2023-03-23 17:04:59,284:INFO:_display_container: 2
2023-03-23 17:04:59,284:INFO:LGBMRegressor(random_state=666)
2023-03-23 17:04:59,285:INFO:create_model() successfully completed......................................
2023-03-23 17:04:59,551:INFO:SubProcess create_model() end ==================================
2023-03-23 17:04:59,551:INFO:Creating metrics dataframe
2023-03-23 17:04:59,564:INFO:Initializing CatBoost Regressor
2023-03-23 17:04:59,565:INFO:Total runtime is 1.9163528045018516 minutes
2023-03-23 17:04:59,568:INFO:SubProcess create_model() called ==================================
2023-03-23 17:04:59,569:INFO:Initializing create_model()
2023-03-23 17:04:59,569:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:04:59,569:INFO:Checking exceptions
2023-03-23 17:04:59,569:INFO:Importing libraries
2023-03-23 17:04:59,569:INFO:Copying training dataset
2023-03-23 17:04:59,589:INFO:Defining folds
2023-03-23 17:04:59,589:INFO:Declaring metric variables
2023-03-23 17:04:59,595:INFO:Importing untrained model
2023-03-23 17:04:59,822:INFO:CatBoost Regressor Imported successfully
2023-03-23 17:04:59,836:INFO:Starting cross validation
2023-03-23 17:04:59,838:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:05:25,724:INFO:Calculating mean and std
2023-03-23 17:05:25,725:INFO:Creating metrics dataframe
2023-03-23 17:05:26,208:INFO:Uploading results into container
2023-03-23 17:05:26,209:INFO:Uploading model into container now
2023-03-23 17:05:26,210:INFO:_master_model_container: 19
2023-03-23 17:05:26,210:INFO:_display_container: 2
2023-03-23 17:05:26,210:INFO:<catboost.core.CatBoostRegressor object at 0x0000016C90C3D970>
2023-03-23 17:05:26,210:INFO:create_model() successfully completed......................................
2023-03-23 17:05:26,481:INFO:SubProcess create_model() end ==================================
2023-03-23 17:05:26,481:INFO:Creating metrics dataframe
2023-03-23 17:05:26,499:INFO:Initializing Dummy Regressor
2023-03-23 17:05:26,499:INFO:Total runtime is 2.3652637839317325 minutes
2023-03-23 17:05:26,504:INFO:SubProcess create_model() called ==================================
2023-03-23 17:05:26,504:INFO:Initializing create_model()
2023-03-23 17:05:26,504:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C877CF610>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:05:26,504:INFO:Checking exceptions
2023-03-23 17:05:26,505:INFO:Importing libraries
2023-03-23 17:05:26,505:INFO:Copying training dataset
2023-03-23 17:05:26,525:INFO:Defining folds
2023-03-23 17:05:26,525:INFO:Declaring metric variables
2023-03-23 17:05:26,530:INFO:Importing untrained model
2023-03-23 17:05:26,536:INFO:Dummy Regressor Imported successfully
2023-03-23 17:05:26,546:INFO:Starting cross validation
2023-03-23 17:05:26,547:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:05:30,612:INFO:Calculating mean and std
2023-03-23 17:05:30,612:INFO:Creating metrics dataframe
2023-03-23 17:05:31,043:INFO:Uploading results into container
2023-03-23 17:05:31,044:INFO:Uploading model into container now
2023-03-23 17:05:31,044:INFO:_master_model_container: 20
2023-03-23 17:05:31,045:INFO:_display_container: 2
2023-03-23 17:05:31,045:INFO:DummyRegressor()
2023-03-23 17:05:31,045:INFO:create_model() successfully completed......................................
2023-03-23 17:05:31,345:INFO:SubProcess create_model() end ==================================
2023-03-23 17:05:31,345:INFO:Creating metrics dataframe
2023-03-23 17:05:31,422:INFO:Initializing create_model()
2023-03-23 17:05:31,423:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=GradientBoostingRegressor(random_state=666), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:05:31,423:INFO:Checking exceptions
2023-03-23 17:05:31,426:INFO:Importing libraries
2023-03-23 17:05:31,426:INFO:Copying training dataset
2023-03-23 17:05:31,447:INFO:Defining folds
2023-03-23 17:05:31,447:INFO:Declaring metric variables
2023-03-23 17:05:31,447:INFO:Importing untrained model
2023-03-23 17:05:31,447:INFO:Declaring custom model
2023-03-23 17:05:31,448:INFO:Gradient Boosting Regressor Imported successfully
2023-03-23 17:05:31,449:INFO:Cross validation set to False
2023-03-23 17:05:31,449:INFO:Fitting Model
2023-03-23 17:05:34,769:INFO:GradientBoostingRegressor(random_state=666)
2023-03-23 17:05:34,769:INFO:create_model() successfully completed......................................
2023-03-23 17:05:35,077:INFO:_master_model_container: 20
2023-03-23 17:05:35,077:INFO:_display_container: 2
2023-03-23 17:05:35,077:INFO:GradientBoostingRegressor(random_state=666)
2023-03-23 17:05:35,078:INFO:compare_models() successfully completed......................................
2023-03-23 17:07:16,735:INFO:Initializing predict_model()
2023-03-23 17:07:16,735:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C873186D0>, estimator=GradientBoostingRegressor(random_state=666), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000016C90BB3700>)
2023-03-23 17:07:16,735:INFO:Checking exceptions
2023-03-23 17:07:16,736:INFO:Preloading libraries
2023-03-23 17:07:16,738:INFO:Set up data.
2023-03-23 17:07:16,776:INFO:Set up index.
2023-03-23 17:28:06,266:INFO:PyCaret RegressionExperiment
2023-03-23 17:28:06,267:INFO:Logging name: reg-default-name
2023-03-23 17:28:06,267:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-23 17:28:06,267:INFO:version 3.0.0
2023-03-23 17:28:06,267:INFO:Initializing setup()
2023-03-23 17:28:06,267:INFO:self.USI: 3c81
2023-03-23 17:28:06,267:INFO:self._variable_keys: {'memory', 'gpu_n_jobs_param', 'exp_id', 'html_param', 'y_test', 'seed', 'X_test', 'n_jobs_param', 'data', 'gpu_param', 'X', 'target_param', 'y', 'USI', 'log_plots_param', 'y_train', 'idx', 'logging_param', '_ml_usecase', '_available_plots', 'fold_shuffle_param', 'fold_generator', 'X_train', 'transform_target_param', 'fold_groups_param', 'pipeline', 'exp_name_log'}
2023-03-23 17:28:06,267:INFO:Checking environment
2023-03-23 17:28:06,267:INFO:python_version: 3.9.12
2023-03-23 17:28:06,267:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-23 17:28:06,267:INFO:machine: AMD64
2023-03-23 17:28:06,267:INFO:platform: Windows-10-10.0.19045-SP0
2023-03-23 17:28:06,267:INFO:Memory: svmem(total=8496553984, available=2002882560, percent=76.4, used=6493671424, free=2002882560)
2023-03-23 17:28:06,268:INFO:Physical Core: 4
2023-03-23 17:28:06,268:INFO:Logical Core: 8
2023-03-23 17:28:06,268:INFO:Checking libraries
2023-03-23 17:28:06,268:INFO:System:
2023-03-23 17:28:06,268:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-23 17:28:06,268:INFO:executable: C:\Users\CHUKWUKA\anaconda3\python.exe
2023-03-23 17:28:06,268:INFO:   machine: Windows-10-10.0.19045-SP0
2023-03-23 17:28:06,268:INFO:PyCaret required dependencies:
2023-03-23 17:28:06,268:INFO:                 pip: 22.3.1
2023-03-23 17:28:06,268:INFO:          setuptools: 65.5.0
2023-03-23 17:28:06,268:INFO:             pycaret: 3.0.0
2023-03-23 17:28:06,268:INFO:             IPython: 8.7.0
2023-03-23 17:28:06,268:INFO:          ipywidgets: 7.6.5
2023-03-23 17:28:06,268:INFO:                tqdm: 4.64.1
2023-03-23 17:28:06,268:INFO:               numpy: 1.21.5
2023-03-23 17:28:06,268:INFO:              pandas: 1.4.4
2023-03-23 17:28:06,269:INFO:              jinja2: 2.11.3
2023-03-23 17:28:06,269:INFO:               scipy: 1.9.3
2023-03-23 17:28:06,269:INFO:              joblib: 1.2.0
2023-03-23 17:28:06,269:INFO:             sklearn: 1.0.2
2023-03-23 17:28:06,269:INFO:                pyod: 1.0.9
2023-03-23 17:28:06,269:INFO:            imblearn: 0.10.1
2023-03-23 17:28:06,269:INFO:   category_encoders: 2.6.0
2023-03-23 17:28:06,269:INFO:            lightgbm: 3.3.5
2023-03-23 17:28:06,269:INFO:               numba: 0.56.4
2023-03-23 17:28:06,269:INFO:            requests: 2.27.1
2023-03-23 17:28:06,269:INFO:          matplotlib: 3.6.2
2023-03-23 17:28:06,269:INFO:          scikitplot: 0.3.7
2023-03-23 17:28:06,269:INFO:         yellowbrick: 1.5
2023-03-23 17:28:06,269:INFO:              plotly: 5.9.0
2023-03-23 17:28:06,269:INFO:             kaleido: 0.2.1
2023-03-23 17:28:06,269:INFO:         statsmodels: 0.13.2
2023-03-23 17:28:06,269:INFO:              sktime: 0.16.1
2023-03-23 17:28:06,269:INFO:               tbats: 1.1.2
2023-03-23 17:28:06,269:INFO:            pmdarima: 2.0.3
2023-03-23 17:28:06,269:INFO:              psutil: 5.9.0
2023-03-23 17:28:06,269:INFO:PyCaret optional dependencies:
2023-03-23 17:28:06,269:INFO:                shap: Not installed
2023-03-23 17:28:06,270:INFO:           interpret: Not installed
2023-03-23 17:28:06,270:INFO:                umap: Not installed
2023-03-23 17:28:06,270:INFO:    pandas_profiling: Not installed
2023-03-23 17:28:06,270:INFO:  explainerdashboard: Not installed
2023-03-23 17:28:06,270:INFO:             autoviz: Not installed
2023-03-23 17:28:06,270:INFO:           fairlearn: Not installed
2023-03-23 17:28:06,270:INFO:             xgboost: 1.7.1
2023-03-23 17:28:06,270:INFO:            catboost: 1.0.6
2023-03-23 17:28:06,270:INFO:              kmodes: Not installed
2023-03-23 17:28:06,270:INFO:             mlxtend: Not installed
2023-03-23 17:28:06,270:INFO:       statsforecast: Not installed
2023-03-23 17:28:06,270:INFO:        tune_sklearn: Not installed
2023-03-23 17:28:06,270:INFO:                 ray: Not installed
2023-03-23 17:28:06,270:INFO:            hyperopt: Not installed
2023-03-23 17:28:06,270:INFO:              optuna: Not installed
2023-03-23 17:28:06,270:INFO:               skopt: Not installed
2023-03-23 17:28:06,270:INFO:              mlflow: Not installed
2023-03-23 17:28:06,270:INFO:              gradio: Not installed
2023-03-23 17:28:06,270:INFO:             fastapi: Not installed
2023-03-23 17:28:06,270:INFO:             uvicorn: Not installed
2023-03-23 17:28:06,270:INFO:              m2cgen: Not installed
2023-03-23 17:28:06,270:INFO:           evidently: Not installed
2023-03-23 17:28:06,270:INFO:               fugue: Not installed
2023-03-23 17:28:06,271:INFO:           streamlit: Not installed
2023-03-23 17:28:06,271:INFO:             prophet: Not installed
2023-03-23 17:28:06,271:INFO:None
2023-03-23 17:28:06,271:INFO:Set up data.
2023-03-23 17:28:06,280:INFO:Set up train/test split.
2023-03-23 17:28:06,280:INFO:Set up data.
2023-03-23 17:28:06,288:INFO:Set up index.
2023-03-23 17:28:06,288:INFO:Set up folding strategy.
2023-03-23 17:28:06,288:INFO:Assigning column types.
2023-03-23 17:28:06,293:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-23 17:28:06,330:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-23 17:28:06,335:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 17:28:06,340:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 17:28:06,410:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:28:06,455:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:28:06,456:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:28:06,459:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:28:06,459:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-23 17:28:06,464:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 17:28:06,469:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 17:28:06,537:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:28:06,583:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:28:06,583:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:28:06,586:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:28:06,586:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-23 17:28:06,591:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 17:28:06,596:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,222:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,268:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,269:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:28:07,272:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:28:07,277:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,282:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,354:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,400:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,400:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:28:07,403:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:28:07,403:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-23 17:28:07,413:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,482:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,528:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,529:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:28:07,531:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:28:07,541:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,609:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,654:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,655:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:28:07,658:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:28:07,658:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-23 17:28:07,735:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,780:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,781:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:28:07,784:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:28:07,861:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,906:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 17:28:07,907:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:28:07,909:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:28:07,910:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-23 17:28:07,988:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:28:08,035:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:28:08,038:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:28:08,116:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 17:28:08,162:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:28:08,165:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:28:08,165:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-23 17:28:08,291:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:28:08,294:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:28:08,417:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:28:08,420:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:28:08,423:INFO:Set up column name cleaning.
2023-03-23 17:28:08,440:INFO:Finished creating preprocessing pipeline.
2023-03-23 17:28:08,441:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\CHUKWUKA\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-03-23 17:28:08,441:INFO:Creating final display dataframe.
2023-03-23 17:28:08,515:INFO:Setup _display_container:                    Description              Value
0                   Session id                666
1                       Target  Item_Outlet_Sales
2                  Target type         Regression
3          Original data shape         (8523, 19)
4       Transformed data shape        (13637, 19)
5  Transformed train set shape         (8523, 19)
6   Transformed test set shape         (5114, 19)
7             Numeric features                 18
2023-03-23 17:28:08,650:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:28:08,653:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:28:08,777:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 17:28:08,780:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 17:28:08,781:INFO:setup() successfully completed in 2.87s...............
2023-03-23 17:28:17,924:INFO:Initializing compare_models()
2023-03-23 17:28:17,924:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, include=None, fold=None, round=4, cross_validation=True, sort=MSE, n_select=1, budget_time=10, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MSE', 'n_select': 1, 'budget_time': 10, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-23 17:28:17,924:INFO:Checking exceptions
2023-03-23 17:28:17,932:INFO:Preparing display monitor
2023-03-23 17:28:17,963:INFO:Time budget is 10 minutes
2023-03-23 17:28:17,963:INFO:Initializing Linear Regression
2023-03-23 17:28:17,963:INFO:Total runtime is 0.0 minutes
2023-03-23 17:28:17,967:INFO:SubProcess create_model() called ==================================
2023-03-23 17:28:17,968:INFO:Initializing create_model()
2023-03-23 17:28:17,968:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:28:17,968:INFO:Checking exceptions
2023-03-23 17:28:17,968:INFO:Importing libraries
2023-03-23 17:28:17,968:INFO:Copying training dataset
2023-03-23 17:28:17,995:INFO:Defining folds
2023-03-23 17:28:17,995:INFO:Declaring metric variables
2023-03-23 17:28:17,999:INFO:Importing untrained model
2023-03-23 17:28:18,004:INFO:Linear Regression Imported successfully
2023-03-23 17:28:18,015:INFO:Starting cross validation
2023-03-23 17:28:18,016:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:28:29,304:INFO:Calculating mean and std
2023-03-23 17:28:29,306:INFO:Creating metrics dataframe
2023-03-23 17:28:29,759:INFO:Uploading results into container
2023-03-23 17:28:29,759:INFO:Uploading model into container now
2023-03-23 17:28:29,760:INFO:_master_model_container: 1
2023-03-23 17:28:29,760:INFO:_display_container: 2
2023-03-23 17:28:29,760:INFO:LinearRegression(n_jobs=-1)
2023-03-23 17:28:29,760:INFO:create_model() successfully completed......................................
2023-03-23 17:28:30,090:INFO:SubProcess create_model() end ==================================
2023-03-23 17:28:30,091:INFO:Creating metrics dataframe
2023-03-23 17:28:30,099:INFO:Initializing Lasso Regression
2023-03-23 17:28:30,100:INFO:Total runtime is 0.20228668053944907 minutes
2023-03-23 17:28:30,105:INFO:SubProcess create_model() called ==================================
2023-03-23 17:28:30,105:INFO:Initializing create_model()
2023-03-23 17:28:30,105:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:28:30,105:INFO:Checking exceptions
2023-03-23 17:28:30,105:INFO:Importing libraries
2023-03-23 17:28:30,105:INFO:Copying training dataset
2023-03-23 17:28:30,125:INFO:Defining folds
2023-03-23 17:28:30,125:INFO:Declaring metric variables
2023-03-23 17:28:30,129:INFO:Importing untrained model
2023-03-23 17:28:30,133:INFO:Lasso Regression Imported successfully
2023-03-23 17:28:30,142:INFO:Starting cross validation
2023-03-23 17:28:30,143:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:28:33,212:INFO:Calculating mean and std
2023-03-23 17:28:33,213:INFO:Creating metrics dataframe
2023-03-23 17:28:33,657:INFO:Uploading results into container
2023-03-23 17:28:33,658:INFO:Uploading model into container now
2023-03-23 17:28:33,658:INFO:_master_model_container: 2
2023-03-23 17:28:33,658:INFO:_display_container: 2
2023-03-23 17:28:33,659:INFO:Lasso(random_state=666)
2023-03-23 17:28:33,659:INFO:create_model() successfully completed......................................
2023-03-23 17:28:33,961:INFO:SubProcess create_model() end ==================================
2023-03-23 17:28:34,065:INFO:Creating metrics dataframe
2023-03-23 17:28:34,075:INFO:Initializing Ridge Regression
2023-03-23 17:28:34,075:INFO:Total runtime is 0.26853753328323365 minutes
2023-03-23 17:28:34,079:INFO:SubProcess create_model() called ==================================
2023-03-23 17:28:34,079:INFO:Initializing create_model()
2023-03-23 17:28:34,080:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:28:34,080:INFO:Checking exceptions
2023-03-23 17:28:34,080:INFO:Importing libraries
2023-03-23 17:28:34,080:INFO:Copying training dataset
2023-03-23 17:28:34,100:INFO:Defining folds
2023-03-23 17:28:34,100:INFO:Declaring metric variables
2023-03-23 17:28:34,104:INFO:Importing untrained model
2023-03-23 17:28:34,108:INFO:Ridge Regression Imported successfully
2023-03-23 17:28:34,116:INFO:Starting cross validation
2023-03-23 17:28:34,116:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:28:34,162:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.19994e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:28:34,173:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17704e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:28:34,176:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17152e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:28:34,189:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.18403e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:28:34,201:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17938e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:28:34,219:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.2047e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:28:34,234:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.19028e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:28:34,254:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.20487e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:28:34,814:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17503e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:28:34,830:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.18623e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 17:28:37,192:INFO:Calculating mean and std
2023-03-23 17:28:37,194:INFO:Creating metrics dataframe
2023-03-23 17:28:37,637:INFO:Uploading results into container
2023-03-23 17:28:37,638:INFO:Uploading model into container now
2023-03-23 17:28:37,638:INFO:_master_model_container: 3
2023-03-23 17:28:37,638:INFO:_display_container: 2
2023-03-23 17:28:37,639:INFO:Ridge(random_state=666)
2023-03-23 17:28:37,639:INFO:create_model() successfully completed......................................
2023-03-23 17:28:37,941:INFO:SubProcess create_model() end ==================================
2023-03-23 17:28:37,941:INFO:Creating metrics dataframe
2023-03-23 17:28:37,951:INFO:Initializing Elastic Net
2023-03-23 17:28:37,951:INFO:Total runtime is 0.33313624064127606 minutes
2023-03-23 17:28:37,955:INFO:SubProcess create_model() called ==================================
2023-03-23 17:28:37,956:INFO:Initializing create_model()
2023-03-23 17:28:37,956:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:28:37,956:INFO:Checking exceptions
2023-03-23 17:28:37,956:INFO:Importing libraries
2023-03-23 17:28:37,956:INFO:Copying training dataset
2023-03-23 17:28:37,978:INFO:Defining folds
2023-03-23 17:28:37,978:INFO:Declaring metric variables
2023-03-23 17:28:37,984:INFO:Importing untrained model
2023-03-23 17:28:37,990:INFO:Elastic Net Imported successfully
2023-03-23 17:28:37,997:INFO:Starting cross validation
2023-03-23 17:28:37,998:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:28:41,094:INFO:Calculating mean and std
2023-03-23 17:28:41,095:INFO:Creating metrics dataframe
2023-03-23 17:28:41,547:INFO:Uploading results into container
2023-03-23 17:28:41,548:INFO:Uploading model into container now
2023-03-23 17:28:41,548:INFO:_master_model_container: 4
2023-03-23 17:28:41,549:INFO:_display_container: 2
2023-03-23 17:28:41,549:INFO:ElasticNet(random_state=666)
2023-03-23 17:28:41,549:INFO:create_model() successfully completed......................................
2023-03-23 17:28:41,852:INFO:SubProcess create_model() end ==================================
2023-03-23 17:28:41,852:INFO:Creating metrics dataframe
2023-03-23 17:28:41,862:INFO:Initializing Least Angle Regression
2023-03-23 17:28:41,862:INFO:Total runtime is 0.3983202576637268 minutes
2023-03-23 17:28:41,866:INFO:SubProcess create_model() called ==================================
2023-03-23 17:28:41,866:INFO:Initializing create_model()
2023-03-23 17:28:41,867:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:28:41,867:INFO:Checking exceptions
2023-03-23 17:28:41,867:INFO:Importing libraries
2023-03-23 17:28:41,867:INFO:Copying training dataset
2023-03-23 17:28:41,891:INFO:Defining folds
2023-03-23 17:28:41,891:INFO:Declaring metric variables
2023-03-23 17:28:41,895:INFO:Importing untrained model
2023-03-23 17:28:41,902:INFO:Least Angle Regression Imported successfully
2023-03-23 17:28:41,911:INFO:Starting cross validation
2023-03-23 17:28:41,911:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:28:41,955:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:41,962:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:41,975:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:41,982:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:42,004:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:42,016:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=5.274e-06, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:28:42,019:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.991e-06, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:28:42,022:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:42,039:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:42,040:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.908e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:28:42,044:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.479e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:28:42,045:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.082e-06, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:28:42,046:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.189e-06, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:28:42,046:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.051e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:28:42,048:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.194e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:28:42,051:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=5.760e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:28:42,056:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:42,068:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.362e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:28:42,069:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.152e-06, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:28:42,069:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.659e-06, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:28:42,598:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:42,610:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.008e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 17:28:42,626:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:44,986:INFO:Calculating mean and std
2023-03-23 17:28:44,988:INFO:Creating metrics dataframe
2023-03-23 17:28:45,434:INFO:Uploading results into container
2023-03-23 17:28:45,435:INFO:Uploading model into container now
2023-03-23 17:28:45,435:INFO:_master_model_container: 5
2023-03-23 17:28:45,435:INFO:_display_container: 2
2023-03-23 17:28:45,436:INFO:Lars(random_state=666)
2023-03-23 17:28:45,436:INFO:create_model() successfully completed......................................
2023-03-23 17:28:45,737:INFO:SubProcess create_model() end ==================================
2023-03-23 17:28:45,738:INFO:Creating metrics dataframe
2023-03-23 17:28:45,748:INFO:Initializing Lasso Least Angle Regression
2023-03-23 17:28:45,748:INFO:Total runtime is 0.4630858580271403 minutes
2023-03-23 17:28:45,753:INFO:SubProcess create_model() called ==================================
2023-03-23 17:28:45,753:INFO:Initializing create_model()
2023-03-23 17:28:45,754:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:28:45,754:INFO:Checking exceptions
2023-03-23 17:28:45,754:INFO:Importing libraries
2023-03-23 17:28:45,754:INFO:Copying training dataset
2023-03-23 17:28:45,775:INFO:Defining folds
2023-03-23 17:28:45,775:INFO:Declaring metric variables
2023-03-23 17:28:45,780:INFO:Importing untrained model
2023-03-23 17:28:45,787:INFO:Lasso Least Angle Regression Imported successfully
2023-03-23 17:28:45,795:INFO:Starting cross validation
2023-03-23 17:28:45,796:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:28:45,840:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:28:45,849:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:28:45,863:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:28:45,875:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:28:45,890:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:28:45,907:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:28:45,925:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:28:45,942:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:28:46,481:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:28:46,500:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 17:28:48,890:INFO:Calculating mean and std
2023-03-23 17:28:48,891:INFO:Creating metrics dataframe
2023-03-23 17:28:49,370:INFO:Uploading results into container
2023-03-23 17:28:49,371:INFO:Uploading model into container now
2023-03-23 17:28:49,371:INFO:_master_model_container: 6
2023-03-23 17:28:49,371:INFO:_display_container: 2
2023-03-23 17:28:49,372:INFO:LassoLars(random_state=666)
2023-03-23 17:28:49,372:INFO:create_model() successfully completed......................................
2023-03-23 17:28:49,690:INFO:SubProcess create_model() end ==================================
2023-03-23 17:28:49,690:INFO:Creating metrics dataframe
2023-03-23 17:28:49,701:INFO:Initializing Orthogonal Matching Pursuit
2023-03-23 17:28:49,701:INFO:Total runtime is 0.528969689210256 minutes
2023-03-23 17:28:49,705:INFO:SubProcess create_model() called ==================================
2023-03-23 17:28:49,705:INFO:Initializing create_model()
2023-03-23 17:28:49,706:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:28:49,706:INFO:Checking exceptions
2023-03-23 17:28:49,706:INFO:Importing libraries
2023-03-23 17:28:49,706:INFO:Copying training dataset
2023-03-23 17:28:49,725:INFO:Defining folds
2023-03-23 17:28:49,725:INFO:Declaring metric variables
2023-03-23 17:28:49,729:INFO:Importing untrained model
2023-03-23 17:28:49,732:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-23 17:28:49,740:INFO:Starting cross validation
2023-03-23 17:28:49,740:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:28:49,786:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:49,801:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:49,809:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:49,826:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:49,844:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:49,856:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:49,866:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:49,886:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:50,411:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:50,461:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 17:28:52,830:INFO:Calculating mean and std
2023-03-23 17:28:52,831:INFO:Creating metrics dataframe
2023-03-23 17:28:53,276:INFO:Uploading results into container
2023-03-23 17:28:53,277:INFO:Uploading model into container now
2023-03-23 17:28:53,277:INFO:_master_model_container: 7
2023-03-23 17:28:53,277:INFO:_display_container: 2
2023-03-23 17:28:53,278:INFO:OrthogonalMatchingPursuit()
2023-03-23 17:28:53,278:INFO:create_model() successfully completed......................................
2023-03-23 17:28:53,580:INFO:SubProcess create_model() end ==================================
2023-03-23 17:28:53,580:INFO:Creating metrics dataframe
2023-03-23 17:28:53,591:INFO:Initializing Bayesian Ridge
2023-03-23 17:28:53,591:INFO:Total runtime is 0.5937955617904663 minutes
2023-03-23 17:28:53,596:INFO:SubProcess create_model() called ==================================
2023-03-23 17:28:53,596:INFO:Initializing create_model()
2023-03-23 17:28:53,596:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:28:53,596:INFO:Checking exceptions
2023-03-23 17:28:53,597:INFO:Importing libraries
2023-03-23 17:28:53,597:INFO:Copying training dataset
2023-03-23 17:28:53,616:INFO:Defining folds
2023-03-23 17:28:53,617:INFO:Declaring metric variables
2023-03-23 17:28:53,621:INFO:Importing untrained model
2023-03-23 17:28:53,626:INFO:Bayesian Ridge Imported successfully
2023-03-23 17:28:53,637:INFO:Starting cross validation
2023-03-23 17:28:53,638:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:28:56,744:INFO:Calculating mean and std
2023-03-23 17:28:56,746:INFO:Creating metrics dataframe
2023-03-23 17:28:57,192:INFO:Uploading results into container
2023-03-23 17:28:57,193:INFO:Uploading model into container now
2023-03-23 17:28:57,194:INFO:_master_model_container: 8
2023-03-23 17:28:57,194:INFO:_display_container: 2
2023-03-23 17:28:57,194:INFO:BayesianRidge()
2023-03-23 17:28:57,194:INFO:create_model() successfully completed......................................
2023-03-23 17:28:57,495:INFO:SubProcess create_model() end ==================================
2023-03-23 17:28:57,496:INFO:Creating metrics dataframe
2023-03-23 17:28:57,507:INFO:Initializing Passive Aggressive Regressor
2023-03-23 17:28:57,507:INFO:Total runtime is 0.6590703805287679 minutes
2023-03-23 17:28:57,511:INFO:SubProcess create_model() called ==================================
2023-03-23 17:28:57,512:INFO:Initializing create_model()
2023-03-23 17:28:57,512:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:28:57,512:INFO:Checking exceptions
2023-03-23 17:28:57,513:INFO:Importing libraries
2023-03-23 17:28:57,513:INFO:Copying training dataset
2023-03-23 17:28:57,532:INFO:Defining folds
2023-03-23 17:28:57,532:INFO:Declaring metric variables
2023-03-23 17:28:57,537:INFO:Importing untrained model
2023-03-23 17:28:57,541:INFO:Passive Aggressive Regressor Imported successfully
2023-03-23 17:28:57,550:INFO:Starting cross validation
2023-03-23 17:28:57,550:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:29:00,661:INFO:Calculating mean and std
2023-03-23 17:29:00,662:INFO:Creating metrics dataframe
2023-03-23 17:29:01,109:INFO:Uploading results into container
2023-03-23 17:29:01,110:INFO:Uploading model into container now
2023-03-23 17:29:01,110:INFO:_master_model_container: 9
2023-03-23 17:29:01,110:INFO:_display_container: 2
2023-03-23 17:29:01,111:INFO:PassiveAggressiveRegressor(random_state=666)
2023-03-23 17:29:01,111:INFO:create_model() successfully completed......................................
2023-03-23 17:29:01,412:INFO:SubProcess create_model() end ==================================
2023-03-23 17:29:01,412:INFO:Creating metrics dataframe
2023-03-23 17:29:01,424:INFO:Initializing Huber Regressor
2023-03-23 17:29:01,424:INFO:Total runtime is 0.7243474245071411 minutes
2023-03-23 17:29:01,428:INFO:SubProcess create_model() called ==================================
2023-03-23 17:29:01,429:INFO:Initializing create_model()
2023-03-23 17:29:01,429:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:29:01,429:INFO:Checking exceptions
2023-03-23 17:29:01,429:INFO:Importing libraries
2023-03-23 17:29:01,429:INFO:Copying training dataset
2023-03-23 17:29:01,448:INFO:Defining folds
2023-03-23 17:29:01,448:INFO:Declaring metric variables
2023-03-23 17:29:01,453:INFO:Importing untrained model
2023-03-23 17:29:01,457:INFO:Huber Regressor Imported successfully
2023-03-23 17:29:01,466:INFO:Starting cross validation
2023-03-23 17:29:01,467:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:29:02,165:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:29:02,176:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:29:02,214:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:29:02,233:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:29:02,262:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:29:02,307:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:29:02,352:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:29:02,356:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:29:03,196:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:29:03,203:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-03-23 17:29:05,330:INFO:Calculating mean and std
2023-03-23 17:29:05,332:INFO:Creating metrics dataframe
2023-03-23 17:29:05,794:INFO:Uploading results into container
2023-03-23 17:29:05,795:INFO:Uploading model into container now
2023-03-23 17:29:05,795:INFO:_master_model_container: 10
2023-03-23 17:29:05,796:INFO:_display_container: 2
2023-03-23 17:29:05,797:INFO:HuberRegressor()
2023-03-23 17:29:05,797:INFO:create_model() successfully completed......................................
2023-03-23 17:29:06,100:INFO:SubProcess create_model() end ==================================
2023-03-23 17:29:06,101:INFO:Creating metrics dataframe
2023-03-23 17:29:06,113:INFO:Initializing K Neighbors Regressor
2023-03-23 17:29:06,113:INFO:Total runtime is 0.8024952928225199 minutes
2023-03-23 17:29:06,116:INFO:SubProcess create_model() called ==================================
2023-03-23 17:29:06,117:INFO:Initializing create_model()
2023-03-23 17:29:06,117:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:29:06,117:INFO:Checking exceptions
2023-03-23 17:29:06,117:INFO:Importing libraries
2023-03-23 17:29:06,117:INFO:Copying training dataset
2023-03-23 17:29:06,152:INFO:Defining folds
2023-03-23 17:29:06,153:INFO:Declaring metric variables
2023-03-23 17:29:06,159:INFO:Importing untrained model
2023-03-23 17:29:06,164:INFO:K Neighbors Regressor Imported successfully
2023-03-23 17:29:06,174:INFO:Starting cross validation
2023-03-23 17:29:06,176:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:29:10,063:INFO:Calculating mean and std
2023-03-23 17:29:10,064:INFO:Creating metrics dataframe
2023-03-23 17:29:10,517:INFO:Uploading results into container
2023-03-23 17:29:10,518:INFO:Uploading model into container now
2023-03-23 17:29:10,518:INFO:_master_model_container: 11
2023-03-23 17:29:10,518:INFO:_display_container: 2
2023-03-23 17:29:10,519:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-23 17:29:10,519:INFO:create_model() successfully completed......................................
2023-03-23 17:29:10,845:INFO:SubProcess create_model() end ==================================
2023-03-23 17:29:10,845:INFO:Creating metrics dataframe
2023-03-23 17:29:10,887:INFO:Initializing Decision Tree Regressor
2023-03-23 17:29:10,887:INFO:Total runtime is 0.8820603489875792 minutes
2023-03-23 17:29:10,891:INFO:SubProcess create_model() called ==================================
2023-03-23 17:29:10,891:INFO:Initializing create_model()
2023-03-23 17:29:10,891:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:29:10,892:INFO:Checking exceptions
2023-03-23 17:29:10,892:INFO:Importing libraries
2023-03-23 17:29:10,892:INFO:Copying training dataset
2023-03-23 17:29:10,912:INFO:Defining folds
2023-03-23 17:29:10,912:INFO:Declaring metric variables
2023-03-23 17:29:10,917:INFO:Importing untrained model
2023-03-23 17:29:10,926:INFO:Decision Tree Regressor Imported successfully
2023-03-23 17:29:10,938:INFO:Starting cross validation
2023-03-23 17:29:10,939:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:29:14,163:INFO:Calculating mean and std
2023-03-23 17:29:14,164:INFO:Creating metrics dataframe
2023-03-23 17:29:14,623:INFO:Uploading results into container
2023-03-23 17:29:14,624:INFO:Uploading model into container now
2023-03-23 17:29:14,625:INFO:_master_model_container: 12
2023-03-23 17:29:14,625:INFO:_display_container: 2
2023-03-23 17:29:14,625:INFO:DecisionTreeRegressor(random_state=666)
2023-03-23 17:29:14,625:INFO:create_model() successfully completed......................................
2023-03-23 17:29:14,939:INFO:SubProcess create_model() end ==================================
2023-03-23 17:29:14,939:INFO:Creating metrics dataframe
2023-03-23 17:29:14,953:INFO:Initializing Random Forest Regressor
2023-03-23 17:29:14,954:INFO:Total runtime is 0.9498430848121642 minutes
2023-03-23 17:29:14,959:INFO:SubProcess create_model() called ==================================
2023-03-23 17:29:14,960:INFO:Initializing create_model()
2023-03-23 17:29:14,960:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:29:14,960:INFO:Checking exceptions
2023-03-23 17:29:14,960:INFO:Importing libraries
2023-03-23 17:29:14,960:INFO:Copying training dataset
2023-03-23 17:29:14,982:INFO:Defining folds
2023-03-23 17:29:14,982:INFO:Declaring metric variables
2023-03-23 17:29:14,987:INFO:Importing untrained model
2023-03-23 17:29:14,995:INFO:Random Forest Regressor Imported successfully
2023-03-23 17:29:15,005:INFO:Starting cross validation
2023-03-23 17:29:15,007:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:29:20,436:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-03-23 17:29:25,400:INFO:Calculating mean and std
2023-03-23 17:29:25,402:INFO:Creating metrics dataframe
2023-03-23 17:29:25,798:INFO:Uploading results into container
2023-03-23 17:29:25,798:INFO:Uploading model into container now
2023-03-23 17:29:25,799:INFO:_master_model_container: 13
2023-03-23 17:29:25,799:INFO:_display_container: 2
2023-03-23 17:29:25,799:INFO:RandomForestRegressor(n_jobs=-1, random_state=666)
2023-03-23 17:29:25,799:INFO:create_model() successfully completed......................................
2023-03-23 17:29:26,120:INFO:SubProcess create_model() end ==================================
2023-03-23 17:29:26,120:INFO:Creating metrics dataframe
2023-03-23 17:29:26,134:INFO:Initializing Extra Trees Regressor
2023-03-23 17:29:26,134:INFO:Total runtime is 1.1361775517463684 minutes
2023-03-23 17:29:26,139:INFO:SubProcess create_model() called ==================================
2023-03-23 17:29:26,139:INFO:Initializing create_model()
2023-03-23 17:29:26,139:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:29:26,139:INFO:Checking exceptions
2023-03-23 17:29:26,140:INFO:Importing libraries
2023-03-23 17:29:26,140:INFO:Copying training dataset
2023-03-23 17:29:26,160:INFO:Defining folds
2023-03-23 17:29:26,160:INFO:Declaring metric variables
2023-03-23 17:29:26,164:INFO:Importing untrained model
2023-03-23 17:29:26,168:INFO:Extra Trees Regressor Imported successfully
2023-03-23 17:29:26,175:INFO:Starting cross validation
2023-03-23 17:29:26,176:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:29:35,995:INFO:Calculating mean and std
2023-03-23 17:29:35,997:INFO:Creating metrics dataframe
2023-03-23 17:29:36,856:INFO:Uploading results into container
2023-03-23 17:29:36,857:INFO:Uploading model into container now
2023-03-23 17:29:36,857:INFO:_master_model_container: 14
2023-03-23 17:29:36,857:INFO:_display_container: 2
2023-03-23 17:29:36,858:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=666)
2023-03-23 17:29:36,858:INFO:create_model() successfully completed......................................
2023-03-23 17:29:37,183:INFO:SubProcess create_model() end ==================================
2023-03-23 17:29:37,183:INFO:Creating metrics dataframe
2023-03-23 17:29:37,282:INFO:Initializing AdaBoost Regressor
2023-03-23 17:29:37,282:INFO:Total runtime is 1.3219815015792846 minutes
2023-03-23 17:29:37,285:INFO:SubProcess create_model() called ==================================
2023-03-23 17:29:37,286:INFO:Initializing create_model()
2023-03-23 17:29:37,286:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:29:37,286:INFO:Checking exceptions
2023-03-23 17:29:37,286:INFO:Importing libraries
2023-03-23 17:29:37,286:INFO:Copying training dataset
2023-03-23 17:29:37,305:INFO:Defining folds
2023-03-23 17:29:37,305:INFO:Declaring metric variables
2023-03-23 17:29:37,309:INFO:Importing untrained model
2023-03-23 17:29:37,312:INFO:AdaBoost Regressor Imported successfully
2023-03-23 17:29:37,319:INFO:Starting cross validation
2023-03-23 17:29:37,320:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:29:41,061:INFO:Calculating mean and std
2023-03-23 17:29:41,062:INFO:Creating metrics dataframe
2023-03-23 17:29:41,524:INFO:Uploading results into container
2023-03-23 17:29:41,525:INFO:Uploading model into container now
2023-03-23 17:29:41,525:INFO:_master_model_container: 15
2023-03-23 17:29:41,525:INFO:_display_container: 2
2023-03-23 17:29:41,526:INFO:AdaBoostRegressor(random_state=666)
2023-03-23 17:29:41,526:INFO:create_model() successfully completed......................................
2023-03-23 17:29:41,828:INFO:SubProcess create_model() end ==================================
2023-03-23 17:29:41,828:INFO:Creating metrics dataframe
2023-03-23 17:29:41,842:INFO:Initializing Gradient Boosting Regressor
2023-03-23 17:29:41,842:INFO:Total runtime is 1.3979876478513082 minutes
2023-03-23 17:29:41,846:INFO:SubProcess create_model() called ==================================
2023-03-23 17:29:41,847:INFO:Initializing create_model()
2023-03-23 17:29:41,847:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:29:41,847:INFO:Checking exceptions
2023-03-23 17:29:41,847:INFO:Importing libraries
2023-03-23 17:29:41,847:INFO:Copying training dataset
2023-03-23 17:29:41,867:INFO:Defining folds
2023-03-23 17:29:41,867:INFO:Declaring metric variables
2023-03-23 17:29:41,871:INFO:Importing untrained model
2023-03-23 17:29:41,876:INFO:Gradient Boosting Regressor Imported successfully
2023-03-23 17:29:41,883:INFO:Starting cross validation
2023-03-23 17:29:41,884:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:29:47,447:INFO:Calculating mean and std
2023-03-23 17:29:47,448:INFO:Creating metrics dataframe
2023-03-23 17:29:47,917:INFO:Uploading results into container
2023-03-23 17:29:47,918:INFO:Uploading model into container now
2023-03-23 17:29:47,918:INFO:_master_model_container: 16
2023-03-23 17:29:47,919:INFO:_display_container: 2
2023-03-23 17:29:47,919:INFO:GradientBoostingRegressor(random_state=666)
2023-03-23 17:29:47,919:INFO:create_model() successfully completed......................................
2023-03-23 17:29:48,223:INFO:SubProcess create_model() end ==================================
2023-03-23 17:29:48,224:INFO:Creating metrics dataframe
2023-03-23 17:29:48,238:INFO:Initializing Extreme Gradient Boosting
2023-03-23 17:29:48,238:INFO:Total runtime is 1.5045820991198222 minutes
2023-03-23 17:29:48,647:INFO:SubProcess create_model() called ==================================
2023-03-23 17:29:48,648:INFO:Initializing create_model()
2023-03-23 17:29:48,648:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:29:48,648:INFO:Checking exceptions
2023-03-23 17:29:48,648:INFO:Importing libraries
2023-03-23 17:29:48,648:INFO:Copying training dataset
2023-03-23 17:29:48,668:INFO:Defining folds
2023-03-23 17:29:48,668:INFO:Declaring metric variables
2023-03-23 17:29:48,671:INFO:Importing untrained model
2023-03-23 17:29:48,675:INFO:Extreme Gradient Boosting Imported successfully
2023-03-23 17:29:48,682:INFO:Starting cross validation
2023-03-23 17:29:48,683:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:30:00,332:INFO:Calculating mean and std
2023-03-23 17:30:00,334:INFO:Creating metrics dataframe
2023-03-23 17:30:00,808:INFO:Uploading results into container
2023-03-23 17:30:00,808:INFO:Uploading model into container now
2023-03-23 17:30:00,809:INFO:_master_model_container: 17
2023-03-23 17:30:00,809:INFO:_display_container: 2
2023-03-23 17:30:00,810:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=666, ...)
2023-03-23 17:30:00,810:INFO:create_model() successfully completed......................................
2023-03-23 17:30:01,115:INFO:SubProcess create_model() end ==================================
2023-03-23 17:30:01,115:INFO:Creating metrics dataframe
2023-03-23 17:30:01,128:INFO:Initializing Light Gradient Boosting Machine
2023-03-23 17:30:01,129:INFO:Total runtime is 1.7194311896959942 minutes
2023-03-23 17:30:01,132:INFO:SubProcess create_model() called ==================================
2023-03-23 17:30:01,133:INFO:Initializing create_model()
2023-03-23 17:30:01,133:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:30:01,133:INFO:Checking exceptions
2023-03-23 17:30:01,133:INFO:Importing libraries
2023-03-23 17:30:01,133:INFO:Copying training dataset
2023-03-23 17:30:01,152:INFO:Defining folds
2023-03-23 17:30:01,152:INFO:Declaring metric variables
2023-03-23 17:30:01,156:INFO:Importing untrained model
2023-03-23 17:30:01,160:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-23 17:30:01,167:INFO:Starting cross validation
2023-03-23 17:30:01,168:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:30:08,516:INFO:Calculating mean and std
2023-03-23 17:30:08,517:INFO:Creating metrics dataframe
2023-03-23 17:30:09,012:INFO:Uploading results into container
2023-03-23 17:30:09,012:INFO:Uploading model into container now
2023-03-23 17:30:09,013:INFO:_master_model_container: 18
2023-03-23 17:30:09,013:INFO:_display_container: 2
2023-03-23 17:30:09,013:INFO:LGBMRegressor(random_state=666)
2023-03-23 17:30:09,014:INFO:create_model() successfully completed......................................
2023-03-23 17:30:09,315:INFO:SubProcess create_model() end ==================================
2023-03-23 17:30:09,315:INFO:Creating metrics dataframe
2023-03-23 17:30:09,330:INFO:Initializing CatBoost Regressor
2023-03-23 17:30:09,330:INFO:Total runtime is 1.8561230500539145 minutes
2023-03-23 17:30:09,333:INFO:SubProcess create_model() called ==================================
2023-03-23 17:30:09,334:INFO:Initializing create_model()
2023-03-23 17:30:09,334:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:30:09,334:INFO:Checking exceptions
2023-03-23 17:30:09,334:INFO:Importing libraries
2023-03-23 17:30:09,334:INFO:Copying training dataset
2023-03-23 17:30:09,353:INFO:Defining folds
2023-03-23 17:30:09,353:INFO:Declaring metric variables
2023-03-23 17:30:09,357:INFO:Importing untrained model
2023-03-23 17:30:09,389:INFO:CatBoost Regressor Imported successfully
2023-03-23 17:30:09,396:INFO:Starting cross validation
2023-03-23 17:30:09,397:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:30:31,025:INFO:Calculating mean and std
2023-03-23 17:30:31,026:INFO:Creating metrics dataframe
2023-03-23 17:30:31,568:INFO:Uploading results into container
2023-03-23 17:30:31,569:INFO:Uploading model into container now
2023-03-23 17:30:31,569:INFO:_master_model_container: 19
2023-03-23 17:30:31,569:INFO:_display_container: 2
2023-03-23 17:30:31,569:INFO:<catboost.core.CatBoostRegressor object at 0x0000016C944E3640>
2023-03-23 17:30:31,569:INFO:create_model() successfully completed......................................
2023-03-23 17:30:31,885:INFO:SubProcess create_model() end ==================================
2023-03-23 17:30:31,885:INFO:Creating metrics dataframe
2023-03-23 17:30:31,899:INFO:Initializing Dummy Regressor
2023-03-23 17:30:31,899:INFO:Total runtime is 2.2322758316993716 minutes
2023-03-23 17:30:31,904:INFO:SubProcess create_model() called ==================================
2023-03-23 17:30:31,904:INFO:Initializing create_model()
2023-03-23 17:30:31,904:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000016C950C1D00>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:30:31,904:INFO:Checking exceptions
2023-03-23 17:30:31,904:INFO:Importing libraries
2023-03-23 17:30:31,904:INFO:Copying training dataset
2023-03-23 17:30:31,924:INFO:Defining folds
2023-03-23 17:30:31,924:INFO:Declaring metric variables
2023-03-23 17:30:31,928:INFO:Importing untrained model
2023-03-23 17:30:31,934:INFO:Dummy Regressor Imported successfully
2023-03-23 17:30:31,945:INFO:Starting cross validation
2023-03-23 17:30:31,946:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 17:30:35,823:INFO:Calculating mean and std
2023-03-23 17:30:35,824:INFO:Creating metrics dataframe
2023-03-23 17:30:36,313:INFO:Uploading results into container
2023-03-23 17:30:36,313:INFO:Uploading model into container now
2023-03-23 17:30:36,314:INFO:_master_model_container: 20
2023-03-23 17:30:36,314:INFO:_display_container: 2
2023-03-23 17:30:36,314:INFO:DummyRegressor()
2023-03-23 17:30:36,315:INFO:create_model() successfully completed......................................
2023-03-23 17:30:36,619:INFO:SubProcess create_model() end ==================================
2023-03-23 17:30:36,619:INFO:Creating metrics dataframe
2023-03-23 17:30:36,647:INFO:Initializing create_model()
2023-03-23 17:30:36,647:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=GradientBoostingRegressor(random_state=666), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-23 17:30:36,647:INFO:Checking exceptions
2023-03-23 17:30:36,649:INFO:Importing libraries
2023-03-23 17:30:36,649:INFO:Copying training dataset
2023-03-23 17:30:36,670:INFO:Defining folds
2023-03-23 17:30:36,670:INFO:Declaring metric variables
2023-03-23 17:30:36,670:INFO:Importing untrained model
2023-03-23 17:30:36,671:INFO:Declaring custom model
2023-03-23 17:30:36,672:INFO:Gradient Boosting Regressor Imported successfully
2023-03-23 17:30:36,673:INFO:Cross validation set to False
2023-03-23 17:30:36,673:INFO:Fitting Model
2023-03-23 17:30:38,557:INFO:GradientBoostingRegressor(random_state=666)
2023-03-23 17:30:38,557:INFO:create_model() successfully completed......................................
2023-03-23 17:30:38,904:INFO:_master_model_container: 20
2023-03-23 17:30:38,905:INFO:_display_container: 2
2023-03-23 17:30:38,906:INFO:GradientBoostingRegressor(random_state=666)
2023-03-23 17:30:38,906:INFO:compare_models() successfully completed......................................
2023-03-23 17:31:27,071:INFO:Initializing predict_model()
2023-03-23 17:31:27,071:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000016C94FA4910>, estimator=GradientBoostingRegressor(random_state=666), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000016C9643B040>)
2023-03-23 17:31:27,072:INFO:Checking exceptions
2023-03-23 17:31:27,072:INFO:Preloading libraries
2023-03-23 17:31:27,074:INFO:Set up data.
2023-03-23 17:31:27,082:INFO:Set up index.
2023-03-23 18:04:47,877:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_b24d0768ce744357a33f5704c9cb75b8
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,877:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_7619f9a6ee534fefbd4e6251debf27e5_916de616ae004290a36d393fbc6f26d5
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,878:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_3eb2147895ad485dabb87d7ea7229990
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,879:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_97300efb728a4453a92cafc29e29229a_ee51fea256694b47bb1bb78866cab5c0
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,879:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_49e79ee244084f938e9cf0d9c7a3a2ee
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,879:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_da9da234c1d747b4974425dd3a849984_6485de9e180f4708813e36a2c7478dde
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,880:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_752fe9bf31034962ac27dc0c83789a39
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,880:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_d67392675dc747adad5787cd986c9aa5_3189f46ac0fe4da0a20f3d3369fcc351
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,880:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_ee039cfa5d45474396281c25dd2aed97
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,880:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_19e489d0e85e47cfa9ecd75f839a92c7_30272fe8ee0a4bb5b3300352e229481f
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,880:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_fa4895bcc0f34658a8b34702ae0338f1
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,880:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_27677c4c802f4d2da442d945fd1b3689_2531fa638eb4487ca3b0b850e8965a0b
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,880:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_df28faa3c145435b9b2f7800dd8cb1d3
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,881:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_711c1d3d9eef4b458e31d4337d9a7dbe_5f71ae197bd547c08e027c2f278e5418
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,881:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_754ec14a8c9e4b50986248487f98607a
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,881:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_1fd1484cf12f4540b9289e052b918777_3af788e3ce9b4e3ba08a8626a811b500
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,881:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_875979236e614b648e758cd365ca50c7
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,881:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_da9b83daedd34e6f8da6f2c055c4ebde_ac021b36d4af406791bc4dde2fc1b740
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,881:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_52de92e8a3564713a47c7a551b8b6f0a
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,881:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_fb2e45584bcd430a9adcefe11d187cf5_b09734d5f1ea4f0294fa892fd2492146
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,881:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_de3b1699aa9142608d1ba4af6e31e3e2
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,882:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_c1d12977e4ec454582d89b5b1c89c677_6c766a0cfc07419880b1ff215832a304
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,882:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_560f245a0d394a12b40658d5b6ceebeb
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,882:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_a986822933f94dba938cbdf4e1fd3823_ae2423a376c344d6a7130becaac488a7
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,882:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_426250d4e92d48c5894c29ac814e2588
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,882:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_0b6f2f2489cf4cd2b07245850bd819fb_8bdd7a16872e4691a3284be5e3faeeb1
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,882:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_0ac48a669f464aab81145e28a0e05ea7
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,882:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_819e75c9f06e454a94db265f25878a4f_3f77382e80654eb8acba129fae102d8a
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,883:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_5e04629bf7014701922abe1a3db3eef9
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,883:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_4177b0d9a84344bb9cc75c5aec4716e3_deae0ed9099d498da5f1ad2754c83910
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,883:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_473b7d44b57749a2944cf9eae8243da0
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,883:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_a3e3f51a597a4298900df30aa8d7a10d_16d3a93f96ab476ab86d213381394457
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,883:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_454c26ab3348478cbc2c22d748a159bd
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,883:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_39c896cb3d9749f9bf829f23144b4aea_edb3eef369ed42849377b1469284516a
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,883:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_d297eaacfa2c46fdadbaf545c35234c1
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,883:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_90118db5953c409faaaae407241593e6_d7aeb41630614bc39cd2dcec19b491c8
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,884:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_283e1734b0824d07b8a86902ca1b86d0
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,884:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_f05f458fe8a44ed4b8eac6da87da7cb5_8bedde485319407d91bd72dd28f57e4a
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,884:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_f24b24edc48244cd95d8124c64e1c0ca
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,884:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_63889d965d6d458b9902d6391b636ef0_8b38d2e31faa4c7a888886e3fc03c92f
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,884:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_68605e7e7200416783b2836167d2a568
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,884:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_5d4b9379da1e486ea5d86b25389a17a3_a2b7c333b54c4cb7a444c6f3d54f8707
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,884:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_cac56feba2cb4060be81a5e19f89e72d
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,884:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_36e89e059073481db503895b9eb4382b_fd524a4643d044719c4f30e25eb82485
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,885:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_944c8843fe204ef9850aa56303ef24bb
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,885:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_4ded8bab3b604bb0b32e90d453623798_eb4eb0532f3f4664ace9211340cbf695
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,885:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_346059f797d941a7afb910d591dd82d0
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,885:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_13c99b0733974f20aaccd0d15a4cebe0_31e50465c2f74a2bb33686b3cb4fe3a1
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,885:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_5f869ad4ae9a4b2cbb35ece670903429
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,885:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_fc4efb91d103407f843f30c2a0be2a52_8f1c2100aaab4276b2d93bc00e1bb2ea
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,885:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_fed681e256044e9eacb17829855c4de7
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,885:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_96f9ed9c7348402d8031f2f13951b17f_c60ad06bde7c4071b0e713d3a99ea76b
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,886:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_440c003ee3ad44c09d4847d7cb18aea9
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,886:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_e8d33b2e88c64d1bac4422624e920a34_0238fd5f6c0949518229abb872caa48b
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,886:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_407569b7ff4e4586b87f0651e65a9818
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,886:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_ab07515fd01f4c0f959e61cedb83b083_406e82d8e2234781969444f45ecf97b0
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,886:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_d3a1cf9a5a0443d68f2f694af34c8237
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,886:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_9bc10b28c1114ed7b004a0db3a0aa14a_ad6d488c961d4661967b2d79549122d2
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,886:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_5337481c5a314012998c067dfe545f0e
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,886:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_efdcbdf0ab8941b3a181f6ceb265ea91_4efa22b53ef1402ab2af49bdf7f4f6af
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,887:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_a143367d38a341f4bc332974aa7a704d
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,887:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_a2147ba7df554628acd9cc28cee064c7_3544697151a148be8e50b493c99f36bc
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,887:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_1b7127fcb83a4cb08477dff09bffe63e
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,887:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_79d83fd7470a46d2bf11f257fdefba17_a2d3b2e751c54a56b25f42df141cb063
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,887:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_4cb1d8fbfa984f8286c54120dabb9a20
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,887:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_1f2fde61c323455daf6844e0f48639e7_fad2c56f2c88465fbbc2ce6162116179
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,887:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_df3cd4c31328489eb857359801b63884
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,888:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_62f0082d77a74595a14c427f043885a3_e86db9c175b0485c8cd8bf6d17e80a6a
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,888:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_22d4ebd514fa497ead3894129cd38f9c
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,888:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_4c5222e12cee48b9b299720fa1ab9032_edcfaf5efdf34baeae660b38b2775ae8
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,888:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_010740f97b3645ada5f53d7d5e959394
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,888:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_b2929e49d9474440b1951176c573aefe_4327f907d0d14045b6e75676668ab722
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,888:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_7383bd2bae084a94893c7cf2dbebdaac
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,888:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_7830cda2bd36423d9aeec06f28a077dc_3d83b0c119f54f1fa590dcc715137b4f
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,888:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_f817e77e952b4f4586f657f6d3ff1fc5
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,889:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_971cc3510f7a4b4ca1edfc7255fbc927_38d59025e95c4eb7b30438c799f05691
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,889:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_485d449fa6544b5485e483a65ce75cb3
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,889:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_0428e23b775a4323a37911707ff45545_45b0b8139a6c47cfa4829c86716bed75
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,889:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_16a2b7f88f164cdbba7046b4a2c1b414
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:04:47,889:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_8836_8593c307543043cea5fb348a0a1e29b7_fd96767ab1394590b1022ac3ed6742ad
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:08:09,730:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-23 18:08:09,730:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-23 18:08:09,730:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-23 18:08:09,730:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-23 18:08:11,024:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-23 18:08:11,747:INFO:PyCaret RegressionExperiment
2023-03-23 18:08:11,747:INFO:Logging name: reg-default-name
2023-03-23 18:08:11,748:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-23 18:08:11,748:INFO:version 3.0.0
2023-03-23 18:08:11,748:INFO:Initializing setup()
2023-03-23 18:08:11,748:INFO:self.USI: bf2f
2023-03-23 18:08:11,748:INFO:self._variable_keys: {'X', '_ml_usecase', 'n_jobs_param', '_available_plots', 'html_param', 'memory', 'transform_target_param', 'pipeline', 'exp_id', 'y_test', 'USI', 'y', 'X_test', 'target_param', 'seed', 'fold_shuffle_param', 'log_plots_param', 'idx', 'gpu_n_jobs_param', 'logging_param', 'fold_groups_param', 'X_train', 'y_train', 'data', 'gpu_param', 'exp_name_log', 'fold_generator'}
2023-03-23 18:08:11,748:INFO:Checking environment
2023-03-23 18:08:11,748:INFO:python_version: 3.9.12
2023-03-23 18:08:11,748:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-23 18:08:11,748:INFO:machine: AMD64
2023-03-23 18:08:11,748:INFO:platform: Windows-10-10.0.19045-SP0
2023-03-23 18:08:11,748:INFO:Memory: svmem(total=8496553984, available=1381769216, percent=83.7, used=7114784768, free=1381769216)
2023-03-23 18:08:11,748:INFO:Physical Core: 4
2023-03-23 18:08:11,748:INFO:Logical Core: 8
2023-03-23 18:08:11,748:INFO:Checking libraries
2023-03-23 18:08:11,748:INFO:System:
2023-03-23 18:08:11,748:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-23 18:08:11,748:INFO:executable: C:\Users\CHUKWUKA\anaconda3\python.exe
2023-03-23 18:08:11,748:INFO:   machine: Windows-10-10.0.19045-SP0
2023-03-23 18:08:11,748:INFO:PyCaret required dependencies:
2023-03-23 18:08:11,749:INFO:                 pip: 22.3.1
2023-03-23 18:08:11,749:INFO:          setuptools: 65.5.0
2023-03-23 18:08:11,749:INFO:             pycaret: 3.0.0
2023-03-23 18:08:11,749:INFO:             IPython: 8.7.0
2023-03-23 18:08:11,749:INFO:          ipywidgets: 7.6.5
2023-03-23 18:08:11,749:INFO:                tqdm: 4.64.1
2023-03-23 18:08:11,749:INFO:               numpy: 1.21.5
2023-03-23 18:08:11,749:INFO:              pandas: 1.4.4
2023-03-23 18:08:11,749:INFO:              jinja2: 2.11.3
2023-03-23 18:08:11,749:INFO:               scipy: 1.9.3
2023-03-23 18:08:11,749:INFO:              joblib: 1.2.0
2023-03-23 18:08:11,749:INFO:             sklearn: 1.0.2
2023-03-23 18:08:11,749:INFO:                pyod: 1.0.9
2023-03-23 18:08:11,749:INFO:            imblearn: 0.10.1
2023-03-23 18:08:11,749:INFO:   category_encoders: 2.6.0
2023-03-23 18:08:11,749:INFO:            lightgbm: 3.3.5
2023-03-23 18:08:11,749:INFO:               numba: 0.56.4
2023-03-23 18:08:11,749:INFO:            requests: 2.27.1
2023-03-23 18:08:11,749:INFO:          matplotlib: 3.6.2
2023-03-23 18:08:11,749:INFO:          scikitplot: 0.3.7
2023-03-23 18:08:11,749:INFO:         yellowbrick: 1.5
2023-03-23 18:08:11,750:INFO:              plotly: 5.9.0
2023-03-23 18:08:11,750:INFO:             kaleido: 0.2.1
2023-03-23 18:08:11,750:INFO:         statsmodels: 0.13.2
2023-03-23 18:08:11,750:INFO:              sktime: 0.16.1
2023-03-23 18:08:11,750:INFO:               tbats: 1.1.2
2023-03-23 18:08:11,750:INFO:            pmdarima: 2.0.3
2023-03-23 18:08:11,750:INFO:              psutil: 5.9.0
2023-03-23 18:08:11,750:INFO:PyCaret optional dependencies:
2023-03-23 18:08:11,763:INFO:                shap: Not installed
2023-03-23 18:08:11,763:INFO:           interpret: Not installed
2023-03-23 18:08:11,763:INFO:                umap: Not installed
2023-03-23 18:08:11,763:INFO:    pandas_profiling: Not installed
2023-03-23 18:08:11,763:INFO:  explainerdashboard: Not installed
2023-03-23 18:08:11,763:INFO:             autoviz: Not installed
2023-03-23 18:08:11,763:INFO:           fairlearn: Not installed
2023-03-23 18:08:11,763:INFO:             xgboost: 1.7.1
2023-03-23 18:08:11,763:INFO:            catboost: 1.0.6
2023-03-23 18:08:11,763:INFO:              kmodes: Not installed
2023-03-23 18:08:11,763:INFO:             mlxtend: Not installed
2023-03-23 18:08:11,763:INFO:       statsforecast: Not installed
2023-03-23 18:08:11,763:INFO:        tune_sklearn: Not installed
2023-03-23 18:08:11,763:INFO:                 ray: Not installed
2023-03-23 18:08:11,764:INFO:            hyperopt: Not installed
2023-03-23 18:08:11,764:INFO:              optuna: Not installed
2023-03-23 18:08:11,764:INFO:               skopt: Not installed
2023-03-23 18:08:11,764:INFO:              mlflow: Not installed
2023-03-23 18:08:11,764:INFO:              gradio: Not installed
2023-03-23 18:08:11,764:INFO:             fastapi: Not installed
2023-03-23 18:08:11,764:INFO:             uvicorn: Not installed
2023-03-23 18:08:11,764:INFO:              m2cgen: Not installed
2023-03-23 18:08:11,764:INFO:           evidently: Not installed
2023-03-23 18:08:11,764:INFO:               fugue: Not installed
2023-03-23 18:08:11,764:INFO:           streamlit: Not installed
2023-03-23 18:08:11,764:INFO:             prophet: Not installed
2023-03-23 18:08:11,764:INFO:None
2023-03-23 18:08:11,764:INFO:Set up data.
2023-03-23 18:08:11,773:INFO:Set up train/test split.
2023-03-23 18:08:11,773:INFO:Set up data.
2023-03-23 18:08:11,781:INFO:Set up index.
2023-03-23 18:08:11,781:INFO:Set up folding strategy.
2023-03-23 18:08:11,781:INFO:Assigning column types.
2023-03-23 18:08:11,786:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-23 18:08:11,786:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-23 18:08:11,791:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 18:08:11,796:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 18:08:11,865:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:08:11,911:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:08:11,912:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:08:12,169:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:08:12,198:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,203:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,207:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,276:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,322:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,322:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:08:12,325:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:08:12,325:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-23 18:08:12,330:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,335:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,403:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,448:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,449:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:08:12,452:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:08:12,457:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,461:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,529:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,575:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,575:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:08:12,578:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:08:12,578:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-23 18:08:12,588:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,660:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,708:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,708:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:08:12,711:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:08:12,721:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,788:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,834:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,834:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:08:12,837:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:08:12,837:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-23 18:08:12,914:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,960:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:08:12,960:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:08:12,963:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:08:13,040:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:08:13,085:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:08:13,086:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:08:13,089:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:08:13,089:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-23 18:08:13,166:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:08:13,213:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:08:13,216:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:08:13,294:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:08:13,341:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:08:13,344:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:08:13,345:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-23 18:08:13,470:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:08:13,473:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:08:13,600:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:08:13,602:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:08:13,607:INFO:Set up column name cleaning.
2023-03-23 18:08:13,623:INFO:Finished creating preprocessing pipeline.
2023-03-23 18:08:13,624:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\CHUKWUKA\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-03-23 18:08:13,624:INFO:Creating final display dataframe.
2023-03-23 18:08:13,696:INFO:Setup _display_container:                    Description              Value
0                   Session id                666
1                       Target  Item_Outlet_Sales
2                  Target type         Regression
3          Original data shape         (8523, 19)
4       Transformed data shape        (13637, 19)
5  Transformed train set shape         (8523, 19)
6   Transformed test set shape         (5114, 19)
7             Numeric features                 18
2023-03-23 18:08:13,834:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:08:13,837:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:08:13,964:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:08:13,967:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:08:13,968:INFO:setup() successfully completed in 2.61s...............
2023-03-23 18:08:13,988:INFO:Initializing compare_models()
2023-03-23 18:08:13,988:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, include=None, fold=None, round=4, cross_validation=True, sort=MSE, n_select=1, budget_time=10, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MSE', 'n_select': 1, 'budget_time': 10, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-23 18:08:13,988:INFO:Checking exceptions
2023-03-23 18:08:13,997:INFO:Preparing display monitor
2023-03-23 18:08:14,030:INFO:Time budget is 10 minutes
2023-03-23 18:08:14,031:INFO:Initializing Linear Regression
2023-03-23 18:08:14,031:INFO:Total runtime is 1.668532689412435e-05 minutes
2023-03-23 18:08:14,038:INFO:SubProcess create_model() called ==================================
2023-03-23 18:08:14,038:INFO:Initializing create_model()
2023-03-23 18:08:14,038:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:08:14,038:INFO:Checking exceptions
2023-03-23 18:08:14,039:INFO:Importing libraries
2023-03-23 18:08:14,039:INFO:Copying training dataset
2023-03-23 18:08:14,058:INFO:Defining folds
2023-03-23 18:08:14,059:INFO:Declaring metric variables
2023-03-23 18:08:14,062:INFO:Importing untrained model
2023-03-23 18:08:14,067:INFO:Linear Regression Imported successfully
2023-03-23 18:08:14,077:INFO:Starting cross validation
2023-03-23 18:08:14,086:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:08:24,379:INFO:Calculating mean and std
2023-03-23 18:08:24,381:INFO:Creating metrics dataframe
2023-03-23 18:08:24,870:INFO:Uploading results into container
2023-03-23 18:08:24,871:INFO:Uploading model into container now
2023-03-23 18:08:24,871:INFO:_master_model_container: 1
2023-03-23 18:08:24,871:INFO:_display_container: 2
2023-03-23 18:08:24,872:INFO:LinearRegression(n_jobs=-1)
2023-03-23 18:08:24,872:INFO:create_model() successfully completed......................................
2023-03-23 18:08:24,994:INFO:SubProcess create_model() end ==================================
2023-03-23 18:08:24,994:INFO:Creating metrics dataframe
2023-03-23 18:08:25,004:INFO:Initializing Lasso Regression
2023-03-23 18:08:25,005:INFO:Total runtime is 0.18290485541025797 minutes
2023-03-23 18:08:25,009:INFO:SubProcess create_model() called ==================================
2023-03-23 18:08:25,009:INFO:Initializing create_model()
2023-03-23 18:08:25,009:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:08:25,010:INFO:Checking exceptions
2023-03-23 18:08:25,010:INFO:Importing libraries
2023-03-23 18:08:25,010:INFO:Copying training dataset
2023-03-23 18:08:25,028:INFO:Defining folds
2023-03-23 18:08:25,028:INFO:Declaring metric variables
2023-03-23 18:08:25,032:INFO:Importing untrained model
2023-03-23 18:08:25,036:INFO:Lasso Regression Imported successfully
2023-03-23 18:08:25,046:INFO:Starting cross validation
2023-03-23 18:08:25,047:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:08:28,683:INFO:Calculating mean and std
2023-03-23 18:08:28,684:INFO:Creating metrics dataframe
2023-03-23 18:08:29,237:INFO:Uploading results into container
2023-03-23 18:08:29,237:INFO:Uploading model into container now
2023-03-23 18:08:29,238:INFO:_master_model_container: 2
2023-03-23 18:08:29,238:INFO:_display_container: 2
2023-03-23 18:08:29,238:INFO:Lasso(random_state=666)
2023-03-23 18:08:29,238:INFO:create_model() successfully completed......................................
2023-03-23 18:08:29,363:INFO:SubProcess create_model() end ==================================
2023-03-23 18:08:29,363:INFO:Creating metrics dataframe
2023-03-23 18:08:29,373:INFO:Initializing Ridge Regression
2023-03-23 18:08:29,373:INFO:Total runtime is 0.2557060440381368 minutes
2023-03-23 18:08:29,378:INFO:SubProcess create_model() called ==================================
2023-03-23 18:08:29,378:INFO:Initializing create_model()
2023-03-23 18:08:29,379:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:08:29,379:INFO:Checking exceptions
2023-03-23 18:08:29,379:INFO:Importing libraries
2023-03-23 18:08:29,379:INFO:Copying training dataset
2023-03-23 18:08:29,397:INFO:Defining folds
2023-03-23 18:08:29,398:INFO:Declaring metric variables
2023-03-23 18:08:29,402:INFO:Importing untrained model
2023-03-23 18:08:29,407:INFO:Ridge Regression Imported successfully
2023-03-23 18:08:29,419:INFO:Starting cross validation
2023-03-23 18:08:29,420:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:08:29,471:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.19994e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:08:29,485:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17704e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:08:29,499:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17152e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:08:29,506:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.18403e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:08:29,517:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17938e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:08:29,532:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.2047e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:08:29,543:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.19028e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:08:29,557:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.20487e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:08:30,246:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17503e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:08:30,263:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.18623e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:08:32,974:INFO:Calculating mean and std
2023-03-23 18:08:32,975:INFO:Creating metrics dataframe
2023-03-23 18:08:33,467:INFO:Uploading results into container
2023-03-23 18:08:33,468:INFO:Uploading model into container now
2023-03-23 18:08:33,468:INFO:_master_model_container: 3
2023-03-23 18:08:33,468:INFO:_display_container: 2
2023-03-23 18:08:33,468:INFO:Ridge(random_state=666)
2023-03-23 18:08:33,469:INFO:create_model() successfully completed......................................
2023-03-23 18:08:33,591:INFO:SubProcess create_model() end ==================================
2023-03-23 18:08:33,591:INFO:Creating metrics dataframe
2023-03-23 18:08:33,601:INFO:Initializing Elastic Net
2023-03-23 18:08:33,601:INFO:Total runtime is 0.3261847496032715 minutes
2023-03-23 18:08:33,607:INFO:SubProcess create_model() called ==================================
2023-03-23 18:08:33,608:INFO:Initializing create_model()
2023-03-23 18:08:33,608:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:08:33,608:INFO:Checking exceptions
2023-03-23 18:08:33,608:INFO:Importing libraries
2023-03-23 18:08:33,608:INFO:Copying training dataset
2023-03-23 18:08:33,626:INFO:Defining folds
2023-03-23 18:08:33,626:INFO:Declaring metric variables
2023-03-23 18:08:33,630:INFO:Importing untrained model
2023-03-23 18:08:33,634:INFO:Elastic Net Imported successfully
2023-03-23 18:08:33,641:INFO:Starting cross validation
2023-03-23 18:08:33,641:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:08:37,143:INFO:Calculating mean and std
2023-03-23 18:08:37,144:INFO:Creating metrics dataframe
2023-03-23 18:08:37,633:INFO:Uploading results into container
2023-03-23 18:08:37,633:INFO:Uploading model into container now
2023-03-23 18:08:37,634:INFO:_master_model_container: 4
2023-03-23 18:08:37,634:INFO:_display_container: 2
2023-03-23 18:08:37,634:INFO:ElasticNet(random_state=666)
2023-03-23 18:08:37,635:INFO:create_model() successfully completed......................................
2023-03-23 18:08:37,757:INFO:SubProcess create_model() end ==================================
2023-03-23 18:08:37,758:INFO:Creating metrics dataframe
2023-03-23 18:08:37,768:INFO:Initializing Least Angle Regression
2023-03-23 18:08:37,769:INFO:Total runtime is 0.3956367015838623 minutes
2023-03-23 18:08:37,774:INFO:SubProcess create_model() called ==================================
2023-03-23 18:08:37,774:INFO:Initializing create_model()
2023-03-23 18:08:37,774:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:08:37,774:INFO:Checking exceptions
2023-03-23 18:08:37,774:INFO:Importing libraries
2023-03-23 18:08:37,774:INFO:Copying training dataset
2023-03-23 18:08:37,793:INFO:Defining folds
2023-03-23 18:08:37,793:INFO:Declaring metric variables
2023-03-23 18:08:37,797:INFO:Importing untrained model
2023-03-23 18:08:37,802:INFO:Least Angle Regression Imported successfully
2023-03-23 18:08:37,813:INFO:Starting cross validation
2023-03-23 18:08:37,814:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:08:37,861:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:37,871:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:37,880:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:37,897:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:37,898:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:37,910:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=5.274e-06, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:08:37,912:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.991e-06, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:08:37,925:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:37,933:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:37,933:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.908e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:08:37,938:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.479e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:08:37,939:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.082e-06, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:08:37,939:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.189e-06, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:08:37,940:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.051e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:08:37,942:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.194e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:08:37,945:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=5.760e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:08:37,954:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:37,971:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.362e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:08:37,972:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.152e-06, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:08:37,972:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.659e-06, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:08:38,596:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:38,608:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.008e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:08:38,623:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:41,301:INFO:Calculating mean and std
2023-03-23 18:08:41,303:INFO:Creating metrics dataframe
2023-03-23 18:08:41,794:INFO:Uploading results into container
2023-03-23 18:08:41,795:INFO:Uploading model into container now
2023-03-23 18:08:41,795:INFO:_master_model_container: 5
2023-03-23 18:08:41,795:INFO:_display_container: 2
2023-03-23 18:08:41,795:INFO:Lars(random_state=666)
2023-03-23 18:08:41,796:INFO:create_model() successfully completed......................................
2023-03-23 18:08:41,918:INFO:SubProcess create_model() end ==================================
2023-03-23 18:08:41,918:INFO:Creating metrics dataframe
2023-03-23 18:08:41,930:INFO:Initializing Lasso Least Angle Regression
2023-03-23 18:08:41,930:INFO:Total runtime is 0.4649980187416077 minutes
2023-03-23 18:08:41,934:INFO:SubProcess create_model() called ==================================
2023-03-23 18:08:41,934:INFO:Initializing create_model()
2023-03-23 18:08:41,934:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:08:41,934:INFO:Checking exceptions
2023-03-23 18:08:41,934:INFO:Importing libraries
2023-03-23 18:08:41,934:INFO:Copying training dataset
2023-03-23 18:08:41,952:INFO:Defining folds
2023-03-23 18:08:41,952:INFO:Declaring metric variables
2023-03-23 18:08:41,956:INFO:Importing untrained model
2023-03-23 18:08:41,961:INFO:Lasso Least Angle Regression Imported successfully
2023-03-23 18:08:41,969:INFO:Starting cross validation
2023-03-23 18:08:41,970:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:08:42,022:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:08:42,029:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:08:42,039:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:08:42,055:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:08:42,063:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:08:42,081:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:08:42,093:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:08:42,113:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:08:42,728:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:08:42,744:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:08:45,424:INFO:Calculating mean and std
2023-03-23 18:08:45,425:INFO:Creating metrics dataframe
2023-03-23 18:08:45,913:INFO:Uploading results into container
2023-03-23 18:08:45,914:INFO:Uploading model into container now
2023-03-23 18:08:45,914:INFO:_master_model_container: 6
2023-03-23 18:08:45,914:INFO:_display_container: 2
2023-03-23 18:08:45,915:INFO:LassoLars(random_state=666)
2023-03-23 18:08:45,915:INFO:create_model() successfully completed......................................
2023-03-23 18:08:46,039:INFO:SubProcess create_model() end ==================================
2023-03-23 18:08:46,039:INFO:Creating metrics dataframe
2023-03-23 18:08:46,050:INFO:Initializing Orthogonal Matching Pursuit
2023-03-23 18:08:46,050:INFO:Total runtime is 0.5336527268091837 minutes
2023-03-23 18:08:46,055:INFO:SubProcess create_model() called ==================================
2023-03-23 18:08:46,055:INFO:Initializing create_model()
2023-03-23 18:08:46,055:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:08:46,055:INFO:Checking exceptions
2023-03-23 18:08:46,055:INFO:Importing libraries
2023-03-23 18:08:46,055:INFO:Copying training dataset
2023-03-23 18:08:46,074:INFO:Defining folds
2023-03-23 18:08:46,074:INFO:Declaring metric variables
2023-03-23 18:08:46,077:INFO:Importing untrained model
2023-03-23 18:08:46,081:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-23 18:08:46,088:INFO:Starting cross validation
2023-03-23 18:08:46,089:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:08:46,127:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:46,142:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:46,158:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:46,172:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:46,185:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:46,205:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:46,218:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:46,232:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:46,824:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:46,876:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:08:49,591:INFO:Calculating mean and std
2023-03-23 18:08:49,592:INFO:Creating metrics dataframe
2023-03-23 18:08:50,084:INFO:Uploading results into container
2023-03-23 18:08:50,084:INFO:Uploading model into container now
2023-03-23 18:08:50,085:INFO:_master_model_container: 7
2023-03-23 18:08:50,085:INFO:_display_container: 2
2023-03-23 18:08:50,085:INFO:OrthogonalMatchingPursuit()
2023-03-23 18:08:50,085:INFO:create_model() successfully completed......................................
2023-03-23 18:08:50,209:INFO:SubProcess create_model() end ==================================
2023-03-23 18:08:50,209:INFO:Creating metrics dataframe
2023-03-23 18:08:50,220:INFO:Initializing Bayesian Ridge
2023-03-23 18:08:50,220:INFO:Total runtime is 0.603166921933492 minutes
2023-03-23 18:08:50,225:INFO:SubProcess create_model() called ==================================
2023-03-23 18:08:50,226:INFO:Initializing create_model()
2023-03-23 18:08:50,226:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:08:50,226:INFO:Checking exceptions
2023-03-23 18:08:50,226:INFO:Importing libraries
2023-03-23 18:08:50,226:INFO:Copying training dataset
2023-03-23 18:08:50,244:INFO:Defining folds
2023-03-23 18:08:50,244:INFO:Declaring metric variables
2023-03-23 18:08:50,248:INFO:Importing untrained model
2023-03-23 18:08:50,251:INFO:Bayesian Ridge Imported successfully
2023-03-23 18:08:50,258:INFO:Starting cross validation
2023-03-23 18:08:50,259:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:08:53,803:INFO:Calculating mean and std
2023-03-23 18:08:53,804:INFO:Creating metrics dataframe
2023-03-23 18:08:54,320:INFO:Uploading results into container
2023-03-23 18:08:54,320:INFO:Uploading model into container now
2023-03-23 18:08:54,321:INFO:_master_model_container: 8
2023-03-23 18:08:54,321:INFO:_display_container: 2
2023-03-23 18:08:54,321:INFO:BayesianRidge()
2023-03-23 18:08:54,321:INFO:create_model() successfully completed......................................
2023-03-23 18:08:54,443:INFO:SubProcess create_model() end ==================================
2023-03-23 18:08:54,443:INFO:Creating metrics dataframe
2023-03-23 18:08:54,455:INFO:Initializing Passive Aggressive Regressor
2023-03-23 18:08:54,455:INFO:Total runtime is 0.6737456878026326 minutes
2023-03-23 18:08:54,459:INFO:SubProcess create_model() called ==================================
2023-03-23 18:08:54,460:INFO:Initializing create_model()
2023-03-23 18:08:54,460:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:08:54,460:INFO:Checking exceptions
2023-03-23 18:08:54,460:INFO:Importing libraries
2023-03-23 18:08:54,460:INFO:Copying training dataset
2023-03-23 18:08:54,479:INFO:Defining folds
2023-03-23 18:08:54,479:INFO:Declaring metric variables
2023-03-23 18:08:54,485:INFO:Importing untrained model
2023-03-23 18:08:54,489:INFO:Passive Aggressive Regressor Imported successfully
2023-03-23 18:08:54,498:INFO:Starting cross validation
2023-03-23 18:08:54,498:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:08:58,003:INFO:Calculating mean and std
2023-03-23 18:08:58,005:INFO:Creating metrics dataframe
2023-03-23 18:08:58,500:INFO:Uploading results into container
2023-03-23 18:08:58,501:INFO:Uploading model into container now
2023-03-23 18:08:58,502:INFO:_master_model_container: 9
2023-03-23 18:08:58,502:INFO:_display_container: 2
2023-03-23 18:08:58,502:INFO:PassiveAggressiveRegressor(random_state=666)
2023-03-23 18:08:58,502:INFO:create_model() successfully completed......................................
2023-03-23 18:08:58,634:INFO:SubProcess create_model() end ==================================
2023-03-23 18:08:58,635:INFO:Creating metrics dataframe
2023-03-23 18:08:58,648:INFO:Initializing Huber Regressor
2023-03-23 18:08:58,648:INFO:Total runtime is 0.7436290820439656 minutes
2023-03-23 18:08:58,653:INFO:SubProcess create_model() called ==================================
2023-03-23 18:08:58,653:INFO:Initializing create_model()
2023-03-23 18:08:58,653:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:08:58,653:INFO:Checking exceptions
2023-03-23 18:08:58,653:INFO:Importing libraries
2023-03-23 18:08:58,653:INFO:Copying training dataset
2023-03-23 18:08:58,672:INFO:Defining folds
2023-03-23 18:08:58,672:INFO:Declaring metric variables
2023-03-23 18:08:58,677:INFO:Importing untrained model
2023-03-23 18:08:58,683:INFO:Huber Regressor Imported successfully
2023-03-23 18:08:58,693:INFO:Starting cross validation
2023-03-23 18:08:58,694:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:09:02,217:INFO:Calculating mean and std
2023-03-23 18:09:02,218:INFO:Creating metrics dataframe
2023-03-23 18:09:02,707:INFO:Uploading results into container
2023-03-23 18:09:02,708:INFO:Uploading model into container now
2023-03-23 18:09:02,708:INFO:_master_model_container: 10
2023-03-23 18:09:02,709:INFO:_display_container: 2
2023-03-23 18:09:02,709:INFO:HuberRegressor()
2023-03-23 18:09:02,709:INFO:create_model() successfully completed......................................
2023-03-23 18:09:02,832:INFO:SubProcess create_model() end ==================================
2023-03-23 18:09:02,833:INFO:Creating metrics dataframe
2023-03-23 18:09:02,846:INFO:Initializing K Neighbors Regressor
2023-03-23 18:09:02,846:INFO:Total runtime is 0.8135988712310791 minutes
2023-03-23 18:09:02,850:INFO:SubProcess create_model() called ==================================
2023-03-23 18:09:02,850:INFO:Initializing create_model()
2023-03-23 18:09:02,851:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:09:02,851:INFO:Checking exceptions
2023-03-23 18:09:02,851:INFO:Importing libraries
2023-03-23 18:09:02,851:INFO:Copying training dataset
2023-03-23 18:09:02,868:INFO:Defining folds
2023-03-23 18:09:02,869:INFO:Declaring metric variables
2023-03-23 18:09:02,873:INFO:Importing untrained model
2023-03-23 18:09:02,877:INFO:K Neighbors Regressor Imported successfully
2023-03-23 18:09:02,887:INFO:Starting cross validation
2023-03-23 18:09:02,888:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:09:07,709:INFO:Calculating mean and std
2023-03-23 18:09:07,710:INFO:Creating metrics dataframe
2023-03-23 18:09:08,202:INFO:Uploading results into container
2023-03-23 18:09:08,203:INFO:Uploading model into container now
2023-03-23 18:09:08,203:INFO:_master_model_container: 11
2023-03-23 18:09:08,203:INFO:_display_container: 2
2023-03-23 18:09:08,204:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-23 18:09:08,204:INFO:create_model() successfully completed......................................
2023-03-23 18:09:08,328:INFO:SubProcess create_model() end ==================================
2023-03-23 18:09:08,329:INFO:Creating metrics dataframe
2023-03-23 18:09:08,341:INFO:Initializing Decision Tree Regressor
2023-03-23 18:09:08,341:INFO:Total runtime is 0.9051787932713826 minutes
2023-03-23 18:09:08,345:INFO:SubProcess create_model() called ==================================
2023-03-23 18:09:08,346:INFO:Initializing create_model()
2023-03-23 18:09:08,346:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:09:08,346:INFO:Checking exceptions
2023-03-23 18:09:08,346:INFO:Importing libraries
2023-03-23 18:09:08,346:INFO:Copying training dataset
2023-03-23 18:09:08,363:INFO:Defining folds
2023-03-23 18:09:08,363:INFO:Declaring metric variables
2023-03-23 18:09:08,368:INFO:Importing untrained model
2023-03-23 18:09:08,372:INFO:Decision Tree Regressor Imported successfully
2023-03-23 18:09:08,379:INFO:Starting cross validation
2023-03-23 18:09:08,380:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:09:12,414:INFO:Calculating mean and std
2023-03-23 18:09:12,416:INFO:Creating metrics dataframe
2023-03-23 18:09:12,911:INFO:Uploading results into container
2023-03-23 18:09:12,912:INFO:Uploading model into container now
2023-03-23 18:09:12,912:INFO:_master_model_container: 12
2023-03-23 18:09:12,912:INFO:_display_container: 2
2023-03-23 18:09:12,912:INFO:DecisionTreeRegressor(random_state=666)
2023-03-23 18:09:12,912:INFO:create_model() successfully completed......................................
2023-03-23 18:09:13,035:INFO:SubProcess create_model() end ==================================
2023-03-23 18:09:13,035:INFO:Creating metrics dataframe
2023-03-23 18:09:13,048:INFO:Initializing Random Forest Regressor
2023-03-23 18:09:13,048:INFO:Total runtime is 0.9836290041605631 minutes
2023-03-23 18:09:13,052:INFO:SubProcess create_model() called ==================================
2023-03-23 18:09:13,053:INFO:Initializing create_model()
2023-03-23 18:09:13,053:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:09:13,053:INFO:Checking exceptions
2023-03-23 18:09:13,053:INFO:Importing libraries
2023-03-23 18:09:13,053:INFO:Copying training dataset
2023-03-23 18:09:13,071:INFO:Defining folds
2023-03-23 18:09:13,071:INFO:Declaring metric variables
2023-03-23 18:09:13,075:INFO:Importing untrained model
2023-03-23 18:09:13,079:INFO:Random Forest Regressor Imported successfully
2023-03-23 18:09:13,086:INFO:Starting cross validation
2023-03-23 18:09:13,086:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:09:17,830:INFO:Calculating mean and std
2023-03-23 18:09:17,831:INFO:Creating metrics dataframe
2023-03-23 18:09:18,326:INFO:Uploading results into container
2023-03-23 18:09:18,327:INFO:Uploading model into container now
2023-03-23 18:09:18,328:INFO:_master_model_container: 13
2023-03-23 18:09:18,328:INFO:_display_container: 2
2023-03-23 18:09:18,328:INFO:RandomForestRegressor(n_jobs=-1, random_state=666)
2023-03-23 18:09:18,328:INFO:create_model() successfully completed......................................
2023-03-23 18:09:18,450:INFO:SubProcess create_model() end ==================================
2023-03-23 18:09:18,450:INFO:Creating metrics dataframe
2023-03-23 18:09:18,464:INFO:Initializing Extra Trees Regressor
2023-03-23 18:09:18,464:INFO:Total runtime is 1.0739006757736205 minutes
2023-03-23 18:09:18,468:INFO:SubProcess create_model() called ==================================
2023-03-23 18:09:18,469:INFO:Initializing create_model()
2023-03-23 18:09:18,469:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:09:18,469:INFO:Checking exceptions
2023-03-23 18:09:18,469:INFO:Importing libraries
2023-03-23 18:09:18,470:INFO:Copying training dataset
2023-03-23 18:09:18,488:INFO:Defining folds
2023-03-23 18:09:18,489:INFO:Declaring metric variables
2023-03-23 18:09:18,493:INFO:Importing untrained model
2023-03-23 18:09:18,499:INFO:Extra Trees Regressor Imported successfully
2023-03-23 18:09:18,508:INFO:Starting cross validation
2023-03-23 18:09:18,510:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:09:24,537:INFO:Calculating mean and std
2023-03-23 18:09:24,539:INFO:Creating metrics dataframe
2023-03-23 18:09:25,128:INFO:Uploading results into container
2023-03-23 18:09:25,129:INFO:Uploading model into container now
2023-03-23 18:09:25,130:INFO:_master_model_container: 14
2023-03-23 18:09:25,130:INFO:_display_container: 2
2023-03-23 18:09:25,131:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=666)
2023-03-23 18:09:25,132:INFO:create_model() successfully completed......................................
2023-03-23 18:09:25,317:INFO:SubProcess create_model() end ==================================
2023-03-23 18:09:25,317:INFO:Creating metrics dataframe
2023-03-23 18:09:25,336:INFO:Initializing AdaBoost Regressor
2023-03-23 18:09:25,337:INFO:Total runtime is 1.18844496011734 minutes
2023-03-23 18:09:25,344:INFO:SubProcess create_model() called ==================================
2023-03-23 18:09:25,345:INFO:Initializing create_model()
2023-03-23 18:09:25,346:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:09:25,346:INFO:Checking exceptions
2023-03-23 18:09:25,346:INFO:Importing libraries
2023-03-23 18:09:25,346:INFO:Copying training dataset
2023-03-23 18:09:25,370:INFO:Defining folds
2023-03-23 18:09:25,371:INFO:Declaring metric variables
2023-03-23 18:09:25,378:INFO:Importing untrained model
2023-03-23 18:09:25,384:INFO:AdaBoost Regressor Imported successfully
2023-03-23 18:09:25,399:INFO:Starting cross validation
2023-03-23 18:09:25,400:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:09:30,466:INFO:Calculating mean and std
2023-03-23 18:09:30,467:INFO:Creating metrics dataframe
2023-03-23 18:09:30,959:INFO:Uploading results into container
2023-03-23 18:09:30,959:INFO:Uploading model into container now
2023-03-23 18:09:30,960:INFO:_master_model_container: 15
2023-03-23 18:09:30,960:INFO:_display_container: 2
2023-03-23 18:09:30,960:INFO:AdaBoostRegressor(random_state=666)
2023-03-23 18:09:30,960:INFO:create_model() successfully completed......................................
2023-03-23 18:09:31,084:INFO:SubProcess create_model() end ==================================
2023-03-23 18:09:31,084:INFO:Creating metrics dataframe
2023-03-23 18:09:31,098:INFO:Initializing Gradient Boosting Regressor
2023-03-23 18:09:31,098:INFO:Total runtime is 1.284464613596598 minutes
2023-03-23 18:09:31,102:INFO:SubProcess create_model() called ==================================
2023-03-23 18:09:31,102:INFO:Initializing create_model()
2023-03-23 18:09:31,102:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:09:31,102:INFO:Checking exceptions
2023-03-23 18:09:31,102:INFO:Importing libraries
2023-03-23 18:09:31,102:INFO:Copying training dataset
2023-03-23 18:09:31,124:INFO:Defining folds
2023-03-23 18:09:31,124:INFO:Declaring metric variables
2023-03-23 18:09:31,128:INFO:Importing untrained model
2023-03-23 18:09:31,133:INFO:Gradient Boosting Regressor Imported successfully
2023-03-23 18:09:31,145:INFO:Starting cross validation
2023-03-23 18:09:31,146:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:09:35,090:INFO:Calculating mean and std
2023-03-23 18:09:35,092:INFO:Creating metrics dataframe
2023-03-23 18:09:35,585:INFO:Uploading results into container
2023-03-23 18:09:35,585:INFO:Uploading model into container now
2023-03-23 18:09:35,586:INFO:_master_model_container: 16
2023-03-23 18:09:35,586:INFO:_display_container: 2
2023-03-23 18:09:35,586:INFO:GradientBoostingRegressor(random_state=666)
2023-03-23 18:09:35,586:INFO:create_model() successfully completed......................................
2023-03-23 18:09:35,708:INFO:SubProcess create_model() end ==================================
2023-03-23 18:09:35,708:INFO:Creating metrics dataframe
2023-03-23 18:09:35,722:INFO:Initializing Extreme Gradient Boosting
2023-03-23 18:09:35,722:INFO:Total runtime is 1.3615286072095232 minutes
2023-03-23 18:09:35,726:INFO:SubProcess create_model() called ==================================
2023-03-23 18:09:35,726:INFO:Initializing create_model()
2023-03-23 18:09:35,727:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:09:35,727:INFO:Checking exceptions
2023-03-23 18:09:35,727:INFO:Importing libraries
2023-03-23 18:09:35,727:INFO:Copying training dataset
2023-03-23 18:09:35,746:INFO:Defining folds
2023-03-23 18:09:35,746:INFO:Declaring metric variables
2023-03-23 18:09:35,751:INFO:Importing untrained model
2023-03-23 18:09:35,756:INFO:Extreme Gradient Boosting Imported successfully
2023-03-23 18:09:35,765:INFO:Starting cross validation
2023-03-23 18:09:35,766:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:09:40,243:INFO:Calculating mean and std
2023-03-23 18:09:40,244:INFO:Creating metrics dataframe
2023-03-23 18:09:40,779:INFO:Uploading results into container
2023-03-23 18:09:40,780:INFO:Uploading model into container now
2023-03-23 18:09:40,780:INFO:_master_model_container: 17
2023-03-23 18:09:40,781:INFO:_display_container: 2
2023-03-23 18:09:40,782:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=666, ...)
2023-03-23 18:09:40,782:INFO:create_model() successfully completed......................................
2023-03-23 18:09:40,924:INFO:SubProcess create_model() end ==================================
2023-03-23 18:09:40,924:INFO:Creating metrics dataframe
2023-03-23 18:09:40,938:INFO:Initializing Light Gradient Boosting Machine
2023-03-23 18:09:40,938:INFO:Total runtime is 1.448461600144704 minutes
2023-03-23 18:09:40,942:INFO:SubProcess create_model() called ==================================
2023-03-23 18:09:40,942:INFO:Initializing create_model()
2023-03-23 18:09:40,943:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:09:40,943:INFO:Checking exceptions
2023-03-23 18:09:40,943:INFO:Importing libraries
2023-03-23 18:09:40,943:INFO:Copying training dataset
2023-03-23 18:09:40,962:INFO:Defining folds
2023-03-23 18:09:40,962:INFO:Declaring metric variables
2023-03-23 18:09:40,967:INFO:Importing untrained model
2023-03-23 18:09:40,971:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-23 18:09:40,980:INFO:Starting cross validation
2023-03-23 18:09:40,981:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:09:46,554:INFO:Calculating mean and std
2023-03-23 18:09:46,556:INFO:Creating metrics dataframe
2023-03-23 18:09:47,048:INFO:Uploading results into container
2023-03-23 18:09:47,049:INFO:Uploading model into container now
2023-03-23 18:09:47,049:INFO:_master_model_container: 18
2023-03-23 18:09:47,049:INFO:_display_container: 2
2023-03-23 18:09:47,049:INFO:LGBMRegressor(random_state=666)
2023-03-23 18:09:47,049:INFO:create_model() successfully completed......................................
2023-03-23 18:09:47,173:INFO:SubProcess create_model() end ==================================
2023-03-23 18:09:47,173:INFO:Creating metrics dataframe
2023-03-23 18:09:47,187:INFO:Initializing CatBoost Regressor
2023-03-23 18:09:47,187:INFO:Total runtime is 1.552617835998535 minutes
2023-03-23 18:09:47,192:INFO:SubProcess create_model() called ==================================
2023-03-23 18:09:47,192:INFO:Initializing create_model()
2023-03-23 18:09:47,192:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:09:47,194:INFO:Checking exceptions
2023-03-23 18:09:47,194:INFO:Importing libraries
2023-03-23 18:09:47,194:INFO:Copying training dataset
2023-03-23 18:09:47,211:INFO:Defining folds
2023-03-23 18:09:47,211:INFO:Declaring metric variables
2023-03-23 18:09:47,215:INFO:Importing untrained model
2023-03-23 18:09:47,225:INFO:CatBoost Regressor Imported successfully
2023-03-23 18:09:47,232:INFO:Starting cross validation
2023-03-23 18:09:47,233:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:09:52,480:INFO:Calculating mean and std
2023-03-23 18:09:52,481:INFO:Creating metrics dataframe
2023-03-23 18:09:52,974:INFO:Uploading results into container
2023-03-23 18:09:52,975:INFO:Uploading model into container now
2023-03-23 18:09:52,975:INFO:_master_model_container: 19
2023-03-23 18:09:52,975:INFO:_display_container: 2
2023-03-23 18:09:52,975:INFO:<catboost.core.CatBoostRegressor object at 0x0000015A796DC9A0>
2023-03-23 18:09:52,976:INFO:create_model() successfully completed......................................
2023-03-23 18:09:53,098:INFO:SubProcess create_model() end ==================================
2023-03-23 18:09:53,098:INFO:Creating metrics dataframe
2023-03-23 18:09:53,113:INFO:Initializing Dummy Regressor
2023-03-23 18:09:53,113:INFO:Total runtime is 1.6513717373212178 minutes
2023-03-23 18:09:53,120:INFO:SubProcess create_model() called ==================================
2023-03-23 18:09:53,120:INFO:Initializing create_model()
2023-03-23 18:09:53,120:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015A79723E80>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:09:53,120:INFO:Checking exceptions
2023-03-23 18:09:53,120:INFO:Importing libraries
2023-03-23 18:09:53,120:INFO:Copying training dataset
2023-03-23 18:09:53,139:INFO:Defining folds
2023-03-23 18:09:53,139:INFO:Declaring metric variables
2023-03-23 18:09:53,143:INFO:Importing untrained model
2023-03-23 18:09:53,147:INFO:Dummy Regressor Imported successfully
2023-03-23 18:09:53,156:INFO:Starting cross validation
2023-03-23 18:09:53,158:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:09:56,856:INFO:Calculating mean and std
2023-03-23 18:09:56,857:INFO:Creating metrics dataframe
2023-03-23 18:09:57,349:INFO:Uploading results into container
2023-03-23 18:09:57,349:INFO:Uploading model into container now
2023-03-23 18:09:57,350:INFO:_master_model_container: 20
2023-03-23 18:09:57,350:INFO:_display_container: 2
2023-03-23 18:09:57,350:INFO:DummyRegressor()
2023-03-23 18:09:57,350:INFO:create_model() successfully completed......................................
2023-03-23 18:09:57,474:INFO:SubProcess create_model() end ==================================
2023-03-23 18:09:57,475:INFO:Creating metrics dataframe
2023-03-23 18:09:57,504:INFO:Initializing create_model()
2023-03-23 18:09:57,504:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=GradientBoostingRegressor(random_state=666), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:09:57,504:INFO:Checking exceptions
2023-03-23 18:09:57,506:INFO:Importing libraries
2023-03-23 18:09:57,506:INFO:Copying training dataset
2023-03-23 18:09:57,526:INFO:Defining folds
2023-03-23 18:09:57,526:INFO:Declaring metric variables
2023-03-23 18:09:57,527:INFO:Importing untrained model
2023-03-23 18:09:57,527:INFO:Declaring custom model
2023-03-23 18:09:57,528:INFO:Gradient Boosting Regressor Imported successfully
2023-03-23 18:09:57,528:INFO:Cross validation set to False
2023-03-23 18:09:57,528:INFO:Fitting Model
2023-03-23 18:09:57,893:INFO:GradientBoostingRegressor(random_state=666)
2023-03-23 18:09:57,893:INFO:create_model() successfully completed......................................
2023-03-23 18:09:58,053:INFO:_master_model_container: 20
2023-03-23 18:09:58,054:INFO:_display_container: 2
2023-03-23 18:09:58,054:INFO:GradientBoostingRegressor(random_state=666)
2023-03-23 18:09:58,054:INFO:compare_models() successfully completed......................................
2023-03-23 18:09:58,078:INFO:Initializing predict_model()
2023-03-23 18:09:58,078:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000015A662D3EB0>, estimator=GradientBoostingRegressor(random_state=666), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000015A798D31F0>)
2023-03-23 18:09:58,078:INFO:Checking exceptions
2023-03-23 18:09:58,078:INFO:Preloading libraries
2023-03-23 18:09:58,080:INFO:Set up data.
2023-03-23 18:09:58,087:INFO:Set up index.
2023-03-23 18:15:55,179:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_01363837d58d458488cc0631898522bf
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,179:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_43e156a9f6b048528e3bc9968377b55c_6f12e782960e4be0af5ddb931247e67e
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,180:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_42d319097fd844b395f22116d616c053
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,180:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_8a10ade879db438bbe05461b9be49053_616c412d73264ef7829e31bc75280545
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,180:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_7c82936910bd4c28b7d8f064d596e624
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,180:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_13d791638f014826b5d7ff477ffe5cd4_6f5bf49d50ba4788bf8df9f677efcd86
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,180:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_d79feb48d9004849ab6b7cc9d1c882d3
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,180:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_6011246121bf4aeea32c0861ed050cc5_4a1b6977e5ee4c45864d3ba6ab431a5e
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,180:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_ca2f6b61b988426fa8052fd4cb1ef221
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,181:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_e183a8e740ce43a98c7ae8490cd1131c_5a65dce3ae4340f1a8486c6aec4b50de
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,181:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_fda662052c784b0182da3943df04916f
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,181:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_8e7e6bd5a0164a9e84ce94f8e2db46b9_869b0bb453e3473f98fbc2003d006734
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,181:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_6ebf88a471d646d0815fe8ec33683195
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,181:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_844a884e617c474fa0a9a50dc634b8c6_d44e5ef8e6074f5db6bfc9e9eb95763d
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,181:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_192f8a5e8e974af388d83afa64a2d77d
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,181:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_19422e3f183942129cf26253d379d54e_3ac5480d946a4d9d8ec0f18611bbaa65
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,181:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_79f5060e7cbc4bbd8ae19994e6a65c7f
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,182:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_3e58572df89e4ba59f9e07fb5390127c_c2d9d6a9706d400a90a41af5e674fad6
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,182:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_2140ed49d82a48c9a47760b9aee43600
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,182:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_f4505c71559c4f50b9b750ad8b5a6645_cdd41b40063146c8ab0b09df4c2b3d22
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,182:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_88717adafd0b4f28ac0665d66c54a9af
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,182:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_78893bceb547439db9ddb0380888d2f5_4c1d5813152d48a5a7b6ac7a700141d8
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,182:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_19dd047d80cb4edda429b880a182ea0c
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,182:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b2c0f4a639bc4dfcb8e01e8e3434e865_2174538cc9314f2894a013bf319c5f7f
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,182:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_3a34cecc09d840ec920dd850215dbec9
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,183:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_5195f23288ce4c8aa866f62f9621c09e_d16f96e7b343478199025f782744b737
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,183:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_bbddaf25c3ad4f5998a1c48fe4a6e70a
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,183:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_c0d7dba527c9451f90c5d2d152302e49_5e5b6911900c4003a100ec1e13308e8d
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,183:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_2182be4dfdab4ba5b492e8dcfb9f45d7
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,183:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_ac9dd776bc8843d6a8fe2a17fce04bfd_9cfe6eaf4da54c6e91200a8e47ab3b69
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,183:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_02cf76d195fe4da29fabd2f4e889d02d
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,183:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_e5dce849aa854b2a8f1f095e328d0fd7_90a3fb2b3e544676960c645323c2224f
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,183:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_d6bb46b77e224b119bd0980a310538cb
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,184:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_2673ed6bbdb24c5abf783ec3f474bd65_8b07b684d48f4d099a8a4a1fe3318c2a
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,184:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_aa0edc55b9ac4a22a901e28938602613
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,184:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_574145a757194fe6993c570b6feea010_97015e907873409f8a204dc23ecaaba9
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,184:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_742915a4259f4f53b7991daa705da2d0
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,184:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_eca0884066a741b691f95afc34aadc94_0ebb3c354f0840238d1ef3c663edb04b
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,184:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_cb53ae50e81f4ad895f51aadaddc6dc8
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:15:55,184:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\joblib\_memmapping_reducer.py:611: UserWarning: Failed to delete temporary folder: C:\Users\CHUKWUKA\AppData\Local\Temp\joblib_memmapping_folder_21080_b6dc32e2b10c452f8215913774888cba_a3f04a70ea15463994967d2d7483bdb5
  warnings.warn("Failed to delete temporary folder: {}"

2023-03-23 18:17:54,042:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-23 18:17:54,043:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-23 18:17:54,043:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-23 18:17:54,043:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-03-23 18:17:54,873:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-03-23 18:17:55,399:INFO:PyCaret RegressionExperiment
2023-03-23 18:17:55,400:INFO:Logging name: reg-default-name
2023-03-23 18:17:55,400:INFO:ML Usecase: MLUsecase.REGRESSION
2023-03-23 18:17:55,400:INFO:version 3.0.0
2023-03-23 18:17:55,400:INFO:Initializing setup()
2023-03-23 18:17:55,400:INFO:self.USI: 847c
2023-03-23 18:17:55,400:INFO:self._variable_keys: {'_ml_usecase', 'X_test', 'transform_target_param', 'fold_generator', 'pipeline', 'log_plots_param', 'n_jobs_param', 'exp_name_log', 'X_train', 'logging_param', 'data', 'target_param', 'html_param', 'memory', 'exp_id', 'y_test', 'y', 'fold_shuffle_param', 'gpu_n_jobs_param', 'USI', 'gpu_param', 'fold_groups_param', '_available_plots', 'y_train', 'idx', 'X', 'seed'}
2023-03-23 18:17:55,400:INFO:Checking environment
2023-03-23 18:17:55,400:INFO:python_version: 3.9.12
2023-03-23 18:17:55,400:INFO:python_build: ('main', 'Apr  4 2022 05:22:27')
2023-03-23 18:17:55,400:INFO:machine: AMD64
2023-03-23 18:17:55,400:INFO:platform: Windows-10-10.0.19045-SP0
2023-03-23 18:17:55,400:INFO:Memory: svmem(total=8496553984, available=2187329536, percent=74.3, used=6309224448, free=2187329536)
2023-03-23 18:17:55,400:INFO:Physical Core: 4
2023-03-23 18:17:55,400:INFO:Logical Core: 8
2023-03-23 18:17:55,400:INFO:Checking libraries
2023-03-23 18:17:55,400:INFO:System:
2023-03-23 18:17:55,400:INFO:    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
2023-03-23 18:17:55,401:INFO:executable: C:\Users\CHUKWUKA\anaconda3\python.exe
2023-03-23 18:17:55,401:INFO:   machine: Windows-10-10.0.19045-SP0
2023-03-23 18:17:55,401:INFO:PyCaret required dependencies:
2023-03-23 18:17:55,401:INFO:                 pip: 22.3.1
2023-03-23 18:17:55,401:INFO:          setuptools: 65.5.0
2023-03-23 18:17:55,401:INFO:             pycaret: 3.0.0
2023-03-23 18:17:55,401:INFO:             IPython: 8.7.0
2023-03-23 18:17:55,401:INFO:          ipywidgets: 7.6.5
2023-03-23 18:17:55,401:INFO:                tqdm: 4.64.1
2023-03-23 18:17:55,401:INFO:               numpy: 1.21.5
2023-03-23 18:17:55,401:INFO:              pandas: 1.4.4
2023-03-23 18:17:55,401:INFO:              jinja2: 2.11.3
2023-03-23 18:17:55,401:INFO:               scipy: 1.9.3
2023-03-23 18:17:55,401:INFO:              joblib: 1.2.0
2023-03-23 18:17:55,401:INFO:             sklearn: 1.0.2
2023-03-23 18:17:55,401:INFO:                pyod: 1.0.9
2023-03-23 18:17:55,401:INFO:            imblearn: 0.10.1
2023-03-23 18:17:55,401:INFO:   category_encoders: 2.6.0
2023-03-23 18:17:55,402:INFO:            lightgbm: 3.3.5
2023-03-23 18:17:55,402:INFO:               numba: 0.56.4
2023-03-23 18:17:55,402:INFO:            requests: 2.27.1
2023-03-23 18:17:55,402:INFO:          matplotlib: 3.6.2
2023-03-23 18:17:55,402:INFO:          scikitplot: 0.3.7
2023-03-23 18:17:55,402:INFO:         yellowbrick: 1.5
2023-03-23 18:17:55,402:INFO:              plotly: 5.9.0
2023-03-23 18:17:55,402:INFO:             kaleido: 0.2.1
2023-03-23 18:17:55,402:INFO:         statsmodels: 0.13.2
2023-03-23 18:17:55,402:INFO:              sktime: 0.16.1
2023-03-23 18:17:55,402:INFO:               tbats: 1.1.2
2023-03-23 18:17:55,402:INFO:            pmdarima: 2.0.3
2023-03-23 18:17:55,402:INFO:              psutil: 5.9.0
2023-03-23 18:17:55,402:INFO:PyCaret optional dependencies:
2023-03-23 18:17:55,415:INFO:                shap: Not installed
2023-03-23 18:17:55,415:INFO:           interpret: Not installed
2023-03-23 18:17:55,415:INFO:                umap: Not installed
2023-03-23 18:17:55,415:INFO:    pandas_profiling: Not installed
2023-03-23 18:17:55,415:INFO:  explainerdashboard: Not installed
2023-03-23 18:17:55,415:INFO:             autoviz: Not installed
2023-03-23 18:17:55,415:INFO:           fairlearn: Not installed
2023-03-23 18:17:55,415:INFO:             xgboost: 1.7.1
2023-03-23 18:17:55,416:INFO:            catboost: 1.0.6
2023-03-23 18:17:55,416:INFO:              kmodes: Not installed
2023-03-23 18:17:55,416:INFO:             mlxtend: Not installed
2023-03-23 18:17:55,416:INFO:       statsforecast: Not installed
2023-03-23 18:17:55,416:INFO:        tune_sklearn: Not installed
2023-03-23 18:17:55,416:INFO:                 ray: Not installed
2023-03-23 18:17:55,416:INFO:            hyperopt: Not installed
2023-03-23 18:17:55,416:INFO:              optuna: Not installed
2023-03-23 18:17:55,416:INFO:               skopt: Not installed
2023-03-23 18:17:55,416:INFO:              mlflow: Not installed
2023-03-23 18:17:55,416:INFO:              gradio: Not installed
2023-03-23 18:17:55,416:INFO:             fastapi: Not installed
2023-03-23 18:17:55,416:INFO:             uvicorn: Not installed
2023-03-23 18:17:55,416:INFO:              m2cgen: Not installed
2023-03-23 18:17:55,416:INFO:           evidently: Not installed
2023-03-23 18:17:55,416:INFO:               fugue: Not installed
2023-03-23 18:17:55,416:INFO:           streamlit: Not installed
2023-03-23 18:17:55,416:INFO:             prophet: Not installed
2023-03-23 18:17:55,416:INFO:None
2023-03-23 18:17:55,417:INFO:Set up data.
2023-03-23 18:17:55,426:INFO:Set up train/test split.
2023-03-23 18:17:55,426:INFO:Set up data.
2023-03-23 18:17:55,434:INFO:Set up index.
2023-03-23 18:17:55,434:INFO:Set up folding strategy.
2023-03-23 18:17:55,434:INFO:Assigning column types.
2023-03-23 18:17:55,439:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-03-23 18:17:55,439:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-23 18:17:55,445:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 18:17:55,451:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 18:17:55,531:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:17:55,586:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:17:55,587:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:17:55,782:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:17:55,802:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-03-23 18:17:55,806:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 18:17:55,811:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 18:17:55,880:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:17:55,925:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:17:55,926:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:17:55,929:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:17:55,929:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-03-23 18:17:55,934:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 18:17:55,938:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,121:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,166:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,167:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:17:56,170:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:17:56,175:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,179:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,247:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,293:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,293:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:17:56,296:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:17:56,296:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-03-23 18:17:56,306:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,374:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,421:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,421:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:17:56,424:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:17:56,433:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,501:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,546:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,546:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:17:56,549:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:17:56,550:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-03-23 18:17:56,626:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,672:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,672:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:17:56,675:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:17:56,752:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,798:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,799:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:17:56,802:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:17:56,802:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-03-23 18:17:56,879:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:17:56,925:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:17:56,928:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:17:57,012:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-03-23 18:17:57,061:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:17:57,064:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:17:57,064:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-03-23 18:17:57,187:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:17:57,189:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:17:57,313:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:17:57,315:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:17:57,318:INFO:Set up column name cleaning.
2023-03-23 18:17:57,335:INFO:Finished creating preprocessing pipeline.
2023-03-23 18:17:57,336:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\CHUKWUKA\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-03-23 18:17:57,336:INFO:Creating final display dataframe.
2023-03-23 18:17:57,408:INFO:Setup _display_container:                    Description              Value
0                   Session id                666
1                       Target  Item_Outlet_Sales
2                  Target type         Regression
3          Original data shape         (8523, 19)
4       Transformed data shape        (13637, 19)
5  Transformed train set shape         (8523, 19)
6   Transformed test set shape         (5114, 19)
7             Numeric features                 18
2023-03-23 18:17:57,541:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:17:57,544:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:17:57,668:INFO:Soft dependency imported: xgboost: 1.7.1
2023-03-23 18:17:57,671:INFO:Soft dependency imported: catboost: 1.0.6
2023-03-23 18:17:57,671:INFO:setup() successfully completed in 2.61s...............
2023-03-23 18:17:57,684:INFO:Initializing compare_models()
2023-03-23 18:17:57,684:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, include=None, fold=None, round=4, cross_validation=True, sort=MSE, n_select=1, budget_time=10, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MSE', 'n_select': 1, 'budget_time': 10, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-03-23 18:17:57,684:INFO:Checking exceptions
2023-03-23 18:17:57,694:INFO:Preparing display monitor
2023-03-23 18:17:57,723:INFO:Time budget is 10 minutes
2023-03-23 18:17:57,723:INFO:Initializing Linear Regression
2023-03-23 18:17:57,724:INFO:Total runtime is 1.6709168752034504e-05 minutes
2023-03-23 18:17:57,727:INFO:SubProcess create_model() called ==================================
2023-03-23 18:17:57,728:INFO:Initializing create_model()
2023-03-23 18:17:57,728:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:17:57,728:INFO:Checking exceptions
2023-03-23 18:17:57,728:INFO:Importing libraries
2023-03-23 18:17:57,728:INFO:Copying training dataset
2023-03-23 18:17:57,750:INFO:Defining folds
2023-03-23 18:17:57,751:INFO:Declaring metric variables
2023-03-23 18:17:57,755:INFO:Importing untrained model
2023-03-23 18:17:57,760:INFO:Linear Regression Imported successfully
2023-03-23 18:17:57,770:INFO:Starting cross validation
2023-03-23 18:17:57,777:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:18:07,515:INFO:Calculating mean and std
2023-03-23 18:18:07,517:INFO:Creating metrics dataframe
2023-03-23 18:18:08,047:INFO:Uploading results into container
2023-03-23 18:18:08,048:INFO:Uploading model into container now
2023-03-23 18:18:08,048:INFO:_master_model_container: 1
2023-03-23 18:18:08,048:INFO:_display_container: 2
2023-03-23 18:18:08,049:INFO:LinearRegression(n_jobs=-1)
2023-03-23 18:18:08,049:INFO:create_model() successfully completed......................................
2023-03-23 18:18:08,182:INFO:SubProcess create_model() end ==================================
2023-03-23 18:18:08,182:INFO:Creating metrics dataframe
2023-03-23 18:18:08,191:INFO:Initializing Lasso Regression
2023-03-23 18:18:08,191:INFO:Total runtime is 0.1744654377301534 minutes
2023-03-23 18:18:08,196:INFO:SubProcess create_model() called ==================================
2023-03-23 18:18:08,196:INFO:Initializing create_model()
2023-03-23 18:18:08,196:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:18:08,196:INFO:Checking exceptions
2023-03-23 18:18:08,196:INFO:Importing libraries
2023-03-23 18:18:08,197:INFO:Copying training dataset
2023-03-23 18:18:08,215:INFO:Defining folds
2023-03-23 18:18:08,215:INFO:Declaring metric variables
2023-03-23 18:18:08,219:INFO:Importing untrained model
2023-03-23 18:18:08,223:INFO:Lasso Regression Imported successfully
2023-03-23 18:18:08,232:INFO:Starting cross validation
2023-03-23 18:18:08,233:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:18:11,681:INFO:Calculating mean and std
2023-03-23 18:18:11,682:INFO:Creating metrics dataframe
2023-03-23 18:18:12,192:INFO:Uploading results into container
2023-03-23 18:18:12,192:INFO:Uploading model into container now
2023-03-23 18:18:12,193:INFO:_master_model_container: 2
2023-03-23 18:18:12,193:INFO:_display_container: 2
2023-03-23 18:18:12,193:INFO:Lasso(random_state=666)
2023-03-23 18:18:12,194:INFO:create_model() successfully completed......................................
2023-03-23 18:18:12,308:INFO:SubProcess create_model() end ==================================
2023-03-23 18:18:12,308:INFO:Creating metrics dataframe
2023-03-23 18:18:12,319:INFO:Initializing Ridge Regression
2023-03-23 18:18:12,319:INFO:Total runtime is 0.24326481421788534 minutes
2023-03-23 18:18:12,324:INFO:SubProcess create_model() called ==================================
2023-03-23 18:18:12,324:INFO:Initializing create_model()
2023-03-23 18:18:12,324:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:18:12,324:INFO:Checking exceptions
2023-03-23 18:18:12,324:INFO:Importing libraries
2023-03-23 18:18:12,324:INFO:Copying training dataset
2023-03-23 18:18:12,343:INFO:Defining folds
2023-03-23 18:18:12,343:INFO:Declaring metric variables
2023-03-23 18:18:12,348:INFO:Importing untrained model
2023-03-23 18:18:12,352:INFO:Ridge Regression Imported successfully
2023-03-23 18:18:12,362:INFO:Starting cross validation
2023-03-23 18:18:12,363:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:18:12,421:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.19994e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:18:12,426:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17704e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:18:12,443:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17152e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:18:12,454:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.18403e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:18:12,462:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17938e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:18:12,485:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.2047e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:18:12,501:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.19028e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:18:12,521:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.20487e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:18:13,138:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.17503e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:18:13,152:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.18623e-08): result may not be accurate.
  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T

2023-03-23 18:18:15,843:INFO:Calculating mean and std
2023-03-23 18:18:15,844:INFO:Creating metrics dataframe
2023-03-23 18:18:16,373:INFO:Uploading results into container
2023-03-23 18:18:16,374:INFO:Uploading model into container now
2023-03-23 18:18:16,375:INFO:_master_model_container: 3
2023-03-23 18:18:16,375:INFO:_display_container: 2
2023-03-23 18:18:16,375:INFO:Ridge(random_state=666)
2023-03-23 18:18:16,376:INFO:create_model() successfully completed......................................
2023-03-23 18:18:16,490:INFO:SubProcess create_model() end ==================================
2023-03-23 18:18:16,490:INFO:Creating metrics dataframe
2023-03-23 18:18:16,502:INFO:Initializing Elastic Net
2023-03-23 18:18:16,502:INFO:Total runtime is 0.3129870136578878 minutes
2023-03-23 18:18:16,506:INFO:SubProcess create_model() called ==================================
2023-03-23 18:18:16,506:INFO:Initializing create_model()
2023-03-23 18:18:16,506:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:18:16,506:INFO:Checking exceptions
2023-03-23 18:18:16,506:INFO:Importing libraries
2023-03-23 18:18:16,506:INFO:Copying training dataset
2023-03-23 18:18:16,529:INFO:Defining folds
2023-03-23 18:18:16,529:INFO:Declaring metric variables
2023-03-23 18:18:16,535:INFO:Importing untrained model
2023-03-23 18:18:16,540:INFO:Elastic Net Imported successfully
2023-03-23 18:18:16,551:INFO:Starting cross validation
2023-03-23 18:18:16,551:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:18:20,032:INFO:Calculating mean and std
2023-03-23 18:18:20,033:INFO:Creating metrics dataframe
2023-03-23 18:18:20,538:INFO:Uploading results into container
2023-03-23 18:18:20,539:INFO:Uploading model into container now
2023-03-23 18:18:20,539:INFO:_master_model_container: 4
2023-03-23 18:18:20,540:INFO:_display_container: 2
2023-03-23 18:18:20,540:INFO:ElasticNet(random_state=666)
2023-03-23 18:18:20,540:INFO:create_model() successfully completed......................................
2023-03-23 18:18:20,654:INFO:SubProcess create_model() end ==================================
2023-03-23 18:18:20,654:INFO:Creating metrics dataframe
2023-03-23 18:18:20,664:INFO:Initializing Least Angle Regression
2023-03-23 18:18:20,664:INFO:Total runtime is 0.38235425154368086 minutes
2023-03-23 18:18:20,668:INFO:SubProcess create_model() called ==================================
2023-03-23 18:18:20,669:INFO:Initializing create_model()
2023-03-23 18:18:20,669:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:18:20,669:INFO:Checking exceptions
2023-03-23 18:18:20,669:INFO:Importing libraries
2023-03-23 18:18:20,669:INFO:Copying training dataset
2023-03-23 18:18:20,687:INFO:Defining folds
2023-03-23 18:18:20,687:INFO:Declaring metric variables
2023-03-23 18:18:20,691:INFO:Importing untrained model
2023-03-23 18:18:20,695:INFO:Least Angle Regression Imported successfully
2023-03-23 18:18:20,703:INFO:Starting cross validation
2023-03-23 18:18:20,704:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:18:20,751:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:20,758:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:20,771:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:20,777:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:20,791:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:20,802:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=5.274e-06, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:18:20,803:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.991e-06, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:18:20,809:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:20,816:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.908e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:18:20,821:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.479e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:18:20,822:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.082e-06, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:18:20,823:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.189e-06, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:18:20,823:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:20,829:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=6.051e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:18:20,833:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.194e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:18:20,836:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=5.760e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:18:20,845:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:20,855:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.362e-05, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:18:20,856:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.152e-06, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:18:20,857:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.659e-06, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:18:21,496:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:21,506:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_least_angle.py:652: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.008e-05, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-03-23 18:18:21,512:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:24,158:INFO:Calculating mean and std
2023-03-23 18:18:24,159:INFO:Creating metrics dataframe
2023-03-23 18:18:24,681:INFO:Uploading results into container
2023-03-23 18:18:24,682:INFO:Uploading model into container now
2023-03-23 18:18:24,682:INFO:_master_model_container: 5
2023-03-23 18:18:24,682:INFO:_display_container: 2
2023-03-23 18:18:24,682:INFO:Lars(random_state=666)
2023-03-23 18:18:24,682:INFO:create_model() successfully completed......................................
2023-03-23 18:18:24,797:INFO:SubProcess create_model() end ==================================
2023-03-23 18:18:24,798:INFO:Creating metrics dataframe
2023-03-23 18:18:24,809:INFO:Initializing Lasso Least Angle Regression
2023-03-23 18:18:24,809:INFO:Total runtime is 0.45143598318099976 minutes
2023-03-23 18:18:24,813:INFO:SubProcess create_model() called ==================================
2023-03-23 18:18:24,813:INFO:Initializing create_model()
2023-03-23 18:18:24,813:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:18:24,814:INFO:Checking exceptions
2023-03-23 18:18:24,814:INFO:Importing libraries
2023-03-23 18:18:24,814:INFO:Copying training dataset
2023-03-23 18:18:24,833:INFO:Defining folds
2023-03-23 18:18:24,833:INFO:Declaring metric variables
2023-03-23 18:18:24,838:INFO:Importing untrained model
2023-03-23 18:18:24,843:INFO:Lasso Least Angle Regression Imported successfully
2023-03-23 18:18:24,853:INFO:Starting cross validation
2023-03-23 18:18:24,853:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:18:24,896:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:18:24,905:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:18:24,913:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:18:24,930:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:18:24,938:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:18:24,957:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:18:24,974:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:18:24,990:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:18:25,588:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:18:25,640:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-03-23 18:18:28,300:INFO:Calculating mean and std
2023-03-23 18:18:28,301:INFO:Creating metrics dataframe
2023-03-23 18:18:28,825:INFO:Uploading results into container
2023-03-23 18:18:28,826:INFO:Uploading model into container now
2023-03-23 18:18:28,826:INFO:_master_model_container: 6
2023-03-23 18:18:28,826:INFO:_display_container: 2
2023-03-23 18:18:28,827:INFO:LassoLars(random_state=666)
2023-03-23 18:18:28,827:INFO:create_model() successfully completed......................................
2023-03-23 18:18:28,941:INFO:SubProcess create_model() end ==================================
2023-03-23 18:18:28,942:INFO:Creating metrics dataframe
2023-03-23 18:18:28,954:INFO:Initializing Orthogonal Matching Pursuit
2023-03-23 18:18:28,954:INFO:Total runtime is 0.520518712202708 minutes
2023-03-23 18:18:28,959:INFO:SubProcess create_model() called ==================================
2023-03-23 18:18:28,960:INFO:Initializing create_model()
2023-03-23 18:18:28,960:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:18:28,960:INFO:Checking exceptions
2023-03-23 18:18:28,960:INFO:Importing libraries
2023-03-23 18:18:28,960:INFO:Copying training dataset
2023-03-23 18:18:28,980:INFO:Defining folds
2023-03-23 18:18:28,980:INFO:Declaring metric variables
2023-03-23 18:18:28,985:INFO:Importing untrained model
2023-03-23 18:18:28,992:INFO:Orthogonal Matching Pursuit Imported successfully
2023-03-23 18:18:29,003:INFO:Starting cross validation
2023-03-23 18:18:29,003:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:18:29,085:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:29,089:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:29,097:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:29,114:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:29,129:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:29,140:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:29,152:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:29,174:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:29,815:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:29,828:WARNING:C:\Users\CHUKWUKA\anaconda3\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-03-23 18:18:32,506:INFO:Calculating mean and std
2023-03-23 18:18:32,508:INFO:Creating metrics dataframe
2023-03-23 18:18:33,022:INFO:Uploading results into container
2023-03-23 18:18:33,023:INFO:Uploading model into container now
2023-03-23 18:18:33,024:INFO:_master_model_container: 7
2023-03-23 18:18:33,024:INFO:_display_container: 2
2023-03-23 18:18:33,024:INFO:OrthogonalMatchingPursuit()
2023-03-23 18:18:33,024:INFO:create_model() successfully completed......................................
2023-03-23 18:18:33,139:INFO:SubProcess create_model() end ==================================
2023-03-23 18:18:33,139:INFO:Creating metrics dataframe
2023-03-23 18:18:33,152:INFO:Initializing Bayesian Ridge
2023-03-23 18:18:33,152:INFO:Total runtime is 0.5904809554417928 minutes
2023-03-23 18:18:33,156:INFO:SubProcess create_model() called ==================================
2023-03-23 18:18:33,157:INFO:Initializing create_model()
2023-03-23 18:18:33,157:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:18:33,157:INFO:Checking exceptions
2023-03-23 18:18:33,157:INFO:Importing libraries
2023-03-23 18:18:33,157:INFO:Copying training dataset
2023-03-23 18:18:33,176:INFO:Defining folds
2023-03-23 18:18:33,176:INFO:Declaring metric variables
2023-03-23 18:18:33,180:INFO:Importing untrained model
2023-03-23 18:18:33,184:INFO:Bayesian Ridge Imported successfully
2023-03-23 18:18:33,194:INFO:Starting cross validation
2023-03-23 18:18:33,195:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:18:36,632:INFO:Calculating mean and std
2023-03-23 18:18:36,634:INFO:Creating metrics dataframe
2023-03-23 18:18:37,153:INFO:Uploading results into container
2023-03-23 18:18:37,153:INFO:Uploading model into container now
2023-03-23 18:18:37,154:INFO:_master_model_container: 8
2023-03-23 18:18:37,154:INFO:_display_container: 2
2023-03-23 18:18:37,154:INFO:BayesianRidge()
2023-03-23 18:18:37,154:INFO:create_model() successfully completed......................................
2023-03-23 18:18:37,270:INFO:SubProcess create_model() end ==================================
2023-03-23 18:18:37,270:INFO:Creating metrics dataframe
2023-03-23 18:18:37,281:INFO:Initializing Passive Aggressive Regressor
2023-03-23 18:18:37,281:INFO:Total runtime is 0.6593061288197836 minutes
2023-03-23 18:18:37,285:INFO:SubProcess create_model() called ==================================
2023-03-23 18:18:37,286:INFO:Initializing create_model()
2023-03-23 18:18:37,286:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:18:37,286:INFO:Checking exceptions
2023-03-23 18:18:37,286:INFO:Importing libraries
2023-03-23 18:18:37,286:INFO:Copying training dataset
2023-03-23 18:18:37,304:INFO:Defining folds
2023-03-23 18:18:37,304:INFO:Declaring metric variables
2023-03-23 18:18:37,308:INFO:Importing untrained model
2023-03-23 18:18:37,313:INFO:Passive Aggressive Regressor Imported successfully
2023-03-23 18:18:37,322:INFO:Starting cross validation
2023-03-23 18:18:37,323:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:18:40,807:INFO:Calculating mean and std
2023-03-23 18:18:40,808:INFO:Creating metrics dataframe
2023-03-23 18:18:41,350:INFO:Uploading results into container
2023-03-23 18:18:41,351:INFO:Uploading model into container now
2023-03-23 18:18:41,351:INFO:_master_model_container: 9
2023-03-23 18:18:41,351:INFO:_display_container: 2
2023-03-23 18:18:41,351:INFO:PassiveAggressiveRegressor(random_state=666)
2023-03-23 18:18:41,352:INFO:create_model() successfully completed......................................
2023-03-23 18:18:41,465:INFO:SubProcess create_model() end ==================================
2023-03-23 18:18:41,465:INFO:Creating metrics dataframe
2023-03-23 18:18:41,476:INFO:Initializing Huber Regressor
2023-03-23 18:18:41,476:INFO:Total runtime is 0.7292150537172953 minutes
2023-03-23 18:18:41,480:INFO:SubProcess create_model() called ==================================
2023-03-23 18:18:41,481:INFO:Initializing create_model()
2023-03-23 18:18:41,481:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:18:41,481:INFO:Checking exceptions
2023-03-23 18:18:41,481:INFO:Importing libraries
2023-03-23 18:18:41,481:INFO:Copying training dataset
2023-03-23 18:18:41,499:INFO:Defining folds
2023-03-23 18:18:41,499:INFO:Declaring metric variables
2023-03-23 18:18:41,503:INFO:Importing untrained model
2023-03-23 18:18:41,507:INFO:Huber Regressor Imported successfully
2023-03-23 18:18:41,514:INFO:Starting cross validation
2023-03-23 18:18:41,515:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:18:45,063:INFO:Calculating mean and std
2023-03-23 18:18:45,065:INFO:Creating metrics dataframe
2023-03-23 18:18:45,588:INFO:Uploading results into container
2023-03-23 18:18:45,589:INFO:Uploading model into container now
2023-03-23 18:18:45,589:INFO:_master_model_container: 10
2023-03-23 18:18:45,589:INFO:_display_container: 2
2023-03-23 18:18:45,590:INFO:HuberRegressor()
2023-03-23 18:18:45,590:INFO:create_model() successfully completed......................................
2023-03-23 18:18:45,704:INFO:SubProcess create_model() end ==================================
2023-03-23 18:18:45,704:INFO:Creating metrics dataframe
2023-03-23 18:18:45,719:INFO:Initializing K Neighbors Regressor
2023-03-23 18:18:45,719:INFO:Total runtime is 0.7999433279037476 minutes
2023-03-23 18:18:45,723:INFO:SubProcess create_model() called ==================================
2023-03-23 18:18:45,724:INFO:Initializing create_model()
2023-03-23 18:18:45,724:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:18:45,724:INFO:Checking exceptions
2023-03-23 18:18:45,724:INFO:Importing libraries
2023-03-23 18:18:45,724:INFO:Copying training dataset
2023-03-23 18:18:45,742:INFO:Defining folds
2023-03-23 18:18:45,743:INFO:Declaring metric variables
2023-03-23 18:18:45,747:INFO:Importing untrained model
2023-03-23 18:18:45,754:INFO:K Neighbors Regressor Imported successfully
2023-03-23 18:18:45,763:INFO:Starting cross validation
2023-03-23 18:18:45,764:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:18:50,493:INFO:Calculating mean and std
2023-03-23 18:18:50,494:INFO:Creating metrics dataframe
2023-03-23 18:18:51,073:INFO:Uploading results into container
2023-03-23 18:18:51,073:INFO:Uploading model into container now
2023-03-23 18:18:51,074:INFO:_master_model_container: 11
2023-03-23 18:18:51,074:INFO:_display_container: 2
2023-03-23 18:18:51,074:INFO:KNeighborsRegressor(n_jobs=-1)
2023-03-23 18:18:51,074:INFO:create_model() successfully completed......................................
2023-03-23 18:18:51,203:INFO:SubProcess create_model() end ==================================
2023-03-23 18:18:51,203:INFO:Creating metrics dataframe
2023-03-23 18:18:51,216:INFO:Initializing Decision Tree Regressor
2023-03-23 18:18:51,216:INFO:Total runtime is 0.8915450572967529 minutes
2023-03-23 18:18:51,221:INFO:SubProcess create_model() called ==================================
2023-03-23 18:18:51,222:INFO:Initializing create_model()
2023-03-23 18:18:51,222:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:18:51,222:INFO:Checking exceptions
2023-03-23 18:18:51,222:INFO:Importing libraries
2023-03-23 18:18:51,223:INFO:Copying training dataset
2023-03-23 18:18:51,240:INFO:Defining folds
2023-03-23 18:18:51,241:INFO:Declaring metric variables
2023-03-23 18:18:51,244:INFO:Importing untrained model
2023-03-23 18:18:51,248:INFO:Decision Tree Regressor Imported successfully
2023-03-23 18:18:51,256:INFO:Starting cross validation
2023-03-23 18:18:51,257:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:18:55,097:INFO:Calculating mean and std
2023-03-23 18:18:55,098:INFO:Creating metrics dataframe
2023-03-23 18:18:55,630:INFO:Uploading results into container
2023-03-23 18:18:55,631:INFO:Uploading model into container now
2023-03-23 18:18:55,631:INFO:_master_model_container: 12
2023-03-23 18:18:55,632:INFO:_display_container: 2
2023-03-23 18:18:55,632:INFO:DecisionTreeRegressor(random_state=666)
2023-03-23 18:18:55,632:INFO:create_model() successfully completed......................................
2023-03-23 18:18:55,748:INFO:SubProcess create_model() end ==================================
2023-03-23 18:18:55,748:INFO:Creating metrics dataframe
2023-03-23 18:18:55,761:INFO:Initializing Random Forest Regressor
2023-03-23 18:18:55,761:INFO:Total runtime is 0.9672955830891927 minutes
2023-03-23 18:18:55,765:INFO:SubProcess create_model() called ==================================
2023-03-23 18:18:55,765:INFO:Initializing create_model()
2023-03-23 18:18:55,765:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:18:55,765:INFO:Checking exceptions
2023-03-23 18:18:55,766:INFO:Importing libraries
2023-03-23 18:18:55,766:INFO:Copying training dataset
2023-03-23 18:18:55,787:INFO:Defining folds
2023-03-23 18:18:55,787:INFO:Declaring metric variables
2023-03-23 18:18:55,791:INFO:Importing untrained model
2023-03-23 18:18:55,797:INFO:Random Forest Regressor Imported successfully
2023-03-23 18:18:55,807:INFO:Starting cross validation
2023-03-23 18:18:55,808:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:19:00,446:INFO:Calculating mean and std
2023-03-23 18:19:00,448:INFO:Creating metrics dataframe
2023-03-23 18:19:00,995:INFO:Uploading results into container
2023-03-23 18:19:00,996:INFO:Uploading model into container now
2023-03-23 18:19:00,996:INFO:_master_model_container: 13
2023-03-23 18:19:00,997:INFO:_display_container: 2
2023-03-23 18:19:00,997:INFO:RandomForestRegressor(n_jobs=-1, random_state=666)
2023-03-23 18:19:00,997:INFO:create_model() successfully completed......................................
2023-03-23 18:19:01,119:INFO:SubProcess create_model() end ==================================
2023-03-23 18:19:01,119:INFO:Creating metrics dataframe
2023-03-23 18:19:01,132:INFO:Initializing Extra Trees Regressor
2023-03-23 18:19:01,132:INFO:Total runtime is 1.056819196542104 minutes
2023-03-23 18:19:01,136:INFO:SubProcess create_model() called ==================================
2023-03-23 18:19:01,137:INFO:Initializing create_model()
2023-03-23 18:19:01,137:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:19:01,137:INFO:Checking exceptions
2023-03-23 18:19:01,137:INFO:Importing libraries
2023-03-23 18:19:01,137:INFO:Copying training dataset
2023-03-23 18:19:01,156:INFO:Defining folds
2023-03-23 18:19:01,156:INFO:Declaring metric variables
2023-03-23 18:19:01,160:INFO:Importing untrained model
2023-03-23 18:19:01,167:INFO:Extra Trees Regressor Imported successfully
2023-03-23 18:19:01,176:INFO:Starting cross validation
2023-03-23 18:19:01,177:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:19:06,124:INFO:Calculating mean and std
2023-03-23 18:19:06,125:INFO:Creating metrics dataframe
2023-03-23 18:19:06,640:INFO:Uploading results into container
2023-03-23 18:19:06,640:INFO:Uploading model into container now
2023-03-23 18:19:06,641:INFO:_master_model_container: 14
2023-03-23 18:19:06,641:INFO:_display_container: 2
2023-03-23 18:19:06,641:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=666)
2023-03-23 18:19:06,641:INFO:create_model() successfully completed......................................
2023-03-23 18:19:06,767:INFO:SubProcess create_model() end ==================================
2023-03-23 18:19:06,767:INFO:Creating metrics dataframe
2023-03-23 18:19:06,781:INFO:Initializing AdaBoost Regressor
2023-03-23 18:19:06,781:INFO:Total runtime is 1.15097469886144 minutes
2023-03-23 18:19:06,786:INFO:SubProcess create_model() called ==================================
2023-03-23 18:19:06,786:INFO:Initializing create_model()
2023-03-23 18:19:06,786:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:19:06,787:INFO:Checking exceptions
2023-03-23 18:19:06,787:INFO:Importing libraries
2023-03-23 18:19:06,787:INFO:Copying training dataset
2023-03-23 18:19:06,806:INFO:Defining folds
2023-03-23 18:19:06,806:INFO:Declaring metric variables
2023-03-23 18:19:06,810:INFO:Importing untrained model
2023-03-23 18:19:06,815:INFO:AdaBoost Regressor Imported successfully
2023-03-23 18:19:06,825:INFO:Starting cross validation
2023-03-23 18:19:06,826:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:19:10,750:INFO:Calculating mean and std
2023-03-23 18:19:10,751:INFO:Creating metrics dataframe
2023-03-23 18:19:11,252:INFO:Uploading results into container
2023-03-23 18:19:11,253:INFO:Uploading model into container now
2023-03-23 18:19:11,253:INFO:_master_model_container: 15
2023-03-23 18:19:11,254:INFO:_display_container: 2
2023-03-23 18:19:11,254:INFO:AdaBoostRegressor(random_state=666)
2023-03-23 18:19:11,254:INFO:create_model() successfully completed......................................
2023-03-23 18:19:11,379:INFO:SubProcess create_model() end ==================================
2023-03-23 18:19:11,379:INFO:Creating metrics dataframe
2023-03-23 18:19:11,392:INFO:Initializing Gradient Boosting Regressor
2023-03-23 18:19:11,392:INFO:Total runtime is 1.2278154730796813 minutes
2023-03-23 18:19:11,396:INFO:SubProcess create_model() called ==================================
2023-03-23 18:19:11,397:INFO:Initializing create_model()
2023-03-23 18:19:11,397:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:19:11,397:INFO:Checking exceptions
2023-03-23 18:19:11,397:INFO:Importing libraries
2023-03-23 18:19:11,397:INFO:Copying training dataset
2023-03-23 18:19:11,415:INFO:Defining folds
2023-03-23 18:19:11,415:INFO:Declaring metric variables
2023-03-23 18:19:11,419:INFO:Importing untrained model
2023-03-23 18:19:11,423:INFO:Gradient Boosting Regressor Imported successfully
2023-03-23 18:19:11,432:INFO:Starting cross validation
2023-03-23 18:19:11,433:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:19:15,544:INFO:Calculating mean and std
2023-03-23 18:19:15,546:INFO:Creating metrics dataframe
2023-03-23 18:19:16,070:INFO:Uploading results into container
2023-03-23 18:19:16,071:INFO:Uploading model into container now
2023-03-23 18:19:16,071:INFO:_master_model_container: 16
2023-03-23 18:19:16,072:INFO:_display_container: 2
2023-03-23 18:19:16,072:INFO:GradientBoostingRegressor(random_state=666)
2023-03-23 18:19:16,072:INFO:create_model() successfully completed......................................
2023-03-23 18:19:16,186:INFO:SubProcess create_model() end ==================================
2023-03-23 18:19:16,186:INFO:Creating metrics dataframe
2023-03-23 18:19:16,199:INFO:Initializing Extreme Gradient Boosting
2023-03-23 18:19:16,200:INFO:Total runtime is 1.3079459945360818 minutes
2023-03-23 18:19:16,204:INFO:SubProcess create_model() called ==================================
2023-03-23 18:19:16,204:INFO:Initializing create_model()
2023-03-23 18:19:16,204:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:19:16,204:INFO:Checking exceptions
2023-03-23 18:19:16,205:INFO:Importing libraries
2023-03-23 18:19:16,205:INFO:Copying training dataset
2023-03-23 18:19:16,224:INFO:Defining folds
2023-03-23 18:19:16,224:INFO:Declaring metric variables
2023-03-23 18:19:16,228:INFO:Importing untrained model
2023-03-23 18:19:16,232:INFO:Extreme Gradient Boosting Imported successfully
2023-03-23 18:19:16,239:INFO:Starting cross validation
2023-03-23 18:19:16,240:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:19:20,433:INFO:Calculating mean and std
2023-03-23 18:19:20,434:INFO:Creating metrics dataframe
2023-03-23 18:19:20,977:INFO:Uploading results into container
2023-03-23 18:19:20,978:INFO:Uploading model into container now
2023-03-23 18:19:20,978:INFO:_master_model_container: 17
2023-03-23 18:19:20,978:INFO:_display_container: 2
2023-03-23 18:19:20,979:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=666, ...)
2023-03-23 18:19:20,979:INFO:create_model() successfully completed......................................
2023-03-23 18:19:21,093:INFO:SubProcess create_model() end ==================================
2023-03-23 18:19:21,094:INFO:Creating metrics dataframe
2023-03-23 18:19:21,108:INFO:Initializing Light Gradient Boosting Machine
2023-03-23 18:19:21,108:INFO:Total runtime is 1.3897465507189433 minutes
2023-03-23 18:19:21,112:INFO:SubProcess create_model() called ==================================
2023-03-23 18:19:21,113:INFO:Initializing create_model()
2023-03-23 18:19:21,113:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:19:21,113:INFO:Checking exceptions
2023-03-23 18:19:21,113:INFO:Importing libraries
2023-03-23 18:19:21,113:INFO:Copying training dataset
2023-03-23 18:19:21,131:INFO:Defining folds
2023-03-23 18:19:21,131:INFO:Declaring metric variables
2023-03-23 18:19:21,135:INFO:Importing untrained model
2023-03-23 18:19:21,140:INFO:Light Gradient Boosting Machine Imported successfully
2023-03-23 18:19:21,163:INFO:Starting cross validation
2023-03-23 18:19:21,164:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:19:26,532:INFO:Calculating mean and std
2023-03-23 18:19:26,533:INFO:Creating metrics dataframe
2023-03-23 18:19:27,064:INFO:Uploading results into container
2023-03-23 18:19:27,064:INFO:Uploading model into container now
2023-03-23 18:19:27,065:INFO:_master_model_container: 18
2023-03-23 18:19:27,065:INFO:_display_container: 2
2023-03-23 18:19:27,065:INFO:LGBMRegressor(random_state=666)
2023-03-23 18:19:27,065:INFO:create_model() successfully completed......................................
2023-03-23 18:19:27,181:INFO:SubProcess create_model() end ==================================
2023-03-23 18:19:27,181:INFO:Creating metrics dataframe
2023-03-23 18:19:27,195:INFO:Initializing CatBoost Regressor
2023-03-23 18:19:27,196:INFO:Total runtime is 1.4912159204483033 minutes
2023-03-23 18:19:27,200:INFO:SubProcess create_model() called ==================================
2023-03-23 18:19:27,200:INFO:Initializing create_model()
2023-03-23 18:19:27,200:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:19:27,200:INFO:Checking exceptions
2023-03-23 18:19:27,201:INFO:Importing libraries
2023-03-23 18:19:27,201:INFO:Copying training dataset
2023-03-23 18:19:27,220:INFO:Defining folds
2023-03-23 18:19:27,220:INFO:Declaring metric variables
2023-03-23 18:19:27,225:INFO:Importing untrained model
2023-03-23 18:19:27,233:INFO:CatBoost Regressor Imported successfully
2023-03-23 18:19:27,242:INFO:Starting cross validation
2023-03-23 18:19:27,243:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:19:32,237:INFO:Calculating mean and std
2023-03-23 18:19:32,238:INFO:Creating metrics dataframe
2023-03-23 18:19:32,768:INFO:Uploading results into container
2023-03-23 18:19:32,768:INFO:Uploading model into container now
2023-03-23 18:19:32,769:INFO:_master_model_container: 19
2023-03-23 18:19:32,769:INFO:_display_container: 2
2023-03-23 18:19:32,769:INFO:<catboost.core.CatBoostRegressor object at 0x0000021B532CFF70>
2023-03-23 18:19:32,769:INFO:create_model() successfully completed......................................
2023-03-23 18:19:32,887:INFO:SubProcess create_model() end ==================================
2023-03-23 18:19:32,887:INFO:Creating metrics dataframe
2023-03-23 18:19:32,902:INFO:Initializing Dummy Regressor
2023-03-23 18:19:32,902:INFO:Total runtime is 1.5863219022750856 minutes
2023-03-23 18:19:32,906:INFO:SubProcess create_model() called ==================================
2023-03-23 18:19:32,906:INFO:Initializing create_model()
2023-03-23 18:19:32,906:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B5659B820>, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:19:32,907:INFO:Checking exceptions
2023-03-23 18:19:32,907:INFO:Importing libraries
2023-03-23 18:19:32,907:INFO:Copying training dataset
2023-03-23 18:19:32,925:INFO:Defining folds
2023-03-23 18:19:32,925:INFO:Declaring metric variables
2023-03-23 18:19:32,929:INFO:Importing untrained model
2023-03-23 18:19:32,932:INFO:Dummy Regressor Imported successfully
2023-03-23 18:19:32,940:INFO:Starting cross validation
2023-03-23 18:19:32,941:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-03-23 18:19:37,256:INFO:Calculating mean and std
2023-03-23 18:19:37,257:INFO:Creating metrics dataframe
2023-03-23 18:19:37,785:INFO:Uploading results into container
2023-03-23 18:19:37,785:INFO:Uploading model into container now
2023-03-23 18:19:37,786:INFO:_master_model_container: 20
2023-03-23 18:19:37,786:INFO:_display_container: 2
2023-03-23 18:19:37,786:INFO:DummyRegressor()
2023-03-23 18:19:37,786:INFO:create_model() successfully completed......................................
2023-03-23 18:19:37,900:INFO:SubProcess create_model() end ==================================
2023-03-23 18:19:37,901:INFO:Creating metrics dataframe
2023-03-23 18:19:37,926:INFO:Initializing create_model()
2023-03-23 18:19:37,927:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=GradientBoostingRegressor(random_state=666), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-03-23 18:19:37,927:INFO:Checking exceptions
2023-03-23 18:19:37,929:INFO:Importing libraries
2023-03-23 18:19:37,929:INFO:Copying training dataset
2023-03-23 18:19:37,948:INFO:Defining folds
2023-03-23 18:19:37,948:INFO:Declaring metric variables
2023-03-23 18:19:37,949:INFO:Importing untrained model
2023-03-23 18:19:37,949:INFO:Declaring custom model
2023-03-23 18:19:37,950:INFO:Gradient Boosting Regressor Imported successfully
2023-03-23 18:19:37,950:INFO:Cross validation set to False
2023-03-23 18:19:37,950:INFO:Fitting Model
2023-03-23 18:19:38,308:INFO:GradientBoostingRegressor(random_state=666)
2023-03-23 18:19:38,308:INFO:create_model() successfully completed......................................
2023-03-23 18:19:38,462:INFO:_master_model_container: 20
2023-03-23 18:19:38,462:INFO:_display_container: 2
2023-03-23 18:19:38,463:INFO:GradientBoostingRegressor(random_state=666)
2023-03-23 18:19:38,463:INFO:compare_models() successfully completed......................................
2023-03-23 18:19:38,487:INFO:Initializing predict_model()
2023-03-23 18:19:38,487:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000021B51133310>, estimator=GradientBoostingRegressor(random_state=666), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, replace_labels_in_column=<function _SupervisedExperiment.predict_model.<locals>.replace_labels_in_column at 0x0000021B56ADCEE0>)
2023-03-23 18:19:38,488:INFO:Checking exceptions
2023-03-23 18:19:38,488:INFO:Preloading libraries
2023-03-23 18:19:38,492:INFO:Set up data.
2023-03-23 18:19:38,500:INFO:Set up index.
2023-03-23 18:21:52,568:WARNING:C:\Users\CHUKWUKA\AppData\Local\Temp\ipykernel_19712\3592058342.py:2: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  submission['Item_Outlet_Sales'] = transformed_predictions

2023-03-23 18:22:16,734:WARNING:C:\Users\CHUKWUKA\AppData\Local\Temp\ipykernel_19712\3051290019.py:2: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  submission['Item_Outlet_Sales'] = transformed_predictions

